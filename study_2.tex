
\chapter{Study 2  Police Incident Logs}


\section{Introduction}

The last study explored Modus Operandi (MO) data and used pre-trained language models (PTM) to classify the MO texts. This study further investigates the applicability of PTMs by classifying police incident logs.  Police incident logs are text documents that are generally written by police call handlers as they deal with a call for service from the public. Incident logs are important because police forces do not just deal with crimes. In fact up to 90\% of their calls for service are not crime related \ref{demand}, and so will only be recorded as incident logs. As a reminder problem-oriented policing (POP) is also not limited to crime prevention, but seeks to reduce any types of harms that the police can be thought responsible for. Investigating the automatic analysis of incident data is therefore important as it can provide insights into a whole host of police problems. 

This chapter will explore the classification of anti-social behaviour (ASB) police incident logs. These are a subset of incident logs that have been deemed to represent ASB. The remainder of this introduction will briefly define ASB before introducing a research article that influenced then used the results of this study to investigate ASB during the pandemic in the UK.  


\subsection{ASB Definition}
 A recent briefing paper published by the house of commons library \ref{brown_sturge_2021} defines ASB as \say{Anti-social behaviour (ASB) encompasses criminal and nuisance behaviour that causes distress to others. Typical examples include: noisy neighbours, vandalism, graffiti, public drunkenness, littering, fly tipping and street drug dealing.}  Legal definitions define ASB in two different contexts, those emanating in residential contexts and those from public spaces. In both cases the definitions are broad and centre on the impact of the actions rather than defining the actions. Thus what is ASB is hard to define precisely, but it is essentially activity that has a negative impact on others.    
 
 
 \subsection{Published work} This study overlaps with work that the author completed as part of the ESRC project - \emph{Reducing the crime harms of the Covid-19 pandemic}. The author was part of a small team that published a related journal article \textcite{halford_dixon_farrell_2022} that explored the effects of lockdowns on reports of ASB. The results of this study were used directly in that article, the PTMs and classification tasks used in this study were therefore directly influenced by the needs of that article. Therefore within the work surrounding this study there were two high level objectives, 1) Are PTMs useful for classifying police incident text? And 2) How did ASB reports change during the Covid-19 pandemic? The first of these high level objectives is the true purpose of this study and that will be the focus of this Chapter. The second objective helped form the question set for the first objective and so will be explained in the next section to provide context. 

I conducted all of the analysis within the journal article \ref{halford_dixon_farrell_2022}, of which the NLP work constituted around a third. In particular I was the author of the data chapter, the methods chapter and the NLP appendix.

\subsection{Problem overview} In January of 2020 the first Covid-19 cases were confirmed in the UK. This was the start of the Covid-19 pandemic across the UK. Shortly after in March 2020 the UK government ordered national lockdowns that restricted movement across the country and confined people to their homes long periods. Much literature has been published about the effects of lockdowns on crime (see \ref{halford2020crime} for an initial review of an area in Northern England and \ref{langton2021six} for a longer term view of the impacts in England and Wales). One of the significant increases in activity that the police recorded however, against a general backdrop in crime declines, was an increase in reported ASB. The increase in ASB was initially thought to be due to reports of lockdown breaches being recorded as ASB. However there was also competing hypothesis that confining more people into residential areas for longer had in fact caused more ASB. 

The aim of the research paper was to investigate the cause of the increase in ASB. In particular the research question for the Covid-19 project was whether the increase in ASB due to reports of people breaching Covid-19 legislation. Or was it an increase in more traditional forms of ASB such as noise complaints due to a defacto increased population density. 

As ASB is not a crime the recording practices surrounding it are not as rigorous as they are for crime data. Consequently the police force data that I explore has little structured data that would allow an understanding of intra-incident variation, and therefore how it may have varied during lockdowns. There was one additional structured data field that was added to the data during the lockdowns and this was a Covid marker that the call handlers could use if the incident was related to Covid-19. However, the police analysts were not confident that this marker had been used consistently or comprehensively due to the speed with which it was introduced. Therefore NLP models were used to classify the data, and the changes in these classification were then observed over time. These classifications are now explored in the next section.
 
\subsection{Classification Tasks} As with the earlier studies this study focusses on using PTMs to classify police texts. The three classification tasks for this study were picked to help answer the questions related to the effects of covid lockdowns on ASB. Those classification tasks are explained below and examples to differentiate between classifications are given afterwards in table \ref{tab:class_example}:

\begin{enumerate}
\item{Traditional ASB.} The first category was whether an incident was considered traditional ASB or not. As ASB can encompass a huge variety of activities this can also be thought of as \say{Could this ASB incident have happened before the pandemic?}. If it was related to only a covid incident - then it could not have happened before the pandemic had started, however if it was a party or a noise complaint then it could have happened before the pandemic - as long as the complaint wasn't soley focussed on Covid regulation breaches.
\item{Covid Complaint.} This second category only relates to the presence of a specific complaint about the breaking of covid regulations. For example reports of failing to wear a face mask.
\item{Groups.} This final category relates too if a group was complained about in the ASB log or not. Groups were classified as three or more people, though references to families were excluded. For example \say{Four adults having a party in a garden.} Would be an example of reference to a group.

\end{enumerate}


\begin{table}[]
\begin{tabular}{p{0.55\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}}
\hline
Example Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          & Traditional ASB & Covid Complaint & Group \\ \hline
an email request has been made . default email notification has been made to xxxxx . com . email received xxxxx xxxxx 22/10/2020 22 xxxxx 12 incident relates to xxxxx group time of incident xxxxx 22 xxxxx 05 date of incident xxxxx additional information xxxxx i believe my neighbours are currently having a party with people outside of their household . i also believe that they have done this a few times recently . location address xxxxx flat xxxx , the village , xxxx xxxxx road , xxxxx xxxxx name of persons involved if known xxxxx is the subject displaying any covid 19 symptoms xxxxx unknown & N               & Y                                                          & Y     \\
- INFORMANT reporting there are 6 young men on motorbikes on the xxxxx way , riding round - INFORMANT said he cant see regs and DOESN'T want to get up close to them , - INFORMANT said they are right to the xxxxx way - xxxxx to covid-19 this is low asb and                                                                                                                                                                                                                                                                                                                                                       & Y               & N                                                          & Y     \\ \hline
\end{tabular}
\caption{\label{tab:class_example} Examples of ASB incident logs and the labelled classifications}
\end{table}

\subsection{Article Conclusion}
The conclusion of the article is that ASB reports did increase and that increase was in part due to reports of Covid infringements. Although around half of these additional complaints also included traditional ASB (e.g. noise complaints). Figure \ref{fig:ASB}  is taken from \ref{halford_dixon_farrell_2022} and is a graphical summary of the results of the NLP analysis. The blue bars are the traditional ASB reports, and the black line is a forecast of ASB levels we would have expected absent a pandemic. The purple bars are ASB reports with both a traditional ASB complaint and a covid complaint(e.g someone failing to wear a mask). The red bars are ASB incidents where it only contains a Covid complaint. In general it can be seen that the level of just traditional ASB is consistent with the expected, and that the additional reports included covid regulation breaches. What could not be answered however is whether the increase in traditional ASB reporting was due to additional ASB or a lowering of the reporting threshold as the reporters had an additional excuse to call the police - the covid infringement.

The remainder of this chapter will follow the same format as the earlier studies exploring the utility of PTMs with police free text. The specific research questions for this chapter are set out next. This will be followed by a review of the data, methods, results and then finally the discussion and conclusion.


\begin{figure}
  \includegraphics[width=\linewidth]{images/covid_label_plot_new_colours.png}
  \caption[ASB in the Pandemic]{A plot showing recorded ASB for one northern police force during the covid-19 pandemic.  Reproduced from  \textcite{halford_dixon_farrell_2022} }
  \label{fig:ASB}
\end{figure}






\section{Data}

The data used for this study consisted of police ASB incident logs from Lancashire Police for the year 2020,  93,809 logs. Incident logs were only included if the final classification of that log was ASB. A detailed description of the ASB data was given in the data chapter, Chapter 8.  However as a recap there are three main differences between the MO data used in the earlier studies and the incident log data analysed here. The first difference is length, the incident logs are much longer than the MO data. The median word count for the MO data was 31 and for the ASB logs the median count is 166. Secondly the police incident logs are also generated in a different manner to the MO data, they are an on going log of what is happening in that incident. The logs are rarely edited, rather they are generated as the incident unfolds by the operator in the the control room.  Incident logs are intended for internal use only, whereas MO data is generally written post hoc by a police officer for external use - so names of suspects or other personal data are not routinely used. Thirdly although the same whitelisting process was used for both text types, the process was tailored to the redaction of the MO data and not the ASB logs. Coupled with a different generation process this means that a higher proportion of words were redacted in the ASB log data (8\%) than the MO data (2\%), meaning that close to one in every 12 words was redacted.

\section{Method}


\subsection{Data labelling} The data was labelled by two researchers according to the classifications outlined earlier. Disagreements between the two labellers were decided by the author. The data was selected using active learning based upon the Covid complaint classification task. As before a test and validation set were randomly selected before the training set was developed. The batch size for the active learning was 50, again this roughly equated to one hour of labelling for each batch of texts. In total 900 incident logs were labelled. There were 200 logs labelled for both the validation and the test set and there were an additional 500 logs labelled for the training set. Labelling was stopped when the researcher resource had been expended.

\subsection{Fine-tuning the PTM} As the incident texts are generally longer than the MO texts it was not possible to use the BERT model as in the previous studies. As mentioned in the Methods Chapter (Chapter 9) there is a similar model, also a PTM called Longformer \ref{beltagy2020longformer}  that is designed for longer pieces of text. The Longformer model was used throughout this study for the classification of police incident logs. The length of the text still posed problems for the computing power available for this study, particularly the memory available in the computer. For this reason the hyperparameters for the model were adjusted to avoid memory problems ( i.e attempting to use more memory than the computer had) rather than optimising for model accuracy. Even with the adjustment of hyper parameters the max text length had to be set at 1500 meaning that some ($<1\%$) of the incident logs would have the final words trimmed as they entered the model.

In addition whilst fine-tuning the model it was discovered that removing the \say{xxxxx} token from the incident logs increased classification performance. Recall that during the whitelisting process the \say{xxxxx} token replaces any words that were not on the safe list, typically pronouns. Therefore all model fine-tuning in this study was conducted with the \say{xxxxx} token removed from all incident logs. 

\subsection{Performance} As with the earlier studies performance will be measured across MCC metrics to see how correct the model is. Explainability will be explored through the use of the LIME tool.  Word clouds will be generated presenting the most important words for each classification. Bias will be explored in the context of request method as victim data is not available. Request method relates to how the request was received, typically by phone call or electronically (online form or email).

 

\section{Results}

\subsection{MCC} The MCC metrics results for the ASB police incident logs are generally worse than the metrics from the earlier studies. No classification model achieved a MCC metric of over 0.9. As with earlier studies each model is built ten times to explore variation due to randomness in the model builds. There was also considerable variation across model builds. Variation is due to random initialisation of the models. Using the best metrics produced the \emph{Groups} classification of police incident logs had the highest MCC score (0.83). Next was the \emph{Covid} classification (0.81) and finally \emph{Traditional ASB} (0.71). The F1 scores are recorded for comparison. The F1 scores are comparable to, but lower ($\approx 0.05$) than those scores from Longformer models fine-tuned on standard academic NLP tests (see Table 7 of \textcite{beltagy2020longformer}) 

\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
Run      & Trad ASB & Covid & Groups \\ \midrule
1        & 0.59     & 0     & 0.78   \\
2        & 0.63     & 0.78  & 0.79   \\
3        & 0.67     & 0.62  & 0.8    \\
4        & 0.68     & 0.73  & 0.8    \\
5        & 0.66     & 0.81  & 0.81   \\
6        & 0.52     & 0.81  & 0.77   \\
7        & 0.59     & 0.75  & 0.83   \\
8        & 0.64     & 0     & 0.81   \\
9        & 0.71     & 0.74  & 0.79   \\
10       & 0.67     & 0.73  & 0.77   \\
Mean     & 0.636    & 0.597 & 0.795  \\
Best Run & 0.71     & 0.81  & 0.83   \\ \bottomrule
\end{tabular}
    \caption{{Table of ASB model metrics} MCC scores for the three ASB classification problems. Each model was trained 10 times with the same data.}
    \label{tab: asb_metrics}
\end{table}

\subsection{Explainability} As with the previous studies the results of the explainability are provided as word clouds. A good result is larger words within the cloud having an intuitive bearing on the classification type. Unlike the previous studies there is only one word cloud per classification type. Producing word clouds for the police incident data required a temporary uplift to computer memory. However the idea to  produce both the negative and positive word clouds for each classification only came after the temporary uplift had ended. Hence - only the positive word clouds are presented. 

The word cloud for the traditional ASB classification is at Figure \ref{fig:wordcloud_trad}. Compared to previous word clouds, and other ASB word clouds, it is clear that there are not a few select words influencing the prediction. The word cloud contains many words all of a similar size. Indicating that the words have a similar impact on the classification. This was similar to the word clouds in study 1 where there was no direct mention of an event such as when a car was not stolen (see Figures \ref{fig: wordcloud_mv_rev} and \ref{fig: wordcloud_mv_rev_lancs})

\begin{figure}[h]
    \includegraphics[width=\textwidth]{images/trad_asb_wordcloud_100.png}
    \caption{{Word cloud for traditional ASB classification.} The top 100 words that contributed to a positive classification of traditional ASB}
    \label{fig: wordcloud_trad}
\end{figure}

Figure \ref{fig: wordcloud_covid} is the word cloud for the classification of Covid complaints. This word cloud represents the most influential words for determining if an ASB log contained a covid complaint. The largest word is \emph{Covid}, this is not surprising and gives confidence that the model is working as expected. However another notable word is \emph{Default}, there is no obvious connection between a covid complaint and the word \emph{Default}  so this may require further inspection to understand how the model is using that word.


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/covid_wordcloud_100.png}
    \caption{{Word cloud for Covid ASB classification}Words that contributed to a positive classification}
    \label{fig: wordcloud_covid}
\end{figure}

The third word cloud relates to the Group classification. This word cloud is at Figure \ref{fig:wordcloud_gather}. The largest words in this word cloud are \emph{group} , \emph{party} and \emph{groups}. These words clearly relate to groups or gatherings and so offer additional trust that the model is working on the expected words. Another, less prominent word, is \emph{males}. Possibly because most gatherings related to ASB contain mostly or only males. However this may mean that groups of only females will be more difficult to identify if the model is looking for masculine groups. That is there may be bias towards males being identified as groups. Sadly without data relating to the sex of the (potential) offenders, this potential bias cannot be explored here. 


\begin{figure}[h]
    \includegraphics[width=\textwidth]{images/gather_wordcloud_100.png}
    \caption{{Wordcloud for group ASB classification.} Words that contributed to a positive classification}
    \label{fig: wordcloud_gather}
\end{figure}
   


\subsection{Bias}




\section{Discussion} This section discusses the results that have just been presented, particularly with reference to the earlier studies exploring the MO data.  There is consistency between the findings with this study, investigating the use of PTMs with police incident data and the earlier studies with PTMs and MO data. However there are some important differences that will now be explored.


\subsection{Performance} The MCC metrics for the police incident data were lower than the metrics for the MO data. That is the models were not as good at classifying the incident texts as they were for the MO texts. This is likely down to three reasons. Firstly the incident texts are not edited so they are messier. They can also contradict themselves so reading and comprehending the texts is often difficult for a human . Secondly there was less training data. Only 500 texts were used to train the models for the incident texts whereas more data (700 and 900) were used to train the models in study 1a. Thirdly the model architecture was different, recall that the Longfomer model was adopted over the BERT model for the incident texts. The next paragraphs will explore each of these in turn and will then explore if the models generated are suitable for use

\subsubsection{The data} The incident texts were not edited, contained boilerplate wording and were redacted. All of these factors mean that the data to predict is more dissimilar to the original training data that the models were initially trained on. Here I refer to the pre-training on the vast quantities of data outlined in the Methods chapter, not the fine-tuning on the incident logs. When the data for predicting is different from the training data the models become less powerful, because they have not learnt the representations of that language well. Broadly there are two solutions to this problem. Firstly pre-train the model from the outset on similar data. In this case instead of pre-training the model on wikipedia data, then incident log data could be used much earlier in the process. This will however require vast amounts of data and compute power. Secondly if Mohammed wont come to the mountain then we can take the mountain to Mohammed.  That is we can change the police incident texts so that they become more like the training data. How so? Additional words can be added to the model dictionaries so that the model is able to represent more words. Jargon within the incident texts can be changed for more commonly understood words. The data can not be redacted (requiring security needs to be met in other ways). Boilerplate wording can be removed.  Every transformation of the data  will require additional effort, and when dealing with large quantities of data then the transformations will need to be automated, limiting what can be changed. Which solution works best? That remains an open question and will be a good avenue for future research.   

\subsubsection{Labelling} There was less data labelled for these classifications than the earlier studies. This may have led to a lower MCC score. In addition the greater variation in MCC score across the ten random initialisations is also an indicator that the model was not converging on the optimal solution. Working with messier data indicates that more data is required as there is more variation within the texts. Therefore future researchers may wish to ensure that when dealing with unedited and longer text that they set aside more resources for the labelling.

\subsubsection{Computing power}Although the computing power available for this modelling task was adequate - in that the models could be run. It was  sub-optimal because the hyper parameters had to be adapted to lower the memory required for modelling. This unfortunately is a factor when using transformer models with long pieces of data. Computers with access to larger amounts of memory do exist, however they are not typically found in standard desktop computers. This means that access to them for longer texts is problematic and may not be routinely available to police forces. That being said the increasing use of cloud technology may mean that accessing more powerful computers becomes less problematic.

 
 \subsubsection{When is a model good enough?}  \say{All Models are wrong, but some are useful\footnote{Attributed to George E.P. Box.} }.When is a model good enough to be used? Or put another way what MCC score needs to be achieved for the model to be useful? This is an open question with no definitive answer, but the decision can be made by considering three factors. Firstly, at what scale is the model to be used. In this instance we used these models to track the change of ASB overtime. We were observing relative changes not absolute changes and so the correctness of a single instance did not matter. However, if the response to an individual instance did matter then clearly a higher MCC score would be better. Secondly how good is the model compared to the existing process. Typically, the existing process is humans reading the texts. Humans are not infallible, suffer from fatigue and are generally expensive. In the case of our work, we classified some ninety-three thousand texts - at a conservative estimate that would have taken a single person 124 working days to read. That's 24 working weeks or around half a working year. In short without the ML model the work would not have been done. Thirdly and finally consideration must be given to the cost of getting things wrong. There are two possible ways to get things wrong. False positive - it wasn't but the model said it was. False negative - it was but the model said it wasn't. The costs of these may differ, for instance the cost of missing a domestic abuse crime may be greater than the cost of spending resource on a crime that wasn't domestic abuse. Each problem will have its own cost trade-off, and this will affect how robust the model will need to be and in what ways.
 
 In short there, although the performance metrics used here can point to models that are better than others and can quantify the likely errors. The metrics cannot be used in isolation to decide if a model should be used or not. In addition important considerations will also be need to be drawn from the results of the explainability and bias investigations. These are explored next.


\subsection{Explainability} The explainbility results of the incident text models were similar to the MO models of study 1. Where there were specific mentions of a classification, e.g. a Covid complaint,  words that related to that classification were generally more prominent. The traditional ASB classification, which does not relate to a specific type of incident produced a more homogenous word cloud. Reflecting the greater spread of possible descriptions. 

There was one notable exception in the explainability investigation that was highlighted through the word clouds. This was the prominence of the word \emph{Default} for the Covid classification. On investigation the word default is primarily used when an email or online complaint is automatically added to the police incident logs. This means that the word \emph{Default} is really being used a s a proxy for the delivery method of the complaint.  xx\% of online and email reports contained covid complaints.  Whereas only xx\% of  telephone reports contained covid complaints. Therefore, it can be seen that if a complaint was made by email or online then it is much more likely to be a covid complaint than if it was lodged by telephone.   This can be seen further in the bias statistics where the misclassification rates are different between electronic and telephone methods. 

This is a good example of why explainability investigations are important and how they can be useful for making more accurate predictions. In this case removing the standard text that comes with the electronic forms of incident logging is likely to improve classification as the model will not be able to use a proxy for logging type. This text removal was not completed here though and so would be avenue for future research.

 

\subsection{Bias}Although the data could not be checked for bias against victim or offender characteristics it could be checked for bias against input method. The results showed that ......

\subsection{Limitations}

\section{Conclusion} In conclusion the results from using PTM with police incident texts have been encouraging, even if the results have been poorer than the MO text results. The poorer results though were not a surprise. Before modelling began the length, the unedited nature of the texts and the greater loss through whitelisting were all factors identified as possibly contributing to a poorer outcome. These factors coupled with sub-optimal compute power and lower volumes of training data contributed to lower MCC scores. However some of these factors might be overcome in subsequent research and therefore the power of PTMs may be more successfully harnessed. Whitelisting can be tailored and perhaps eradicated if data is kept on police servers. Text data can be modified to remove automatically generated text. More compute power can be resourced through better research plans. However until there are more substantial developments in transformer model architecture, the length of incident logs may prove problematic. This also has implications for other longer pieces of text, such as witness statements, that police forces may wish to analyse. The next section summarises the whole chapter.

\section{Summary} This chapter has introduced and tested the use of transformer models with police incident data. The main difference between this study and the previous studies is the type of data analysed and the model used. 

The data in this study was police incident data. Data which is both longer and messier than the MO data used earlier. The incident data had more words removed through whitelisting and constitutes unedited logs that are written as situations develop. The incident data also includes stock phrases contained with online and email reports. The incident logs are generally longer than MO incident data.

The length of the data meant that the BERT model used in previous studies was unsuitable. Therefore, the Longformer model was used as this is designed for longer pieces of text. The length of the text also requires more computer memory for fine-tuning. Due to limitations with  available memory hyperparameters were selected on the basis of lowering memory requirements rather than optimising performance metrics.

Three classification tasks were conducted. These tasks were developed in order to answer questions surrounding covid legislation breaches during the pandemic lockdown periods in 2020. The classifications were, presence of traditional ASB, presence of a covid complaint and finally the mention of a group of individuals in the incident log. Data was labelled using active learning and the Longfomer model was fine-tuned on these tasks.

Performance metrics were lower than the metrics with the MO data, though they were still comparable to standard benchmark tests. This is partly because of the lack of compute power available  and less labelled data to tune the models correctly.

The word clouds from the explainability investigation generated an anomalous word for the covid classifications - \emph{Default}. On investigation it appears that this word is predominantly seen in incidents that are reported online. The model was using this word as a proxy for online submitted content which had a much higher percentage of covid complaints than other log types. This lead to bias across the log types based on how they were submitted.Analysis of victim bias was not possible due to a lack of metadata. 


This study has shown that PTM can be used to analyse police incident logs at scale. However in order to explore the full potential of PTMs more powerful computers are required. Biases will also have to be guarded against if data contains phrases that can be used as a proxy for external variables.

This was the final case study in this work and therefore concludes this part of the thesis. The next part of the thesis synthesises the lessons from these case studies, draws lessons and comments on possible future research directions.
 


%Initially the model was labeled for around 9 factors(see table \ref{tab:asb_labs} for a complete list). However the first scan of the data, and integrating those factors generated from a theoretical perspective are not always successful. Of the nine factors explored only three were seen through to completion.  
%\setlength{\extrarowheight}{12pt}
%\begin{table}[]
%\centering
%\begin{tabular}{p{0.4\linewidth}|p{0.6\linewidth}}
%\toprule
%\rowcolor[HTML]{C0C0C0} 
%\multicolumn{1}{c}{\cellcolor[HTML]{C0C0C0}\textbf{Label}}          & \multicolumn{1}{c}{\cellcolor[HTML]{C0C0C0}\textbf{Possible Responses}} \\ \midrule
%Specific concern about Covid-19 regulation breach from complainant.     & Primary/Secondary/ No / Not sure          \\
%Traditional ASB                                                         & Yes/No/ Not sure                          \\
%Specific report of social distancing breach                             & Yes/No/ Not sure                          \\
%Specific report of face covering breach                                 & Yes/No/ Not sure                          \\
%Specific report of lockdown breach                                      & Lockdown/ Self-Iso /Quarantine /No / Not sure \\
%Report of any gathering (>3 people)                                     & Yes/ Rule6 / No / Not Sure                \\
%Incident that was not attended due to low priority because of Covid     & Yes / No /Not Sure                        \\
%Incident as a result of police trying to enforce Covid policies         & Yes / No / Border control check           \\
%Complainant sex                                                         & Male/Female/Other/Don’t know              \\ \bottomrule
%\end{tabular}
%\caption{\label{tab:asb_labs} The complete initial list of labels and their potential classifications. Not all labels were fully classified as explained in the main text.}
%\end{table}
