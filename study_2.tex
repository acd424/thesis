
\chapter{Study 2: PF2 ASB Incident Logs}


\section{Introduction}

The last study explored MO data and the use of PTMs to classify texts. This study investigates the applicability of PTMs to the classification of police incident logs. Police incident logs are text documents that are generally written by call handlers as they manage calls for service from the public. Incident logs are important because police forces do not only tackle crime. In fact, up to 90\% of calls for service are not related to crime\parrencite{demand} and may therefore only be recorded as incident logs. By way of reminder, POP is also not limited to crime prevention; it is intended to reduce all types of harm for which the police can be deemed responsible. An investigation of the automatic analysis of incident data is therefore important because it can provide insights into a wide array of problems that police forces encounter.

This chapter explores the classification of antisocial behaviour (ASB) incident logs. These logs are a subset of incident logs that have been deemed to represent ASB. The remainder of this introduction briefly defines ASB before introducing a research article that influenced then applied the results of this study to investigate ASB during the pandemic in the UK.


\subsection{ASB Definition}
A recent briefing paper that was published by the House of Commons Library   \parencite{brown_sturge_2021} defined ASB as follows: \say{Anti-social behaviour (ASB) encompasses criminal and nuisance behaviour that causes distress to others. Typical examples include: noisy neighbours, vandalism, graffiti, public drunkenness, littering, fly tipping and street drug dealing.}  Legally, ASB has also been defined in two different contexts, namely the residential and the public. In both cases, the definitions are broad and revolve around the impact of the actions in question instead of defining them. Therefore, ASB is difficult to define precisely. In essence, it involves activities that have a negative impact on others but fall short of being crimes.    
 
 
 \subsection{Published work} This study overlaps with the work that the author completed as part of the ESRC project Reducing the Crime Harms of the Covid-19 Pandemic. The author was part of a small team that published a related journal article  \parencite{halford_dixon_farrell_2022}. It explored the effects of lockdowns on reports of ASB. The results from the present study were used directly in that article. The PTMs and the classification tasks that were used in this study were likewise directly influenced by the demands of that article. Therefore, the work behind this study was driven by two high-level questions. 1) Are PTMs useful for classifying police incident text? 2) How did ASB reports change during the Covid-19 pandemic? The first of these high-level questions reflects the true purpose of the study and forms the subject matter of this chapter. The second question facilitated the formulation of the question set for the first objective and is therefore explained in the next section in order to provide context. 

I conducted all of the analysis that is presented in the journal article \parencite{halford_dixon_farrell_2022}, of which the NLP work constituted approximately a third. In particular, I was the author of the data section, the methods section, and the NLP appendix.

\subsection{Problem overview} The first Covid-19 cases in the UK were confirmed in January 2020, and they marked the start of the Covid-19 pandemic in the country. Shortly thereafter, in March 2020, the UK government imposed a national lockdown that restricted movement across the country and confined its citizens to their homes for long periods of time. Much has been published about the effects of the lockdowns on crime (see \parencite{halford2020crime} for an initial review on an area in Northern England and \parencite{langton2021six}  for a longer-term perspective on the impacts in England and Wales). Against the general backdrop of a decline in crime during the pandemic, the police recorded a significant increase in reports of ASB. The increase in ASB was initially thought to be due to lockdown breaches being recorded as incidents of ASB. The competing hypothesis was that confining the population to residential areas for long periods of time had, in fact, caused more ASB.

The aim of the research paper was to investigate the cause of the increase in reports of ASB. In particular, the research question that animated the Covid-19 project was “Were reports of people breaching Covid-19 legislation the main cause of an increase in ASB reports?”. The alternative theory was that traditional forms of ASB, such as excessive noise, became more widespread in consequence of the de facto increase in population density.

Since ASB is not a crime, recording practices are not as rigorous as for crime data. Consequently, the ASB data that were available were unstructured and did not enable an examination of intra-incident variation or its variance during the lockdowns. An additional structured data field was introduced during the lockdowns, namely a "Covid” marker that the call handlers could use if an incident was related to the coronavirus. However, the police analysts were not confident that this marker had been used consistently or comprehensively due to the speed with which it had been introduced. Therefore, NLP models were used to classify the data, and the changes in these classifications were observed over time. These classifications are explored in the next section.

 
\subsection{Classification Tasks} Like the previous studies, this one focuses on the use of PTMs to classify police texts. The three classification tasks for this study were picked with a view to answering the questions that are related to the effects of the Covid-19 lockdowns on ASB. Those classification tasks are explained below. Examples that enable the classifications to be differentiated are given in Table \ref{tab:class_example}.

\begin{enumerate}
\item{Traditional ASB.} The first question was whether an incident was a form of traditional ASB or not. ASB can encompass a wide variety of activities. Another formulation of this question runs as follows: \say{Could this ASB incident have happened before the pandemic?}. If it was related only to a Covid-19 incident, then it could not have occurred before the pandemic had started. However, if it was a party or a noise complaint, then it could have happened before the pandemic as long as the complaint did not focus solely breaches of the Covid-19 regulations.

\item{Covid Complaints.} The second category is only related to the presence of a specific complaint about a breach of the Covid-19 regulations. Reports of failing to wear a face mask is a possible example.

\item{Groups.} This final category is also related to whether the ASB log contains a complaint about a group. Groups were assumed to comprise three or more individuals. References to families were excluded. For example, the text \say{Four adults having a party in a garden} would be assumed to refer to a group.

\end{enumerate}


\begin{table}[]
\begin{tabular}{p{0.55\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}}
\hline
Example Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          & Traditional ASB & Covid Complaint & Group \\ \hline
an email request has been made . default email notification has been made to xxxxx . com . email received xxxxx xxxxx 22/10/2020 22 xxxxx 12 incident relates to xxxxx group time of incident xxxxx 22 xxxxx 05 date of incident xxxxx additional information xxxxx i believe my neighbours are currently having a party with people outside of their household . i also believe that they have done this a few times recently . location address xxxxx flat xxxx , the village , xxxx xxxxx road , xxxxx xxxxx name of persons involved if known xxxxx is the subject displaying any covid 19 symptoms xxxxx unknown & N               & Y                                                          & Y     \\
- INFORMANT reporting there are 6 young men on motorbikes on the xxxxx way , riding round - INFORMANT said he cant see regs and DOESN'T want to get up close to them , - INFORMANT said they are right to the xxxxx way - xxxxx to covid-19 this is low asb and                                                                                                                                                                                                                                                                                                                                                       & Y               & N                                                          & Y     \\ \hline
\end{tabular}
\caption[ASB Incident Log Examples]{\label{tab:class_example} Examples of ASB incident logs and the labelled classifications}
\end{table}

\subsection{Article Conclusion}The conclusion of the article is that ASB reports did increase and that the increase was due in part to reports of Covid-19-realted infringements. Approximately half of these additional complaints also referred to a traditional ASB (e.g., noise complaints). Figure \ref{fig:ASB}  is taken from \parencite{halford_dixon_farrell_2022}  and provides a graphical summary of the results from the NLP analysis. The blue bars represent reports of traditional ASB, and the black line is a forecast of the ASB levels that would have been expected had there been no pandemic. They were generated through the use of a time-series forecast. The purple bars are ASB reports that refer both to traditional ASB and to Covid-19-related complaints (e.g., failure to wear a mask). The red bars are ASB incident logs that only refer to Covid-19-related matters. In general, the level of conventional ASB was consistent with expectations, and the additional reports included references to Covid-19 regulation breaches. One question that could not be answered, however, was whether the increase in reports of traditional ASB was attributable to additional ASB or to the lower reporting threshold (the reporters had an additional reason to call the police, namely the Covid-19 infringement).

The remainder of this chapter follows the same format as the earlier studies that explored the utility of PTMs when they are applied to free text. The specific research questions for this chapter are set out in the next section, which is followed by a review of the data, the methods, the results, and, finally, a discussion and a conclusion.


\begin{figure}
  \includegraphics[width=\linewidth]{images/covid_label_plot_new_colours.png}
  \caption[ASB in the Pandemic]{A plot showing recorded ASB for one northern police force during the covid-19 pandemic.  Reproduced from  \textcite{halford_dixon_farrell_2022} }
  \label{fig:ASB}
\end{figure}


\section{Data}

The data that were used for this study consist of ASB incident logs from PF2 for 2020. There are 93,809 logs in total. Incident logs were only included if their final classification was ASB. A detailed description of the ASB data was provided in Chapter 8. To recapitulate, there are three main differences between the MO data that were used in the earlier studies and the incident log data that are analysed here. The first difference is of length – the incident logs are much longer than the MO texts. The median word count for the MO texts is 31, and the median word count for the incident logs is 166. Secondly, the police incident logs are also generated differently. They are ongoing logs of the events that transpire in the course of an incident. The logs are rarely edited; instead, they are generated by operators in control rooms as incidents unfold. Incident logs are intended only for internal use, whereas MO texts are generally written post hoc by police officers and intended for external use. Accordingly, the names of suspects and other forms of personal information are not used routinely. Thirdly, although the same whitelisting procedure was applied to texts of both types, that process was tailored to the redaction of MO data and not to the redaction of ASB logs. Coupled with the different generation process, this feature of the problem means that a higher proportion of words were redacted in the ASB logs (8\%) than in the MO texts (2\%). Close to one in every 12 words in the ASB logs was redacted.

\section{Method}


\subsection{Data Labelling} The data were labelled by two researchers according to the classifications that were outlined earlier. Disagreements between the two labellers were settled by the author. The data were selected by using active learning and on the basis of the Covid-19 complaint classification task. As before, a test and validation set were randomly selected before the training set was developed. The batch size for active learning was set to 50. Again, this is roughly equal to 1 hour of labelling for each batch of texts. A total of 900 incident logs were labelled, with 200 logs labelled for both the validation and the test sets and an additional 500 logs labelled for the training set. Labelling ceased when the resources of the researcher had been expended.

\subsection{Fine-tuning the PTM} Since the incident texts are generally longer than the MO texts, it was not possible to use the BERT model from the previous studies. As mentioned in Chapter 9, the Longformer PTM \parencite{beltagy2020longformer}, a similar model, was designed for longer texts. The Longformer model was used throughout the study for the classification of police incident logs. The length of the text still posed problems for the computing facilities that were available, particularly computer memory. For this reason, the hyperparameters of the model were adjusted to avoid memory problems (i.e., attempting to use more memory than was available) rather than optimised for model accuracy. Even after the adjustment of the hyperparameters, the maximum text length had to be set to 1,500, meaning that the final words of some ($<1\%$) of the incident logs would have to be removed when the logs were entered into the model.

In addition, during the finetuning of the model, it was discovered that removing the  \say{xxxxx}token from the incident logs improved classification performance. In the whitelisting process, the  \say{xxxxx} token replaces words that are not on the safe list, typically nouns. Therefore, all model fine-tuning was conducted with the  \say{xxxxx} token removed from the logs.

\subsection{Performance} Like in the previous studies, performance is measured by reference to MCC metrics in order to determine the accuracy of the model. Explainability is explored through the use of the LIME tool. Word clouds were generated. They contain the most important words for each classification.
No metadata on victim or offender characteristics can be extracted from the police incident logs. Therefore, the investigations of bias are limited once more. However, as will be shown later, the investigation of explainability indicates that the word \say{Default} may have had an undue influence on the model classifications. On further inspection, it emerged that the word \say{Default} is used when an incident log is generated from a complaint that is submitted via electronic means, that is, when a member of the public files their complaint through the online system or via email. For this reason, the partition for the bias investigation is based on request method.

The term \say{request method}refers to the channel by which a request is received. Typically, requests are received by phone or electronically (through online forms or by email). The data were split into three categories, namely \say{telephone} (including emergency and nonemergency calls), \say{electronic} (including online forms and email), and \say{other}(including logs generated by officers). Since the bias investigation is limited to binary splits of the data, two partitions were required for each classification. The two partitions are 1) \say{telephone} versus \say{electronic} and \say{other}, and 2) \say{electronic} versus \say{telephone} and \say{other}. These two partitions were examined for each classification type. Therefore, there are six bias values for each metric. As before, the test set was investigated and a separate tenfold cross-validation experiment was used to understand the potential range of values from 10 different models that were trained on different and randomly selected data, as explained in Chapter 8.


\section{Results}

\subsection{MCC} The MCC metrics for the ASB police incident logs are generally lower than in the earlier studies. No classification model achieved an MCC of more than 0.9. As in the earlier studies, each model was built 10 times in order to explore variation due to randomness. There was considerable variation across model builds. Variation occurs due to the random initialisation of the models. The use of the best metrics produced the Groups classification of police incident logs that had the highest MCC score (0.83). It was followed by the Covid classification (0.81) and the Traditional ASB (0.71) classification. The F1 scores were recorded for comparison. They are comparable to but lower ($\approx 0.05$) than the scores from the Longformer models that were finetuned on standard academic NLP tests (see Table 7 of \textcite{beltagy2020longformer}). 

\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
Run      & Trad ASB & Covid & Groups \\ \midrule
1        & 0.59     & 0     & 0.78   \\
2        & 0.63     & 0.78  & 0.79   \\
3        & 0.67     & 0.62  & 0.8    \\
4        & 0.68     & 0.73  & 0.8    \\
5        & 0.66     & 0.81  & 0.81   \\
6        & 0.52     & 0.81  & 0.77   \\
7        & 0.59     & 0.75  & 0.83   \\
8        & 0.64     & 0     & 0.81   \\
9        & 0.71     & 0.74  & 0.79   \\
10       & 0.67     & 0.73  & 0.77   \\
Mean     & 0.636    & 0.597 & 0.795  \\
Best Run & 0.71     & 0.81  & 0.83   \\ \bottomrule
\end{tabular}
    \caption[Table of ASB model metrics]{{Table of ASB model metrics} MCC scores for the three ASB classification problems. Each model was trained 10 times with the same data.}
    \label{tab: asb_metrics}
\end{table}

\subsection{Explainability} As in the previous studies, the results for explainability are presented as word clouds. A satisfactory result is recorded if the larger words within the cloud have an intuitive bearing on the classification type. Unlike in previous studies, there is only one word cloud for each classification type. Producing word clouds for the police incident data required a temporary increase in computer memory. However, the idea of producing both negative and positive word clouds for each classification was only implemented after that temporary increase had ended. Accordingly, only the positive word clouds are available.

The word cloud for the traditional ASB classification is displayed in Figure \ref{fig:wordcloud_trad}. Compared to previous word clouds and other ASB word clouds, there are not a few select words that influence the prediction. The word cloud contains many words of a similar size, indicating that the words in question have similar impacts on classification. This was analogous to the word clouds in Study 1, in which there was no direct mention of the nonoccurrence of events (such as cars not being stolen; see Figures \ref{fig: wordcloud_mv_rev} and \ref{fig: wordcloud_mv_rev_lancs}).

Figure \ref{fig: wordcloud_covid} displays the word cloud for the classification of Covid complaints. This word cloud contains the most important words for determining whether an ASB log contains a Covid-19-related complaint. The largest word is \say{Covid}. This is not surprising, and it indicates that the model is working as expected. Another notable word is \say{Default}. There is no obvious connection between a Covid-19-related complaint and the word \say{Default}. Further inspection, explained later, does reveal how the model uses that word.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{images/trad_asb_wordcloud_100.png}
    \caption[Word cloud for traditional ASB classification.]{{Word cloud for traditional ASB classification.} The top 100 words that contributed to a positive classification of traditional ASB. Source: Author generated.}
    \label{fig: wordcloud_trad}
\end{figure}



\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/covid_wordcloud_100.png}
    \caption[Word cloud for Covid ASB classification]{{Word cloud for Covid ASB classification}Words that contributed to a positive classification. Source: Author generated.}
    \label{fig: wordcloud_covid}
\end{figure}

The third word cloud is related to the Groups classification. This word cloud is displayed in Figure \ref{images/gather_wordcloud_100.png}. The largest words are \say{group}, \say{party}, and \say{groups}. These words are clearly related to groups or gatherings and indicate that the model is working on words as expected. another significant though less prominent word is “males”. This is possibly prominent because most gatherings that are related to ASB are primarily or exclusively attended by males. However, if the model seeks male groups exclusively, female groups may be more difficult to identify. In other words, there may be a bias towards males being identified as members of groups. Unfortunately, without data on the sex of the (potential) offenders, this potential bias cannot be explored systematically here.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{images/gather_wordcloud_100.png}
    \caption[Wordcloud for group ASB classification.]{{Wordcloud for group ASB classification.} Words that contributed to a positive classification. Source: Author generated.}
    \label{fig: wordcloud_gather}
\end{figure}
   


\subsection{Bias} 

Table \ref{tab:asb_bias} displays the results from the bias investigation. The bias investigation focuses on complaint transmission methods (by telephone or electronically). This investigation revealed evidence of bias. The results are explored by classification type. In each case, the electronic partition is approximately the negative value of the telephone partition. This finding is not unexpected because the \say{other} category is smaller than these two categories. It was included in order to ensure that it would not have a strong effect, which it appears not to have had. The result is that the results can be described exclusively by reference to the \say{electronic} partition.

The first classification type is \say{traditional ASB}. The p values for the tenfold cross-validation experiment indicate that there is bias at a statistically significant level of both EoO and PP. However, there is a disparity in the sizes of the bias between the value of the metric from the test set and the mean value from the CV set. For EoO, the bias is larger in the test set; for PP, the bias is larger in the CV set. For EoO, which has a negative value for the electronic partition, the foregoing supplies evidence of bias against requests that are submitted by electronic means. In other words, the recall power for the electronic methods of making requests is lower than that of the telephone methods. The PTM finds it harder to classify positive instances of traditional ASB reports that are submitted electronically than to classify telephone reports. The values for PP are also negative, indicating that the electronic requests are subject to more errors. As far as PP is concerned, the equivalent result, formulated in words, would be as follows: among the positive instances that the PTM found, the accuracy of the positive classifications was lower if a request had been made electronically.

The next classification type is Covid complaint. The results for EoO and PP are similar. The test set produces larger absolute-sized metrics than the CV set. All CV metrics are statistically significant. However, unlike for the Traditional ASB classification, the signs of the metrics are reversed. In this case, the bias is against the telephone request method. The size of the biases for the Covid complaints are larger than the equivalent metrics for the traditional ASB classification. The EoO outcome is as follows: the PTM finds it easier to classify positive instances of Covid complaints correctly when they are submitted electronically rather than by telephone. The PP outcome, in words, is as follows: among the positive instances of Covid-19 complaints that the PTM 

The final classification type was the Groups classification. The group classification has the smallest bias metrics. Unlike for the other two classifications, the directions of the bias are not consistent, and not all of the metrics are statistically significant. The evidence of bias is therefore weaker for this classification than for the other two. The EoO has mixed signs across the test set and the CV set results. The CV set results are statistically significant. The values of the metrics are the smallest across all three classifications; if there is bias, its effect is small. The evidence of PP bias is weaker still, with the CV mean no longer statistically significant. The conclusion is that there is no strong evidence of EoO or PP bias in the group classification.


\begin{table}[]
\centering
\begin{tabular}{@{}llccc@{}}
\toprule
\rowcolor[HTML]{BFBFBF} 
\multicolumn{5}{c}{\cellcolor[HTML]{BFBFBF}Equality Of Outcome}                                                                             \\ \midrule

\multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Partition} & Test Set & CV Mean & CV p value \\ \midrule
Traditional ASB                                   & Electronic                                            & -0.128   & -0.067  & 0.005*      \\
Traditional ASB                                   & Telephone                                             & 0.103    & 0.060   & 0.003*      \\
Covid complaint                                   & Electronic                                            & 0.264    & 0.190   & 0.002*      \\
Covid Complaint                                   & Telephone                                             & -0.264   & -0.175  & 0.002*      \\
Group                                        & Electronic                                            & -0.023   & 0.039   & 0.007*      \\
Group                                         & Telephone                                             & 0.018    & -0.037  & 0.006*      \\ \midrule
\multicolumn{5}{c}{\cellcolor[HTML]{BFBFBF}Predictive Parity}                                                                                                       \\ \midrule

\multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Partition} & Test Set & CV Mean & CV p value \\ \midrule
Traditional ASB                                   & Electronic                                            & -0.007   & -0.151  & 0.002*      \\
Traditional ASB                                   & Telephone                                             & 0.000    & 0.139   & 0.002*      \\
Covid complaint                                   & Electronic                                            & 0.435    & 0.166   & 0.003*      \\
Covid Complaint                                   & Telephone                                             & -0.375   & -0.160  & 0.008*      \\
Group                                         & Electronic                                            & 0.049    & 0.014   & 0.610      \\
Group                                         & Telephone                                             & -0.004   & -0.009  & 0.754      \\ \bottomrule
\end{tabular}
\caption[ASB incident logs Bias Table]{\label{tab:asb_bias} Extrinsic bias metrics for the PF2 ASB models. The model is denoted by the classification task. The partition is the factor used to split the data. The test set metric is calculated from the original test set. CV refers to cross validation. \emph{CV mean} is the mean of metrics from the 10 fold cross validation experiment. \emph{CV p value} is the p value for the hypothesis that the CV mean value is not zero. * indicates a p value that is significant.}
\end{table}

\section{Discussion} This section discusses the results that were presented in the preceding one, particularly by reference to the previous studies that explored the MO data. The findings of the two sets of studies are consistent. However, there are some important differences that are explored on the pages that follow.

\subsection{Performance} The MCC metrics for the incident data were lower than the metrics for the MO data. The models classified incident texts less successfully than MO texts. This finding is likely attributable to three causes. Firstly, the incident texts were not edited. They were also contradictory. Reading and comprehending such texts is often difficult for humans. Secondly, less training data were available. Only 500 texts were used to train the models for the incident texts, whereas more data (700 and 900 texts) were used to train the models in Study 1a. Thirdly, the model architecture was different. Recall that the Longformer model was preferred over the BERT model for the incident texts. The next paragraphs explore each of these matters in turn and then inquire whether the models are suitable for use.

\subsubsection{The data} The incident texts were not edited. They contain boilerplate, and they were redacted. All of these factors mean that the data to be predicted (the incident texts) were dissimilar to the original training data on which the PTMs were first trained (the pretraining data). When the data to be predicted are different from the pretraining data, models become less powerful because they do not learn the representations of that language well. In broad terms, there are two solutions to this problem. Firstly, the model can be pretrained on similar data from the outset. In this case, incident log data can be used much earlier than Wikipedia data. However, such a solution would require vast amounts of data and computing power.

Secondly, if Mohammed will not come to the mountain, then the mountain can be taken to Mohammed. Can the incident texts be changed so that they become more like the training data?If so, how? Words can be added to the model dictionaries to enable the model to represent more words. Jargon within the incident texts can be translated into more widely understood words. The data cannot be redacted, meaning that security needs to be met in other ways. Boilerplate can be removed. Every transformation of the data would require additional effort. If the quantities of data are large, then the transformations would need to be automated, limiting the scope of the changes. Which solution works best – changing the data or changing the model? This remains an open question that ought to be tackled in future research. 

\subsubsection{Labelling} Fewer data were labelled for these classifications than in the earlier studies, which may have resulted in a lower MCC score. In addition, the larger variation in MCC score across the 10 random initialisations also indicates that the model did not converge on the optimal solution. More data were required because there of the larger variation within the texts. Therefore, future researchers, when confronted with longer unedited texts, may wish to allocate more resources to labelling.

\subsubsection{Computing power}Although the computing power that was available for the modelling task was adequate, in that the models could be run, it was suboptimal because the hyperparameters had to be adapted to in order ensure that the amount of memory required for modelling would be lower. Unfortunately, this limitation is a factor when PTMs are used with long texts. Machines with access to larger amounts of memory do exist, but they are not typically desktop computers. Longer texts are problematic because such computing facilities are not routinely available to police forces. This said, the development of cloud technology may make access to more powerful computers less difficult.

 
 \subsubsection{When is a model good enough?}  \say{All Models are wrong, but some are useful\footnote{Attributed to George E.P. Box.} }.When is a model good enough for use? In other words, what MCC score needs to be achieved for a model to be useful? This is an open question that has no definitive answer, but a decision can be made by considering three questions. Firstly, at what scale is the model intended to be used? In this instance, the models were used to track the change in ASB over time. Relative rather than absolute changes were observed, and the correctness of single instances was not excessively important. However, if the response to an individual instance does matter, then a higher MCC score is clearly better. Secondly, how good is the model when compared to an existing process? Typically, the existing process is that of humans reading texts. Humans are not infallible. They suffer from fatigue, and they are generally expensive. A total of 93,000 texts were classified. As a conservative estimate, a single individual would need 124 working days, that is, 24 working weeks (or approximately half of the working year), to read those texts. In short, without the PTM, the work would not have been completed. Thirdly and lastly, what is the cost of errors? There are two possible ways to commit an error, a false positive and a false negative. The costs of the two may differ. For instance, the cost of missing a domestic abuse crime may be larger than the cost of expending resources on a crime that is not domestic abuse. Each problem entails specific cost-related trade-offs, and these trade-offs affect the robustness requirements for the model. 
 
IIn short, although the performance metrics that were used here can point to models that are superior to others and quantify the likely errors, they cannot be used in isolation to determine whether a model should be used. Important considerations also emerge from the investigations of explainability and bias that follow.


\subsection{Explainability} The explainability results for the incident text models are similar to those for the MO models in Study 1. Specific mentions of a classification, such as a Covid-19 complaint, were associated with related words that were more prominent. The traditional ASB classification, which is not related to a specific type of incident, produced a more homogenous word cloud, reflecting the wider spread of possible descriptions.

One notable exception emerged from the explainability investigation. It was highlighted by the word clouds. This exception was the prominence of the word \say{Default} in the Covid classification. The word \say{Default} is used primarily when an email or an online complaint is added automatically to the police incident logs. The word thus denotes the channel by which the complaint was received. On the whole, 64\% of online and email reports contain Covid-19 complaints. The corresponding figure for the telephone reports is 22\% (see Figure \ref{fig: asb_prop} for all percentages). Therefore, a complaint made by email or online was much more likely to have been a Covid-19 complaint than one made over the telephone. This can be seen further in the bias statistics. For the Covid classification, the misclassification rates differ between the electronic and the telephone delivery methods.

This is a good example of of the importance of explainability investigations and of their usefulness for improving predictive accuracy. In this case, removing the standard text that is added to the electronic forms of incident logs is likely to improve classification because the model cannot use a proxy for logging type. This text was not removed here, which opens an avenue for future research.

 \begin{figure}[h]
    \includegraphics[width=\textwidth]{images/asb_prop.png}
    \caption[Proportion of ASB classification by request type.]{{ASB positive classifications by request type.} The proportion of each positive classification by request type. For example 89\% of telephone requests contained traditional ASB. Percentages calculated from all of the hand labelled data. Source: Author generated}
    \label{fig: asb_prop}
\end{figure}

\subsection{Bias}Although the data could not be examined for bias against victims or offenders with particular characteristics, it could be examined for bias against particular request methods. The results contain evidence of bias for two of the classification types. For traditional ASB, the PTMs were less accurate when classifying electronic requests. For Covid-19 complaints, the PTMs were less accurate when classifying telephone requests.

As shown in the explainability section, it was found that the Covid-19 classification method drew on the elements of the automatically generated text to make predictions about Covid-19-related complaints. This was reinforced with the bias metrics – the Covid-19 classifier made more mistakes when applied to the electronic data because it relied on automatically generated text (e.g., \say{Default}) that does not contain information about Covid-19 complaints but instead reflects correlations.

The traditional ASB classifier made more errors when applied to the telephone data. It is evident from Figure \ref{fig:asb_prop}  that the telephone data predominantly contain logs that are related to traditional ASB. On this occasion, the explainability section supplies no evidence of words being misused. Therefore, it is not clear whether this bias is based on the same mechanism, that is, it is not clear whether there are words or phrases that distinguish the telephone data from the electronic data. A more profound investigation of the error analysis of individual logs and the word clouds for the negative classifications is necessary to understand the mechanisms of bias more adequately.

\subsection{Limitations}

\section{Conclusion} In conclusion, the results from using PTMs with police incident texts are encouraging, even if they are less satisfactory than the results for MO text. The inferior results are unsurprising. Before modelling, the length of the unedited texts and the larger loss from whitelisting were noted as factors that could contribute to inferior outcomes. These factors, coupled with the suboptimal computing power and the lower volumes of training data that were available, contributed to the lower MCC scores. Some of these issues may be overcome in future research, enabling the power of PTMs to be harnessed more effectively. Whitelisting can be tailored and perhaps even eradicated if data are kept on police servers. Text data can be modified to remove automatically generated text. More computing power can be resourced through the adoption of more adequate research plans. However, absent substantial developments in transformer model architecture, the length of the incident logs may continue to be problematic. This issue has implications for other long texts, such as witness statements, that police forces may wish to analyse. The next section summarises the whole chapter.

\section{Summary} This chapter introduced and tested the use of PTMs with police incident data. The main difference between this study and the previous ones lies in the type of data that were analysed and the model that was used.

This study examined police incident data, which is both longer and less structured than the MO data that were used previously. More words were removed from the incident data through whitelisting, and the data constitute unedited logs that were written as situations were developing. The incident data also include stock phrases from online and email reports. The incident logs are generally longer than the MO texts.

The length of the data meant that the BERT model from the previous studies was unsuitable to the present ends. Therefore, the Longformer model was used. It is designed for longer texts. The length of the texts also meant that fine-tuning placed more significant demands on the available computer memory. Due to the limited memory at the disposal of the researcher, the hyperparameters were set so as to lower memory requirements rather than to optimise performance.

Three classification tasks were completed. These tasks were developed in order to answer questions about breaches of the Covid-19 legislation during the 2020 lockdowns. The classifications revolved around the presence of traditional ASB, around Covid-19 complaints, and around mentions of groups of individuals in incident logs. The data were labelled by using active learning, and the Longformer model was fine-tuned on these tasks. The performance metrics were lower than for the MO data. They were nonetheless comparable to standard benchmark tests. This is partly due to the scarcity of computing power and labelled data for model tuning.

The word clouds from the explainability investigation pointed to an anomalous word for the Covid-19 classifications, namely \say{Default}. Further investigation revealed that the word in question is predominantly used in incident logs that are based on electronic requests. The PTM used \say{Default} as a proxy for online reports, which contained a much higher proportion of Covid-19 complaints than others. This tendency generated bias against certain reporting channels in the classification of the incident texts. Victim bias could not be analysed due to a lack of metadata.

This study showed that PTMs can be used to analyse police incident logs at a large scale. More powerful computers would be needed to explore the full potential of PTMs. Biases will also have to be mitigated, especially if the data contain phrases that can be used as proxies for external variables.

This was the final case study in this thesis. The next chapter summarises the results from all of the studies that were conducted.

%Initially the model was labeled for around 9 factors(see table \ref{tab:asb_labs} for a complete list). However the first scan of the data, and integrating those factors generated from a theoretical perspective are not always successful. Of the nine factors explored only three were seen through to completion.  
%\setlength{\extrarowheight}{12pt}
%\begin{table}[]
%\centering
%\begin{tabular}{p{0.4\linewidth}|p{0.6\linewidth}}
%\toprule
%\rowcolor[HTML]{C0C0C0} 
%\multicolumn{1}{c}{\cellcolor[HTML]{C0C0C0}\textbf{Label}}          & \multicolumn{1}{c}{\cellcolor[HTML]{C0C0C0}\textbf{Possible Responses}} \\ \midrule
%Specific concern about Covid-19 regulation breach from complainant.     & Primary/Secondary/ No / Not sure          \\
%Traditional ASB                                                         & Yes/No/ Not sure                          \\
%Specific report of social distancing breach                             & Yes/No/ Not sure                          \\
%Specific report of face covering breach                                 & Yes/No/ Not sure                          \\
%Specific report of lockdown breach                                      & Lockdown/ Self-Iso /Quarantine /No / Not sure \\
%Report of any gathering (>3 people)                                     & Yes/ Rule6 / No / Not Sure                \\
%Incident that was not attended due to low priority because of Covid     & Yes / No /Not Sure                        \\
%Incident as a result of police trying to enforce Covid policies         & Yes / No / Border control check           \\
%Complainant sex                                                         & Male/Female/Other/Don’t know              \\ \bottomrule
%\end{tabular}
%\caption{\label{tab:asb_labs} The complete initial list of labels and their potential classifications. Not all labels were fully classified as explained in the main text.}
%\end{table}
