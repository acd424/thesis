
\chapter{Machine Learning}

\begin{figure}
  \includegraphics[width=\linewidth]{transfer_figs/Slide7.jpeg}
  \caption[Machine learning diagram.]{A diagram to show machine learning. The inputs into the machine learning process are 1) the data and 2) the outcome of interest – commonly referred to as the label. The output from the process is a set of rules that can then be used to extrapolate to other, similar, data sets to make predictions. The rules can also be used to understand relationships within the original data.  Adapted from  \textcite{chollet_allaire_2018} and \textcite{provost2013data} }
  \label{fig:ML}
\end{figure}


\say{Machine Learning: Procedures for extracting algorithms, say for classification, prediction or clustering from complex data} \parencite{spiegelhalter2019art}

\say{With Machine Learning, humans input data as well as the answers expected from the data, and out come the rules} \parencite{chollet_allaire_2018}


As shown in the quotes above, machine learning is about finding a set of rules or an algorithm that allows one to understand the structure of the data. That is, the overarching aim of machine learning is to discover a model or a set of algorithms or rules that assist in explaining the data. However, the real goal for those who use machine learning is often to take these rules and use the information that they provide against other sets of data to make predictions about unknown quantities. A toy example, Figure \ref{fig:ML},  is predicting customer churn in a business \parencite{provost2013data}. Using historical data about customers, some known attributes like income and age can be used to try and explain the object of interest and whether they have left the company. A machine learning algorithm can generate a set of rules to predict whether those historical customers left. These discovered rules can then, if the conditions are similar, be extrapolated to another set of data to predict who will leave in the future.

As shown above, the core of machine learning is about learning rules from data; however, the application of those rules to more data is generally where the main interest lies in the application of machine learning. Once trained, these machines produce algorithms and rules that can then be used against unlabelled data to generate additional labels at a reduced resource intensity but without, hopefully, a significant reduction in accuracy. In this way, machine learning generates rules, which can then be used to automatically extract information from wider sets of data. Extracting information with machine learning algorithms can be classified into two broad categories (though in truth, it is more of a continuum): supervised and unsupervised learning  \parencite{chollet_allaire_2018}. Along the continuum between supervised and unsupervised learning is a process known as self-supervised learning, which is used to generate the models that this research leverages. The next sections explore supervised, unsupervised and semi-supervised learning.

\section{Supervised Learning} The key component of supervised learning is that the input data has already been labelled with information that puts them into their desired class.  Figure \ref{fig:ML} is an example of supervised learning. For instance, a data set of words may already have labels such as ’verb’ or ’noun’. These labels are then used by the machine to being to build rules to classify the data inputs. The final ingredient required for success in machine learning is a measure of whether the rules are doing a good job or not. This can be a measure as simple as accuracy (what \% were correct) to more complex calculations that can account for some permissible variation between given label and generated label. This success measure can then be used by the algorithm to select correct decision points and rules to improve the final rules. These final rules are then applied to unlabelled data, and the hope is that they are able to label the new data with similar accuracy (although almost certainly with lower accuracy). \textcite{chollet_allaire_2018} identify four basic approaches to supervised machine learning, which are discussed below with examples:

\paragraph{Probabilistic Modelling.} This style of model, of which Naive Bayes is the most widespread, attempts to find the probability of each potential classification \emph{given the data inputs}. It is worth noting here that the input data for machine learning typically consists of a set of attributes (akin to explanatory variables) and, as previously mentioned, a data label (akin to a dependant variable). There are no real restrictions on the type of data that these attributes or labels can take. They can be discrete data, names or labels, or they can be continuous data such as rational numbers. However, the style of data one has will help to determine which algorithm to choose. Naive Bayes treats each attribute as equally important and independent from the other attributes, and using Bayes’ theorem will calculate a probability for each potential label. The benefit with this algorithm is that the actual probability is not as important as the probabilities in relation to each other. That is, it is the relative size of the generated probabilities that is important, as the label with the largest probability is selected as the prediction. Another popular method in this class is logistic regression, which is also used to generate probabilities of a certain classification.

\begin{figure}
  \includegraphics[width=\linewidth]{transfer_figs/Slide8.jpeg}
  \caption[Decision tree example.]{An example of a simple decision tree. The tree first splits on the age of the customer, as that produces a homogenous grouping. The tree further splits the left-hand grouping by income, so that all groups are now homogenous.  Source: Author generated }
  \label{fig:tree}
\end{figure}

\paragraph{Divide and Conquer.} This type of algorithm is typified by the decision tree. Here, an attribute is first selected, probably at random, and then the attribute is stratified to split the data with the aim of partitioning it as homogeneously as possible into sub-groups based on the given labels. These sub-groups are then further split by either the same attribute (but with different stratification) or by other attributes. See Figure \ref{fig:tree} for a toy example. This can continue until certain conditions have been met, at which point the algorithm stops, and a set of rules has been generated. If the algorithm goes on for too long, there is a risk that each data point will have its own set of long and complicated rules generated. These rules may define the training data well but may not transfer well to other similar data that may need to be subsequently labelled. This process is known as over-fitting and is a flaw in machine learning algorithms that needs to be guarded against if the desire is to produce rules with a level of generalisability.

In the case of decision trees, the maximum depth may be specified beforehand so that long and complicated rules can be avoided. This specification is an example of a hyper-parameter – that is, an additional guide to the formation of the algorithm that limits the possible set of rules that can be generated. Hyper-parameters can have a dramatic effect on the end result of an algorithm, and it is usually good practice to try a variety of hyperparameters when working through problems to explore result sensitivity. More sophisticated models in this class include random forests, which use lots of smaller, randomly generated decision tress and then combine them to produce a single result. The benefit of this is that it can avoid  \emph{local minima} whereby a normal decision tree is led by its procedure into a non-optimal path. More sophisticated still are gradient boosting trees, which only try to predict the actual data once, then spend the reminder of their time trying to minimise the residuals from the previous models with additional trees.

\begin{figure}
  \includegraphics[width=\linewidth]{transfer_figs/Slide16.jpeg}
  \caption[Kernel Methods Example.]{An example of a how a kernel method will split the data in a 2-dimensional example. Examples with higher dimensions are much more difficult to depict on paper but work in the same way.  Source: Author generated.}
  \label{fig:svm}
\end{figure}

\paragraph{Kernel Methods.} The classic example in this class is the support vector machine (SMV). This algorithm accepts the numeric data and maps the data to find a distinction between the groupings. For instance, if each piece of data consists only of two attributes, this can be graphed on a page (a 2-D vector space). Once all the data points have been plotted, a decision boundary can be formulated by finding a line that minimises the distance between itself and the two groups of data. See Figure \ref{fig:svm} for a graphical example. Data with more than two attributes uses the same process but in higher dimensions of vector space.

The name kernel comes from a statistical process that reduces the computational power required by not requiring the plotting of all points in a vector space, but rather by allowing the distance between all point to be directly computed. This allows a swifter decision boundary formulation. SVMs can be susceptible to overfitting, and they have hyper-parameters that can balance the amount of misclassified instances with the simplicity of the computed boundary, which again leads to better generalisability from the model. 

\paragraph{Neural Networks and Deep Learning.} All the above models are considered shallow, meaning that they only carve the input space into very simple regions and find it difficult to pick up on underlying features in the data that should be invariant to simple changes. In the simpler shallow machine learning algorithms, this means that quite often, features have to be extracted or computed from the data, typically using expert domain knowledge. For instance, this could be done by using a dictionary or list of locally important words and aggregating them so that the presence of any one of these words has the same effect. As an example, a list of profanities can be used to judge if a comment is suitable to be published or not.

With deep learning and neural networks, layers of models can be stacked that automatically form features in the process, passing these features from one layer to the next and therefore skipping the need for time consuming feature engineering. Deep learning has produced some remarkable results across a host of machine learning tasks in recent years and is seen as one of the most powerful machine learning tools. However, for this remarkable performance, a higher cost needs to be paid in 1) the availability of training data (typically neural networks require more training data then simpler models), 2) computational power (they can often need specialist hardware to produce timely results) and 3) model explainability (often the process of decision is hidden within the model and can be difficult to extract). However, as these models are further developed, some of these higher costs are inevitably lowered as they become the focus of more research.


\section{Unsupervised} Unsupervised learning works in a different way than supervised learning. The algorithms attempt to ascertain the inherent structure of the data without any data labels. Unsupervised learning essentially attempts to group separate pieces of data according to the similarity between individual data points. The algorithms then split the data into similar groupings using these similarity measures so that similarities within groups can be identified.

With unsupervised learning, it may be the case that not all available variables are used for training. As an example, a company that might want to learn about customer behaviours may decide to exclude all demographic data (age, sex etc) from the model so that it only divides on purchasing history. The demographic data can later be used to explain the groups and possibly help with interventions. Had all the data variables been used from the outset, this may have introduced proxy measures for the outcome – known as data leakage – which would have affected the accuracy of the models. Two important methods of unsupervised learning are dimensionality reduction and clustering.


\paragraph{Dimensionality Reduction.} Data can have many explanatory variables and attributes, and the values are unlikely to be independent of one another. Dimensionality can combine variables using different weights to help condense the amount of variables (or dimensions) in the data to make it clearer what the most important ones are. Principal component analysis is a popular method for dimensionality reduction that has its roots in the mathematical community. Essentially, this technique recombines all the data in such a way that its dimensions are newly aligned to explain the most variation. Thus, by picking the most important new directions, the data set can be understood in a smaller number of dimensions or variables without significant loss of information. The trade-off is that not all the variation in the data is used, but what is used can be more easily explained and so the underlying causes understood.


\paragraph{Clustering.} Perhaps the most popular unsupervised technique is clustering. Clustering seeks to group the data into different regions given its attributes. One of the most popular clustering algorithms is k-means clustering, which seeks to cluster the data into k different clusters. The algorithm works by selecting k random points in the vector space (the vector space dimensionality is defined by the number of attributes or explanatory variables), then computing distance measures to allocate each data point to a group. Group centres are then recalculated, and distances remeasured, and this continues until the tightest clusters are discovered. The k, how many clusters to use, must be provided to the algorithm at the outset, but is typically not known. k can either be found through running variants of k and finding the ’best’ one or by using hierarchal clustering or expert knowledge. Once clusters have been found, these are then explored to deduce statistical characteristics, or as mentioned above, they can be combined with other data to provide a richer picture.


\section{Semi-supervised Learning}.In between supervised and unsupervised learning is semi-supervised learning. This is a type of learning that uses labelled data, but the data has not been labelled by humans. Typically, the label is known because it is inherent to the data. For example, semi-supervised learning on text data can occur through word prediction. A complete sentence has a word randomly chosen and masked, the machine is then given the sentence, complete with the word gap. and it must guess the masked word. The masked word has a label  –  its actual value  –  and so it is supervised learning, but the label has not been generated by a human, so it is a much less laborious process. Later, it is shown that semi-supervised learning is one of the pillars that has led to the production of PTMs by allowing models to efficiently learn from huge datasets with little human intervention.

\begin{figure}
  \includegraphics[width=\linewidth]{transfer_figs/Slide1.jpeg}
  \caption{A Summary of different labelling strategies. Source: Author generated.}
  \label{fig:label}
\end{figure}

\section{The Labelling Burden.} The key difference between the two main learning methods outlined above is data labelling. Labelling data is not a trivial endeavour though it is often worthwhile.\textcite{castelli1995exponential}  have shown that labelled data examples are worth exponentially more than unlabelled examples (that is, in certain circumstances, they are able to reduce the probability of error exponentially over the same number of unlabelled examples), so even though they are more difficult to come by, they will almost certainly require resources to generate, and it is often worth labelling data to achieve a better outcome in the long run. However, this requires an initial investment of resources, investment in a model that may not work or produce the results wanted. Also problematic is labelling data for fluid problems. What may seem like valid data labelling initially may no longer be so after the problem has morphed. This problem is not new, and many scholars and practitioners have been at work trying to lower the labelling burden. As can be seen in Figure \ref{fig:label} there are a number of strategies that can be employed to reduce the labelling burden, with trade resource utilised for overall accuracy. These methods are explained below.



\paragraph{Brute Force.} This is hand-labelling all the data required. This will include the training set and the test set. It is normally done by humans, who can be employed in a variety of ways. Depending on the subject matter expertise required, the cost of labelling can vary considerably, the skills to label x-rays of fusions in spinal surgeries are almost certainly rarer, and therefore more expensive, than the ability to decide if a tweet is offensive or not. Humans are also not infallible, and they can be subject to biases  \parencite{kahneman2011thinking}, meaning that generally enough people need to be involved to gain a consensus – typically, this means at least three, but some datasets have employed more. However, the brute force system is generally the most accurate of all the measures\footnote{This relates to \say{out of the box} functionality, some data sets have been more accurately labelled by trained machine learning algorithms, (see \url{https://rajpurkar.github.io/SQuAD-explorer/}), but of course the models were first trained on human labelled data.}  - a fine luxury if you have the resources.

\begin{figure}
  \includegraphics[width=\linewidth]{transfer_figs/Slide2.jpeg}
  \caption[Pictorial example of active learning strategy.]{ Panel (a) shows two 1D normal distributions with means 0.3 and 0.7. Panel (b) is the same distributions highlighting those labelled with a random sampling strategy, and the thick black line is a plausible decision boundary. Panel (c) is the same distributions, but now the labelling has been completed in accordance with an active learning strategy. The thick black line is a plausible decision boundary based on this method. Source: Author generated}
  \label{fig:active}
\end{figure}


\paragraph{Active Learning.}  \say{The key idea behind active learning is that a machine learning algorithm can perform better with less training if it is allowed to choose the data from which it learns.} \parencite{settles2009active}. So how does a machine choose which data to learn from? Essentially, the machine is fed a small amount of labelled data, far less than one would hope to use in the normal run of things. The machine learns from this seed data and then assigns a probability to each data point, and a decision boundary is formed. Those data points that were difficult to decide upon, those that were close to the decision boundary, are then chosen for labelling by a human, and the cycle is repeated. See Figure \ref{fig:active} for a simple example. The benefit, as can be seen in Figure \ref{fig:active}, is that each actively labelled data point contributes much more information to the formation of the decision boundary than those selected at random. Selecting points far away from the boundary generally has little effect on the decision boundary, and so for the same labelling resource, less information is achieved. While this is a simple one-dimensional example, it can be scaled to more complex environments with more sophisticated techniques, but the principles remain largely the same.

\paragraph{Transfer Learning.} \say{Transfer learning is used to improve a learner from one domain by transferring information from a related domain.} \parencite{weiss2016survey,}. Transfer learning is centred around using the knowledge gained from one data set, usually in the form or algorithmic rules, on a second, related data set. Typically, there is a resource hurdle for labelling the second data set that can be lowered by utilising the information from a data set that has already been labelled or curated such that the accuracy is known to be high. Examples of this include utilising language algorithms generated for one police force to help label the training data to be used with a second police force, or as we will come to see, transfer learning can also play an important part in key NLP model steps such as PoS tagging and word embedding, where a word is represented by a vector of numbers that reflects its similarity to other words.


\paragraph{Data Programming.}  Data programming is a form of weak supervision where knowledge is used to guide the labelling of data through the application of heuristics or simple rules. Snorkel, \parencite{ratner2017snorkel}, is an example of this type of modelling that takes simple rules developed by SMEs, then combines and weights these rules to automatically produce labels for data points. An example of a simple rule might be \emph{Text contains \say{victim knew offender}} or drawing on a dictionary of known relationships (dict:relationships) the rule might be \emph{Text contains \say{Offender is victim's ( word in dict:relationship)}}. These rules are not tested against labels, but each other to identify where there is agreement and correlation ( too much correlation is bad as it essentially over emphasises the same relationship), rules are then weighted and labels generated. It was found in \textcite{ratner2017snorkel} that time spent generating rules was much more efficient than time spent labelling data, but that did depend on subject matter expertise and rule writing proficiency of the individual authors.


In summary, labelled data for machine learning algorithms is a good thing and can be exponentially beneficial for providing the information sort. However, it is difficult to come by, especially in niche fields where the skills needed to label the data are scarce. Other fields where the questions are more fluid will also encounter labelling issues as, potentially, the data set has to be re-labelled for each purpose, unless the underlying representations can be unlocked. However, a body of research that is developing techniques to lower the labelling burden, without much reduction in overall accuracy, is encouraging. There will always be a requirement to label some data – if only to test that the model is working correctly – but speeding up the process and lowering the hurdle for entry will enable more powerful machine learning techniques to be used.


\section{Predicting Performance} Once a machine learning model has been trained, it is generally then tested on unseen data to understand how good it will be on unseen instances of data. For this research, the models will be used for classification tasks, and so prediction performance will be explored here in that context.

\begin{equation}
Accuracy =  \frac{(TP+TN)}{(TP + TN + FP + FN)}
\label{eqn:acc}
\end{equation}

\begin{equation}
Recall =  \frac{(TP)}{(TP + FN)}
\label{eqn:recall}
\end{equation}

\begin{equation}
Precision=  \frac{(TP)}{(TP + FP)}
\label{eqn:prec}
\end{equation}

\begin{equation}
F1 =  \frac{(2 * TP)}{(2*TP + FP + FN)}
\label{eqn:f1}
\end{equation}

\begin{equation}
MCC =  \frac{(TP*TN – FP*FN)}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}
\label{eqn:mcc}
\end{equation}

Where: TP = True Positive, TN = True Negative, FP = False Positive and FN = False Negative.

The simplest form of metric for predictive performance is Accuracy (a capital \say{A} is used to differentiate the metric from the everyday usage). The equation for Accuracy is given in Equation \ref{eqn:acc}. Essentially, it is the percentage of all correct predictions divided by the total number of elements to be predicted. Accuracy is easy to understand but can sometimes conceal poor performance when the dataset is imbalanced. An imbalanced data set is where one of the classes to be predicted is rare in relation to the other class. For example, imagine trying to predict a crime like domestic abuse when only 1 in 100 crimes are domestic abuse. A classifier that pays no attention to the data and classifies everything as not-domestic abuse would get an Accuracy of 99\%, which is high, but the model is poor, because it will never find any domestic abuse crimes.

When the data has imbalanced classes then it is important to use other metrics in place of Accuracy. Researchers have found two metrics that are useful to tracking classification tasks, \say{Precision} and \say{Recall} \parencite[Chapter~5]{witten_frank_hall_pal_2017}. Precision (Equation \ref{eqn:prec}) is a measure of the relevant instances amongst the retrieved instances and Recall (Equation \ref{eqn:recall}) is a measure of how many relevant instances were retrieved. These two measures typically tend to be inversely related as selecting more of the relevant instances increases the chances of selecting irrelevant instances. For that reason the F1 measure was invented ( Equation \ref{eqn:f1}), this takes the harmonic mean of the recall and the precision and is therefore a combined measure of both of those metrics. 

Further research \parencite{chicco2020advantages} has shown however that the F1 measure can still be misleading and that a more intricate measure - The Mathews Correlation Coefficient - can be more effective at discriminating between classifiers. The major differences between MCC and the F1 score is that MCC is invariant to class change (so if the classes are swapped for each other there is no change in the MCC metric) and secondly but relatedly the F1 score does not account for True Negatives (classifying irrelevant instances as irrelevant).  For these reasons the MCC metric will be adopted throughout this research as the primary means of assessing model performance. 
 

\section{A General Approach to ML} Having introduced the various aspects of machine learning this section will specify the general approach for supervised machine learning as that will be the approach throughout this research. The approach is therefore as follows:

\begin{enumerate}
\item{Split the data.} The data is randomly split into three sets: train, validation and test. The train set is the data that the model will be trained on. The validation set is used to help select the correct hyper parameters for the model. The test set is the data that the model performance is judged upon after the final model selection.
\item{Label the data.} All data in each set are read and labelled by human annotators.
\item{Train the model} The model is trained on the labelled data. Hyper parameters are selected, and the effects are judged using the validation set. In a sense, the validation set is an intermediary test set that helps select hyper parameters.
\item{Test the model.} Once the model has been trained with the final hyper parameter selection, the test set is predicted, and the model-generated labels are compared against the human labels to judge the model performance.
\end{enumerate}

\section{Machine Learning Limitations} Machine Learning has seen a surge in utility in the last decade or so as processing power and data sets have become increasingly available. However, it is not without some drawbacks and issues that can hamper its effectiveness or utility in certain scenarios. Some of these major limitations are explored below.

\begin{figure}
  \includegraphics[width=\linewidth]{transfer_figs/Slide4.jpeg}
  \caption[Pictorial example of Overfitting.]{Pictorial example of Overfitting. Panel (a) shows two 1D normal distributions with means 0.3 and 0.7. Panel (b) is the same distributions with an overfitted decision boundary (black line). Panel (c) is the same distributions, but the thick black line is a plausible decision boundary based on the known distributions (it is slightly left of 0.5 as the red class has a lower variance). Source: Author generated.}
  \label{fig:overfit}
\end{figure}



\subsection{Overfitting.}\say{ The fundamental issue in machine learning is the tension between optimization and generalisation} \parencite{chollet_allaire_2018}. Overfitting in machine learning is where the algorithms have been optimised for the training data, but in doing so have over generalised and have therefore lost some of the prediction power on the data set in general. Every data set has some natural variation that is typically included in the error term. In normal statistical equations, this natural variation is variation in the data that is derived from explanatory variables that are not in the model or interactions of existing variables that are not modelled correctly. When a model over fits, it is essentially predicting this variation from the existing model, but without the mechanisms or information to do so, so it is learning incorrect relationships.  

Figure \ref{fig:overfit} shows pictorially how this may occur, the distributions in panel (a) are random samples from two different normal distributions with separate means and standard deviations. Predictably, there is an overlap between the points, but knowing the distributions makes it possible to mathematically deduce, using probability theory, a decision boundary that will map a line whereby on one side of the line, the probability of a red data point is higher than that of a blue, and on the other side, the converse is true. That is, the optimal decision boundary is known. However, in general, the machine learning algorithms do not have the specified distributions and have to fit on the data provided. Therefore, depending on how much the algorithms value getting every data point classified correctly over the simplicity or generalisability of the rules will depend on how susceptible it is to overfitting. Some techniques to prevent overfitting include the following:


\begin{enumerate}

\item{Have a test set.} It is best practice to split available data right at the outset into a test set and a train set. The train set is set aside and is only used at the end to evaluate performance on the chosen model. It is not used to train models or select models.

\item{Get more data.} The more data one has, the more likely the true patterns are to be found.

\item{Divide the data.} A typical technique here is cross-validation, whereby the train data is randomly split into, typically, ten different groups, and then the model is trained on nine of these groups at a time (a different group of data is left out on each occasion). The resulting models are then tested on the left-out data group, and the results compared and analysed to pick the best generalising model.

\item{Restrict the model.} Do not allow the model to form overly complex rules. This can take the form of only allowing so many branches on a decision tree or by requiring a certain smoothness to a decision boundary in a probabilistic model.

\end{enumerate} 


\subsection{Explainability.} \say{In general, humans are reticent to adopt techniques that are not directly interpretable, tractable and trustworthy.}  \parencite{arrieta2020explainable}. Being able to understand how an algorithm works is important for several reasons, primarily among them being the trust that the end user will place in its predictions. The ability to understand why a decision is made greatly increase the confidence in it. Understanding how a decision was made can also have additional benefits, including ensuring impartiality of decision making, robustness to new data and identification of causality between the variables and the resulting class.

Explainability is relative to the audience: what might make sense to one person may not make sense to another. In general, if a problem is complex, then more complex algorithms lead to more accurate predictions  \parencite{arrieta2020explainable}. This has obvious implications for those who wish to use machine learning, who have complex problems but also a mandate to understand how the predictions were formulated and what bias, if any, are in the system. A field called explainable artificial intelligence (XAI)  \parencite{gunning2019xai} has developed to try and quantify these questions and develop a suite of tools to aid the model builders and the users in understanding their predictions better. However, as the authors of  \parencite{gunning2019xai}  acknowledge, how to reliably and consistently measure a good explanation is still an open research question, not least because the standard and style of the explanation can differ between intended audiences for the same model as well as across models and domains.

In his seminal paper on explanation in AI \parencite{miller2019explanation} Miller gives four major factors for good explanations. First, explanations should be contrastive – they should explain the output of a single instance by contrasting with hypothetical counterfactual cases. For text, this could be changing words within the sentence. Second, the explanation should be selective: the explanation should not try to list every cause of a generated output, just the most important. Third and perhaps the most upsetting to a statistician,  \say{probabilities probably don't matter} , referring to probabilities is less impactful than referring to causes. Last, Miller states that explanations are social, and thus they are contextual relative to the understanding and competence of the explainee.

A popular tool  for interrogating machine learning models is LIME, \parencite{ribeiro2016should}. LIME  builds a simpler local model around a prediction to help draw out the locally important factors for a single instance of data classification. These individual models can then be aggregated to provide a view across a larger dataset.  LIME will be used in the studies within this research and is explained more fully in the Methods chapter. This tool relies on the contrastive model as set out by Miller \parencite{miller2019explanation}. The output of the model can be adjusted or presented in different ways so that the remaining elements of a good explanation can be met, in particular tailoring the explanation to the audience.

\subsection{Bias} Machine learning systems can have bias making them unfair, where unfairness is \say{prejudice or favoritism toward an individual or group based on their inherent or acquired characteristics.} \parencite{mehrabi2021survey}.). Clearly, bias in a ML system is sub-optimal, as it can lead to groups being discriminated against and a reduction in trust in that model and other AI systems. Bias in ML systems stems from two main areas: the data and the algorithm. These two main areas are explored to see how they may introduce bias.

\subsubsection{Data Bias} Data bias can come from a number of different sources, the most important being the following two. 1) representation bias. This can be where the sampling of the population has not been completed in a representative way. Police data suffers from this bias, as recorded crime is not recorded uniformly across victim and crime types  \parencite{baumer2002neighborhood, tarling2010reporting }.2) Omitted variable bias. This occurs when important variables are omitted from the data. Within police text data, this could be observed if certain events are not mentioned in the texts to be analysed. Other sources include aggregation bias, where rare but distinct groups have inferences drawn about them that are derived from population characteristics, and measurement bias, where the quantity and quality of measurement can vary between groups.

\subsubsection{Algorithmic Bias} \say{Algorithmic bias is when the bias is not present in the input data and is added purely by the algorithm } \parencite{mehrabi2021survey}. That is the bias is introduced by the choices the researchers makes in the selection of model types and parameters \parencite{hooker2021moving}.Some models are better at some problems than others. Tuning hyper parameters are also likely to bias in favour of correctly predicting certain instances over others \parencite{paiva2022relating}. In relation to crime text data, there may be rare words in certain crime types that may not be represented well with the models chosen and therefore may lead to inappropriate classifications. This could introduce bias with certain crime or victim types by the selection of the algorithm.

\subsubsection{Measuring bias} We have seen that bias can stem from two main areas, the data or the algorithm. Similarly bias can present in two main areas \parencite{chouldechova2017fair}. Firstly predictive accuracy, do the results from the ML system have the same accuracy across different groups? Secondly when the ML system makes mistakes are those errors equally likely across different groups of people. A well studied example of bias in the literature is that of COMPAS a system used in the USA to predict recidivism rates. This example also allows a better understanding of the two different biases \parencite{kleinberg2016inherent, chouldechova2017fair}.  ProPublica, an investigative journalism group, produced research that showed that error rates with the COMPAS system meant that members of the black population were more likely to be misclassified as high-risk offenders, and white people were more likely to be misclassified as low-risk offenders   \parencite{jefflarson_2016}. Northpointe, the providers of the tool, countered this claim with evidence that showed that the accuracy for prediction across racial groups was similar, in that regardless of racial group, the accuracy of predicting high or low recidivism rates was the same.

Further research  \parencite{kleinberg2016inherent, chouldechova2017fair} not only showed that both pieces of evidence were true, but that they were almost inevitable in a system where the underlying rates are different between different groups (in this case the data used (itself not without inherent biases), has different recidivism rates for the white and black populations).  Therefore, in one sense, there was no bias, because the COMPAS system had the same accuracy across racial groups. However, when looking at the second source of bias, the errors, it was shown that the system was biased, as the direction of the errors was different for the two racial groups, with black people being more likely to be classified as high-risk offenders when they were not and therefore subject to more punitive measures. However, as shown in \textcite{kleinberg2016inherent}, with underlying differences in the recidivism rate for the two groups an unbiased error rate is not possible (except in the case of a perfectly accurate system). 

So what? First, measuring bias is not straightforward and looking at single measures can skew interpretation. Second, understanding the impact of the bias is also crucial, as inaccurate predictions in one direction can be more costly than in another direction. Third, where different underlying rates are recorded a perfectly unbiased system is not possible in practice \parencite{kleinberg2016inherent}. An excellent overview of this problem and its interpretation is given in \parencite{hellman2020measuring}. For this research and measuring the bias, I will therefore measure both the bias in accuracy and the bias in error rates. The two metrics are \emph{predictive parity} and \emph{equality of outcome} respectively. These metrics will be formally introduced in the methods chapter.  



\section{Summary}

This chapter has introduced the broad concepts surrounding machine learning. The chapter explored the main paradigms of machine learning, how they operate, what they require and how success is measured. Important limitations for machine learning include overfitting to the training data, the degree to which models can be explained and any biases they may contain. The research in this thesis is largely based on supervised learning and so requires labelled data. Active learning is used to label the data, and performance is judged through the MCC metric. Further applicability of the models is explored by using the LIME tool to explain how the models came to their decisions, and bias metrics are used to explore bias in the system.

The next chapter moves on to a specific section of machine leaning, NLP. NLP is used when the data to be analysed is textual data. The next chapter takes the concepts explored here and shows how they can be built upon for analysing free text data.
