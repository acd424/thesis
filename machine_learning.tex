
\chapter{Machine Learning}

\begin{figure}
  \includegraphics[width=\linewidth]{transfer_figs/Slide7.jpeg}
  \caption[Machine learning diagram.]{A diagram to show machine learning. The inputs into the machine learning process are 1) the data and 2) the outcome of interest - commonly referred to as the label. The output from the process is a set of rules that can then be used to extrapolate to other, similar, data sets to make predictions. The rules can also be used to understand relationships with in the original data. Adapted from  \textcite{chollet_allaire_2018} and \textcite{provost2013data} }
  \label{fig:ML}
\end{figure}


\say{Machine Learning: Procedures for extracting algorithms, say for classification, prediction or clustering from complex data} \parencite{spiegelhalter2019art}

\say{With Machine Learning, humans input data as well as the answers expected from the data, and out come the rules} \parencite{chollet_allaire_2018}


As we see from the quotes above, Machine learning is about finding a set of rules or an algorithm that allows you to understand the structure of the data. That is the overarching aim of machine learning - to discover a model or a set of algorithms or rules that assist in explaining the data. The real goal for those who use machine learning though is often to then take these rules and use the information that they provide against other sets of data to make predictions about unknown quantities. A toy example, Figure \ref{fig:ML},  is predicting customer churn in a business \parencite{provost2013data}. Using historical data about customers, some known attributes like income and age can be used to try and explain the object of interest, have they left the company or not? A machine learning algorithm can generate a set of rules to predict whether those historical customers left or not. These discovered rules can then , if the conditions are similar, be extrapolated to another set of data to predict who will leave in the future.    


As shown above, the core of machine learning is about learning rules from data, however the application of those rules to more data is generally where the main interest lies in the application of machine learning. Once trained, these machines produce algorithms and rules can then be used against unlabelled data to generate additional labels - at a much reduced resource intensity but without, hopefully, a significant reduction in accuracy. In this way machine learning generates rules, that can then be used to automatically extract information from wider sets of data. Extracting information with machine learning algorithms can be classified into two broad categories, (though in truth it is more a continuum) the two categories are supervised and unsupervised  learning. Two other important categories include self-supervised learning and reinforcement learning, but they will not be examined here \parencite{chollet_allaire_2018}


\section{Supervised Learning} The key component of supervised learning is that the input data has already been labelled with information that already puts them into their desired class, Figure \ref{fig:ML} is an example of supervised learning. For instance a data set of words may already have labels such as 'verb' or 'noun'. These labels are then used by the machine to being to build rules to classify the data inputs. The final ingredient required for success in machine learning is a measure of whether the rules are doing a good job or not. This can be a measure as simple as accuracy(what \% were correct) to more complex calculations that can account for some permissible variation between given label and generated label. This success measure can then be used by the algorithm to select correct decision points and rules to improve the final rules. These final rules are then applied to unlabelled data and the hope is that they are able to label the new data with similar accuracy( although almost certainly with lower accuracy).  \textcite{chollet_allaire_2018} identify four basic approaches to supervised machine learning, these are discussed below with examples:

\paragraph{Probabilistic Modelling.} This style of model, of which Naive Bayes is the most widespread, attempts to find the probability of each potential classification \emph{given the data inputs}. It is worth noting here that the input data for machine learning typically consists of a set of attributes (akin to explanatory variables)  and as previously mentioned a data label (akin to a dependant variable). There are no real restrictions on the type of data that these attributes or labels can take. They can be discrete data, names or labels, or they can be continuous data such as rational numbers. The style of data one has though will help to determine which algorithm to choose.  Naive Bayes treats each attribute as equally important and independent from the other attributes, and using Bayes' theorem will calculate a probability for each potential label. The benefit with this algorithm is that it the actual probability is not as important as the probabilities in relation to each other, that is it is the relative size of the generated probabilities as the label with the largest probability is selected as the prediction. Another popular method in this class is Logistic Regression, which is also used to generate probabilities of a certain classification. 

\begin{figure}
  \includegraphics[width=\linewidth]{transfer_figs/Slide8.jpeg}
  \caption[Decision tree example.]{An example of a simple decision tree. The Tree first splits on the age of the customer as that produces a homogenous grouping. The tree further splits the left hand grouping by income so that all groups are now homogenous. Source: Author generated }
  \label{fig:tree}
\end{figure}

\paragraph{Divide and Conquer.} This type of algorithm is typified by the decision tree. Here an attribute is first selected, probably at random, then the attribute is stratified to split the data with the aim of partitioning the data as homogeneously as possible into sub-groups based on the given labels. These sub groups are then further split by either the same attribute (but with different stratification) or by other attributes, see Figure \ref{fig:tree} for a toy example. This can continue until certain conditions have been met at which point the algorithm stops and a set of rules has been generated. If the algorithm goes on for too long then there is a risk that each data point will have its own set of perhaps long and complicated rules generated. These rules may define the training data well, but may not transfer well to other similar data that we may want to subsequently label. This process is known as over-fitting and is a flaw in machine learning algorithms that needs to be guarded against if the desire is to produce rules with a level of generalisability. 


In the case of decision trees, the maximum depth of a tree may be specified beforehand so that long and complicated rules can be avoided. This specification is an example of a hyper-parameter - that is an additional guide to the formation of the algorithm that limits the possible set of rules that can be generated. Hyper-parameters can have a dramatic effect on the end result of an algorithm and it is usually good practice to try a variety of hyper-parameters when working through problems to explore result sensitivity.  More sophisticated models in this class include random forests  which use lots of smaller decision tress, randomly generated,  then combined together to produce a single result. The benefit of this is that they can avoid \emph{local minima} whereby a normal decision tree is led by its procedure into a non-optimal path. More sophisticated still are gradient boosting trees that, only try to predict the actual data once, then spend the reminder of their time trying to minimise the residuals from the previous models with additional trees.  

\begin{figure}
  \includegraphics[width=\linewidth]{transfer_figs/Slide16.jpeg}
  \caption[Kernel Methods Example.]{An example of a how a kernel method will split the data in a 2-dimensional example. Examples with higher dimensions are much more difficult to depict on paper, but work in exactly the same way. Source: Author.}
  \label{fig:svm}
\end{figure}

\paragraph{Kernel Methods.} The classic example in this class is the Support Vector Machine. This algorithm accepts the data, numeric, and maps the data to find a distinction between the groupings. For instance if each piece of data consists only of two attributes, then this can be graphed on a page (a 2-D vector space). Once all of the data points have been plotted then a decision boundary can be formulated by finding a line that minimises the distance between itself and the two groups of data. See Figure \ref{fig:svm} for a graphical example. Data with more than two attributes uses the same process, but in higher dimensions of vector space. 

The name kernel comes from a statistical process which reduces the computational power required by not requiring the plotting of all points in a vector space, but rather by just allowing the distance between all point to be directly computed. Thus allowing a swifter decision boundary formulation. SVMs can be very susceptible too overfitting and they have hyper parameters that allow can balance the amount of misclassified instances with the simplicity of the computed boundary - this again leads to better generalisability from the model. 

\paragraph{Neural Networks and Deep Learning.} All of the above models are considered to be shallow models, meaning that they only carve the input space into very simple regions, and find it difficult to pick up on underlying features in the data that should be invariant to simple changes. In the simpler shallow machine learning algorithms this means that quite often features have to be extracted or computed from the data typically using expert domain knowledge. This could be for instance done by using a dictionary or list of locally-important words and aggregating them so that the presence of anyone of these words has the same effect. As an example a list of profanities can be used to judge if a comment is suitable to be published or not. 

With deep learning and neural networks layers of models can be stacked that automatically form features in the process, passing these features from one layer to the next and therefore skipping the need for time consuming feature engineering.  Deep learning has produced some remarkable results across a whole host of machine learning tasks in recent years, and is seen as one of the most powerful machine learning tools. However for this remarkable performance a higher cost needs to be paid in 1) the availability of training data (typically neural networks require more training data then simpler models) 2) computational power (they can often need specialist hardware to produce timely results) and 3) model explainability (very often the process of decision is hidden within the model and can be difficult to extract). However as these models are further developed some of these higher costs are inevitably lowered as they become the focus of more research. 


\section{Unsupervised} Unsupervised learning works in a different way to supervised learning. The algorithms attempt to ascertain the inherent structure of the data without any data labels. Unsupervised learning essentially attempts to group separate pieces of data, according to the similarity between individual data points.  The algorithms then split the data into similar groupings using these similarity measures so that similarities within groups can be identified. 

With unsupervised learning it may be the case that not all available variables are used for training. As an example a company that might want to learn about customer behaviours, may decide to exclude all demographic data (age, sex etc) from the model so that it only divides on say purchasing history. The demographic data can later be used to explain the groups and possibly help with interventions. Had all of the data variables been used from the outset this may have introduced  proxy measures for the outcome - known as data leakage - which would have affected the accuracy of the models. Two important methods of unsupervised learning are dimensionality reduction and clustering.


\paragraph{Dimensionality Reduction.} Data can have many explanatory variables an attributes and the values are unlikely to be independent of  one another. dimensionality can combine variables, using different weights, to help condense the amount of variables (or dimensions) in the data to make it clearer what the most important variables are. Principal Component Analysis is a popular method for dimensionality reduction which has it roots firmly in the mathematical community. Essentially this technique recombines all of the data in such a way that the dimensions of the data are newly aligned to explain the most variation, thus by picking the most important new directions, the data set can now be understood in a smaller number of dimensions or variables without significant loss of information. The trade-off is that not all of the variation in the data is not used, but what is used can be more easily explained and so the underlying causes understood.


\paragraph{Clustering.} Perhaps the most popular unsupervised technique though is clustering. Clustering seeks to group the data into different regions given its attributes. One of the most popular clustering algorithms is k-means clustering, which seeks to cluster the data into k different clusters. The algorithm works by selecting k random points in the vector space ( the vector space dimensionality is defined by the number of attributes or explanatory variables) then computing  distance measures to allocate each data point to a group, group centres are then recalculated and distances remeasured, this continues until the tightest clusters are discovered. The k, how many clusters to use, must be provided to the algorithm at the outset, but is typically not known. k can either be found through running variants of k and finding the 'best' one or by using hierarchal clustering or expert knowledge. Once clusters have been found these are then explored to deduce statistical characteristics, or as mentioned above, they can be combined with other data to provide a richer picture.

\begin{figure}
  \includegraphics[width=\linewidth]{transfer_figs/Slide1.jpeg}
  \caption{A Summary of different labelling strategies. }
  \label{fig:label}
\end{figure}

\section{The Labelling Burden.} The key difference between the two methods outlined above is data labelling. Labelling data is not a trivial endeavour though it is often worthwhile.\textcite{castelli1995exponential}  prove that labelled data examples are worth exponentially more than unlabelled examples (that is in certain circumstances they are able to reduce the probability of error exponentially over the same number of unlabelled examples) so even though they are more difficult to come by, they will almost certainly require resources to generate, it is often worth labelling data to achieve a better outcome in the long run. However this requires initial investment of resources, investment in a model that may not work or produce the results wanted. Also problematic is labelling data for fluid problems, what may seem like valid data labelling initially may not be true after the problem has morphed. This problem is not new and many scholars and practitioners have been at work to try and lower the labelling burden, as can be seen in Figure \ref{fig:label}  there are a number of strategies that can be employed to reduce the labelling burden with trade resource utilised for overall accuracy. These methods are explained below.



\paragraph{Brute Force.} This is hand-labelling all the data required. This will include the training set and the test set. Normally done by humans, who can be employed in a variety of ways. Depending on the subject matter expertise required, the cost of labelling can vary considerably, the skills to label x-rays of fusions in spinal surgeries are almost certainly rarer, and therefore  more expensive, than the ability to decide if a tweet is offensive or not. Humans are also not infallible, they can also be subject to biases \parencite{kahneman2011thinking}, meaning that generally enough people need to be involved to gain a consensus - typically this means at least three people, but some datasets have employed more. However the brute force system is generally the most accurate of all the measures\footnote{This relates to \say{out of the box} functionality, some data sets have been more accurately labelled by trained machine learning algorithms, (see \url{https://rajpurkar.github.io/SQuAD-explorer/}), but of course the models were first trained on human labelled data.}  - a fine luxury if you have the resources.

\begin{figure}
  \includegraphics[width=\linewidth]{transfer_figs/Slide2.jpeg}
  \caption[Pictorial example of Active learning strategy.]{Pictorial example of Active learning strategy. Panel (a) shows two 1D normal distributions with means 0.3 and 0.7. Panel (b) is the same distributions highlighting those labelled with a random sampling strategy, the thick black line is a plausible decision boundary. Panel (c) is the same distributions but now the labelling has been completed in accordance with an active learning strategy. The thick black line is a plausible decision boundary based on this method. Self generated. Source: Author generated}
  \label{fig:active}
\end{figure}


\paragraph{Active Learning.}  \say{The key idea behind active learning is that a machine learning algorithm can perform better with less training if it is allowed to choose the data from which it learns.} \parencite{settles2009active}. So how does a machine choose which data to learn from? Essentially the \emph{machine} is fed a small amount of labelled data, far less than one would hope to use in the normal run of things. The machine learns from this seed data and then assigns a probability to each data point, and a decision boundary is formed.  Those data points that were difficult to decide upon i.e. they were close to the decision boundary are then chosen for labelling by a human and the cycle is repeated. See Figure\ref{fig:active} for a simple example. The benefit as can be sen in Fig\ref{fig:active} is that each actively - labelled data point contributes much more information to the formation of the decision boundary than those selected at random. Selecting points far away from the boundary generally has little effect on the decision boundary and so for the same labelling resource less information is achieved. Whilst this is a simple one-dimensional example it can be scaled to more complex environments with more sophisticated techniques, but the principles remain largely the same.

\paragraph{Transfer Learning.} \say{Transfer learning is used to improve a learner from one domain by transferring information from a related domain.} \parencite{weiss2016survey,}. Transfer learning is centred around using the knowledge gained from one data set, usually in the form or algorithmic rules, on a second related data set. Typically there is a resource hurdle for labelling the second data set that can be lowered by utilising the information from a data set that has already been labelled or curated such that the accuracy is known to be high. Examples of this might be utilising language algorithms generated for one police force to help label the training data to be used with a second police force, or as we will come to see transfer learning can also play an important part in key NLP model steps such as PoS tagging and word embedding, were a word is represented by a vector of numbers that reflects its similarity to other words.


\paragraph{Data Programming.}  Data programming is a form of weak supervision where knowledge is used to guide the labelling of data through the application of heuristics or simple rules. Snorkel, \parencite{ratner2017snorkel}, is an example of this type of modelling that takes simple rules developed by SMEs, then combines and weights these rules to automatically produce labels for data points. An example of a simple rule might be \emph{Text contains \say{victim knew offender}} or drawing on a dictionary of known relationships (dict:relationships) the rule might be \emph{Text contains \say{Offender is victim's ( word in dict:relationship)}}. These rules are not tested against labels, but each other to identify where there is agreement and correlation ( too much correlation is bad as it essentially over emphasises the same relationship), rules are then weighted and labels generated. It was found in \textcite{ratner2017snorkel} that time spent generating rules was much more efficient than time spent labelling data, but that did depend on subject matter expertise and rule writing proficiency of the individual authors.


In summary, labelled data for machine learning algorithms is a good thing, and can be exponentially beneficial for providing the information sort, however it is difficult to come by especially in niche fields where the skills to label the data are scarce. Other fields where the questions are more fluid will also encounter labelling issues as, potentially the data set has to be re-labelled for each purpose, unless the underlying representations can be unlocked. What is encouraging though is a body of research that is developing techniques to lower the labelling burden, without much reduction in overall accuracy. There will always be a requirement to label some data - if only to test that the model is working correctly - but by speeding up the process and lowering the hurdle for entry will enable more powerful machine learning techniques to be used.

\section{Issues with Machine Learning} Machine Learning has been used to excellent effect and has seen a surge in utility in the last decade or so as processing power and data sets have become increasingly available. However, it is not without some drawbacks and issues that can hamper its effectiveness, or utility in certain scenarios. Some of these major drawbacks or issues are explored below.

\begin{figure}
  \includegraphics[width=\linewidth]{transfer_figs/Slide4.jpeg}
  \caption[Pictorial example of Overfitting.]{Pictorial example of Overfitting. Panel (a) shows two 1D normal distributions with means 0.3 and 0.7. Panel (b) is the same distributions with an overfitted decision boundary (black line). Panel (c) is the same distributions but the thick black line is a plausible decision boundary based on the known distributions (it is slightly left of 0.5 as the red class has a lower variance. Source: Author generated}
  \label{fig:overfit}
\end{figure}



\paragraph{Overfitting.}\say{ The fundamental issue in machine learning is the tension between optimization and generalisation} \parencite{chollet_allaire_2018}. Overfitting in machine learning is where the algorithms has been optimised for the training data, but in doing so have over generalised and have therefore lost some of the prediction power on the data set in general. Every data set has some natural variation that is typically included in the error term in normal statistical equations, this natural variation is variation in the data that is derived from explanatory variables that are not in the model, or interactions of existing variables that are not model correctly. When a model over fits it is essentially predicting this variation from the existing model, but without the mechanisms or information to do so, so it is learning incorrect relationships. Figure \ref{fig:overfit} shows pictorially how this may occur, the distributions in panel (a) are random samples from two different normal distributions with separate means and standard deviations. As we would expect there is an overlap between the points, but knowing the distributions we can mathematically deduce, using probability theory, a decision boundary that will map a line whereby on one side of the line the probability of a red data point is higher than that of a blue and on the other side the converse is true. That is we know the optimal decision boundary. However in general the machine learning algorithms do not have the specified distributions and have to fit on the data provided. So we see that depending on how much the algorithms value getting every data point classified correctly, over the simplicity or generalisability of the rules, will depend how susceptible it is too overfitting. Some techniques to prevent overfitting include:
\begin{enumerate}

\item{Have a test set.} It is best practice to split available data right at the outset into a test set and a train set. The train set is set aside and is only used at the end to evaluate performance on the chosen model, it is not used to train models or select models.

\item{Get more data.} The more data one has the more likely the true patterns are to be found.

\item{Divide the data.} A typical technique here is cross-validation, whereby the train data is randomly split into typically ten different groups, then the model is trained on a nine of these groups at a time (a different group of data is left out on each occasion). The resulting models are then tested on the left-out data group, and the results compared and analysed to pick the best generalising model.

\item{Restrict the model.} Do not allow the model to form overly complex rules, this can take the form of only allowing so many branches on a decision tree or by requiring a certain smoothness to a decision boundary in a probabilistic model. 

\end{enumerate} 


\paragraph{Interpretability.} \say{In general, humans are reticent to adopt techniques that are not directly interpretable, tractable and trustworthy.}  \parencite{arrieta2020explainable}. Being able to understand how an algorithm works is important for a number of reasons, primarily amongst them is the trust that the end user will place in its predictions. The ability to understand why a decision is made greatly increase the confidence in it - understanding how a decision was made can also have additional benefits including ensuring impartiality of decision making, robustness to new data and identification of causality between the variables and the resulting class. Interpretability is relative to the audience, what might make sense to one person, may not make sense to another. In general, if a problem is complex, then more complex algorithms lead to more accurate predictions \parencite{arrieta2020explainable}. This has obvious connotations for those who wish to use machine learning, who have complex problems but also a mandate to understand how the predictions were formulated and what bias, if any, are in the system. A field called Explainable Artificial Intelligence (XAI) has sprung up to try and quantify these questions and develop a suite of tools to aid the model builders and the users in understanding their predictions better. A popular tool is LIME, \parencite{ribeiro2016should}, that builds a simpler local model around a prediction to help draw out the locally important factors for a single instance of data classification. Other more domain specific models, such as CheckList  \parencite{ribeiro2020beyond} are also becoming available. 

Although not specifically associated with NLP  \textcite{babuta2018machine} is a RUSI publication that explores the use of algorithms in a UK police context. Although the focus is on predictions of individuals' proclivity for future crime, rather than crime events themselves it highlights the lack of frameworks and direction from central policy makers that would be used to govern and direct the research in this area. Explaining the rational behind decisions to interested bodies has and will continue to be an important part of transparency which is one part of policing by consent, this transparency needs to be borne in mind with the adoption of different methods of decision support. 

