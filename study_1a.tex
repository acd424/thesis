\chapter{Study 1a: PF1 Burglary MO}
\section{Introduction}

The preceding chapters laid the groundwork for this study. Firstly, the rationale of the study was set out – it is intended to enable POP by identifying intra-crime variation through the use of free-text data. Thereafter, general theories of machine learning and NLP were laid out so as to preface the methods chapter and the general introduction to the data that are employed in the study. This chapter presents the first study of the thesis, and it focuses on the classification of burglary MO texts from the PF1 data. The study sets out to classify burglary MOs by reference to three factors. The first is car-key burglaries, the second is burglaries in which force is used, and the third is burglaries in which only an outbuilding is targeted. 

The last category was not examined because there were no outbuilding burglaries in the PF1 data. However, it is explored in the replication study (Study 1c). The next two chapters are also related to the present study. They cover 1) the effectiveness of the active learning strategy that is employed here (Study 1b) and 2) the replication of this study with data from a different police force (Study 1c).

The primary focus of this chapter is on the utility of PTMs for MO data. Utility is examined in the context of three questions. Firstly, can PTMs produce accurate results with police MO data? Secondly, can these results be achieved within a reasonable resource envelope? Thirdly, are the models acceptable for use, that is,  are they explainable or affected by bias?

This chapter begins with an overview of the problems and the process by which they were selected. The use of data is explained, and methods are reviewed briefly. One method is added to those that were introduced in the methods chapter, namely the keyword method. This addition facilitates a comparison between the PTM and current police practice. Finally, the results are presented and discussed.

\subsection{Problem overview} In order to test the utility of the PTMs a selection of representative problems had to be selected. Previous work in the field has focused on burglary data  \parencite{birks2020unsupervised, sheard2020developing} in order to explore effectiveness of NLP. This study follows that approach and again utilises burglary data to explore NLP effectiveness. The following sections set out the three classification problems that were identified as suitable for this study.

\begin{enumerate}
   
\item The first classification problem follows on from the work of \parencite{sheard2020developing}. In her thesis Sheard states that \say{this thesis presents empirical evidence that failure to disaggregate beyond official crime classifications risks neglecting heterogeneity of offence characteristics within these. A potential implication of this is that the spatio-temporal parameters on which some prevailing crime modelling techniques are based might not apply to all offences, meaning that any related decision-making could be misinformed}. Sheard investigated this matter by showing that car-key burglaries have a different spatial-temporal pattern from non-car-key burglaries. However, in the process, Sheard observed that differentiating between the two types of burglary is laborious and time consuming. She needed much time to complete the process because there were no encoded variables (flags) in the police data that would enable the crimes to be differentiated. More formally, this classification task entailed highlighting burglaries in which a motor vehicle had been stolen. The category of motor vehicles includes cars, vans, and motorbikes. Although the crimes in question are often called \say{car-key burglaries}, if only the keys are stolen. In this thesis the focus was on the the theft of the motor vehicle as the defining characteristic, not solely the keys.

 \item The second classification task originated from a discussion between Dr Daniel Birks and a Detective Chief Inspector (DCI) from Durham Constabulary. Burglaries in an area of Durham had spiked suddenly. A preliminary review of the problem led the DCI to believe that the spike could be attributed to an increase in the incidence of outbuilding burglaries. However, there were insufficient resources to test this hypothesis comprehensively because it would have been necessary to read the available textual information on all burglaries in order to establish a baseline and to identify a recent trend. This problem is representative of the tasks that a police force may wish to undertake. Burglaries of entire homes may be considered more harmful than burglaries that only target outbuildings. Consequently, police forces may wish to understand intra-crime variation and to allocate resources accordingly. The classification task, therefore, was to highlight burglaries in which only an outbuilding (and not a home) had been broken into; burglaries that had targeted both an outbuilding \emph{and} a home had to be excluded.


 \item The final classification task was designed to complement the first two. Both outbuilding burglaries and car-key burglaries are relatively rare (approximately 9\% of burglaries in the PF1 data involved theft of a motor vehicle), leading to imbalanced classes within the data. A complementary classification with more balanced classes is needed, that is, one in which the act is mentioned much more frequently than the theft of a motor vehicle. A review of a selection of MO texts resulted in the use of force to enter a building being chosen because almost every text appears to contain such information. Use of force is present in approximately 60\% of the MO texts, yielding a much more balanced problem. This classification task, therefore, focuses on the method of entry into a building and the use of force. The use of force within the home, for example to destroy furniture, is not included.

\end{enumerate}


The three classification tasks are intended to yield a superior understanding of intra-crime variation within the burglary MO data. These problems are representative of the tasks that an analyst or a police officer may wish to undertake. The next section summarises the data that are used to explore these classification tasks.


\section{Data} The data were introduced in Chapter 8. For this study only the burglary data from the PF1 data was used. The text data was taken from the \emph{Crimenotes} column. Examples of the MO texts can be found in Table \ref{tab:MOexample}. The PF1 data had already been processed, exact mechanisms unknown, to replace identifiable information with 'XXXXX'. No other data processing was undertaken. There were 9,961 burglary MO texts in total, spanning two years of data.

The data was split into three sets as previously mentioned. The test set was 200 randomly selected texts. The validation set was 200 randomly selected texts. The training set was selected using an active learning strategy. Active learning is explained in the methods chapter and the effectiveness explored separately in the next chapter. In total 1200 MO texts were labelled manually  for the test sets. 


\begin{table}[]
\begin{tabular}{p{0.9\linewidth}}
\toprule
MO 1 “Attacked property is mid town house with driveway to the front along with gardens to both front and rear located within a residential area. At time stated person/s unknown go to front door and open letter box and using unknown instrument hook door key from a shelf in the porch. Use same keys to open front door and gain entry remove two sets of car keys from the porch area. Go to a XXXX parked on the drive gain access using keys. Make off at speed with both vehicles direction of travel towsrds XXXX having been disturbed by the occupant.”             \\ \midrule
MO 2 “Attacked property is a large detached dwelling on a busy road. Property is surrounded by large fences, gates and bushes. Between times stated suspect approach rear patio doors at locus and attempt to gain entry by using mole grip type implement to snap lock. Lock snapped however unable to gain entry. Suspects then use molegrip type implement to snap lock on front porch door. Lock snapped, door opened and house alarm sounds. Suspects jump over wall at front of dwelling, get into vehicle parked opposite and make off down XXXXX in direction of XXXXX.” \\ \bottomrule
\end{tabular}
\caption[Example MO texts]{\label{tab:MOexample} An example of MO texts from PF1 Burglary data. Reproduced from \textcite{birks2020unsupervised}.}
\end{table}


\section{Methods} The methods of all of the studies were introduced in the methods chapter. Accordingly, this section only presents a brief outline and highlights points of divergence the stated method. 

\subsection{Labelling} The data were labelled by a single individual, the author. Data selection for the labelling of the test set was completed by using an active learning strategy. The batch size for active learning was 100. Each model  (“use-of-force”, “motor vehicle stolen”, and “outbuilding”) was labelled according to a separate active learning strategy. However, in order to generate more labelled instances and because the additional time cost was marginal, every MO that the author read was labelled for all catergories, independently of the applicable active learning strategy. For example, when running the active learning model for the use of force, the MO texts were selected by finetuning the force model. However, each MO text was labelled as “force”, “outbuilding”, or “motor vehicle stolen” when the author read it.

After several hundred MO texts had been read, it became clear that burglaries of outbuildings were not mentioned in any of the MO texts. For this reason, the analysis of that category was discontinued, and no models were finetuned. In total, 900 MO texts were labelled for the motor-vehicle model by using the active learning process, and 700 MO texts were labelled for the use-of-force model. In total, 1,500 burglary MOs were labelled (1500 = 900 + 700 - 100 (because both sets were based on same initial random selection)). The active learning process, and therefore data labelling, were terminated when the MCC for the validation set exceeded 0.9.

\subsection{Fine-tuning the PTM} Modelling was completed by using the BERT-large-uncased model. The model was utilised through the transformers package on Python, as explained in the methods section. All hyperparameters were set in the manner that was described in the methods Chapter. The hyperparameters were not tuned, except for the selection of epochs (5) for the final fine-tune. A total of 200 MO texts were used for the test set, and another 200 were used for the validation set.

\subsection{Performance} As described in the methods chapter, three aspects of performance are examined here. MCC is used for overall performance, LIME is used for explainability,  and extrinsic metrics are employed to examine bias.  Since no victim data are available, the statistical properties of the MO are used to determine whether its length and style of composition have implications for model classification.

In addition to these performance metrics, a fourth aspect was added for the purposes of this particular study. That addition entails comparing the PTM to the workflow that analysts use at present. To that end, a basic keyword search was conducted. The search could be completed relatively easily on readily accessible software such as Microsoft Excel. This keyword modelling process is explained below.


\subsubsection{Keyword Model.} The keyword model is based on simple searches for keywords that are likely to feature in the MOs. The model is designed to represent methods that police analysts and/or police officers use at present. Accordingly, it is designed to be relatively simple. The keyword model does not use complex rules in which words can be chained or their presence negated to develop more intricate searches. In essence, if an MO text contains a keyword, the model labels it as a positive example.

The keyword model was developed after reading a substantial number of MOs. Therefore the keyword search may be relatively good compared to one that would have been produced without that experience. The model reflects the manner in which a police analyst may approach their tasks - and so it should be assumed that they will have had some previous exposure to MO texts. The keyword list was built from words that were associated with burglaries where a motor vehicle had been stolen e.g. (car, motorbike). The keyword list was also made more robust to unseen data by adding in a list of popular car brands\footnote{https://yougov.co.uk/ratings/transport/fame/car-brands/all}, as the vehicle can often be referred to by brand name alone. The final list of keywords for the motor vehicle model can be found in Table \ref{tab:Keywords}, A similar process was used for the force model, and the final keywords for that model can be found in Table \ref{tab:Keywords_force}. The model was built in R, and it searches each MO description for all of the words on a list. If any of the keywords are present, the MO is labelled as being in the positive class for the corresponding classification model.

Although it was built in R, the model could easily have been created in Excel or through the use of SQL queries (SQL is a database manipulation language with which police analysts are often familiar). In order to fully explore the differences between the keyword model and the PTMs, it was important to track another metric, which is called “recall”. Recall was introduced in Chapter 4 and is also explained below. Time was also used as a proxy for effort in order to understand how the burden that analysts must shoulder varies between PTMs and traditional keyword models.

\begin{equation}
 Recall = TP / (TP + FN)
 \label{eqn:recall}
\end{equation}

Where TP = True Positive and FN = False negative.
\subsubsection{Recall.} Recall is graded from 0 to 1, and it is related to the proportion of examples that are labelled as positives \ref{eqn:recall}. A score of 1 means that all of the examples are positive. Recall was selected because it is assumed that the police analysts are interested in finding all ipositive outcomes of a given theme. The downside of using recall alone is that a trivial strategy of labelling all cases as positive would yield a recall value of 1 but would not reflect progress in the eyes of the analysts, who would still be faced with the original sample and all of the negative instances. However, to the present ends, the metric is adequate for exploring the differences between the keyword model and the PTMs.


 \subsubsection{Time} Analysts are busy, and the models that they use must not be too time intensive, relative to the results that they offer. Time is used here to understand, at a relatively high level, how much effort an analyst must expend in order to use a model. It is assumed that test and validation sets are required for each model. The training sets need only be utilised by the ML models. During the labelling process, it took the author approximately one hour to read 100 examples. When time is used as a metric, it is important to distinguish between elapsed time and user time. For instance, reading an MO text makes demands on the schedule of the analyst, but waiting for a model to run does not – the analyst can perform another task. Thus, labelling time (user time) is not the same as model training time (elapsed time).
 
\subsubsection{Bias} \label{study1-bias}This section explains how the PTMs were explored for bias. The PF1 data were not accompanied by victim characteristics. Therefore, the investigation of bias in this data set is limited. The PF2 data did include victim characteristics. Accordingly, bias is explored more thoroughly in the following chapters. However, it is useful to inquire whether any of the characteristics of the text data influence the ability of the model to arrive at the correct classification systematically. Three properties were investigated for bias in this study. The first was the length of the MO text. Longer MO texts may contain more information and thus be easier to classify. Recall that BERT can only recognise certain words. Words that are not recognised in their entirety are broken down into pieces until the process can be ran successfully. For example, untidy becomes \say{un, ti and dy}.  Pieces of words are investigated by reference to count per MO and as a proportion of the length of the MO text.

The metric of interest is the Pearson correlation coefficient, which allows for the identification of correlations between a statistical property (e.g., length of text) and the accuracy of the classification of each MO text within the test set. Accuracy of classification was defined by reference to the accuracy of the model probability of classification. For example, if the model predicts that an MO text is a positive example with probability of 0.7 and the text is in fact a positive example, then the error is 0.3. Conversely, if the MO text in question is, in fact, a negative example, then the error is 0.7.

As noted in the methods chapter, in order to arrive at a robust estimate of bias rather than a single value of a metric, a multiple random selection approach was employed to generate a spread – 20\% of the labelled data were randomly selected into the test set. The remaining 80\% were used to train the model. Once the model had been trained, the 20\% test set was used to generate the required metrics. This process was repeated 10 times with different random selections of the test and training sets. Each selection would produce a different value for the metric. Therefore, 10 values were produced for each metric over the course of the experiment.



\begin{table}[]
\begin{tabular}{|c|c|c|c|c|}
\hline
car          & dacia          & lamborghini & nissan      & toyota     \\ \hline
alfa romeo   & ferrari        & land rover  & opel        & van        \\ \hline
aston martin & fiat           & lexus       & peugeot     & vauxhall   \\ \hline
astra        & focus          & lotus       & porsche     &  vehicle  \\ \hline
audi         & ford           & maserati    & renault     &  vehicles    \\ \hline
audi         & general motors & mazda       & rolls-royce & vespa  \\ \hline
bentley      & honda          & mercedes    & saab        & volkswagen      \\ \hline
bmw          & hummer         & mg          & seat        & volvo   \\ \hline
bugatti      & hyundai        & mini        & skoda       & vw       \\ \hline
cadillac     & isuzu          & mitsubishi  & smart       &       \\ \hline
chevrolet    & jaguar         & motor       & subaru      &            \\ \hline
chrysler     & jeep           & motorbike   & suzuki      &            \\ \hline
cireon       & kia            & motorcycle  & tesla       &            \\ \hline
\end{tabular}
\caption[Keywords for keyword model - motor vehicle stolen]{\label{tab:Keywords} List of keywords used to populate the  keyword model for motor vehicle stolen.}
\end{table}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\begin{tabular}{|c|c|c|c|}
\hline
smash  & prized & jemm    & forc      \\ \hline
kick   & break  & attack  & damage    \\ \hline
broken & snap   & removed & shattered \\ \hline
\end{tabular}
\caption[Keywords for keyword model - force used]{\label{tab:Keywords_force} A list of keywords used to populate the keyword model for force used.}
\end{table}


\section{Results} This section will discuss the findings from experimentation of the BERT model with the PF1 burglary data. The section presents findings that pertain to the three main areas of analysis, namely performance, explainability, and bias. The impact of the active learning strategy is discussed in the next chapter. After the findings have been described, their interpretation is presented in the discussion section.


\subsection{MCC} Table \ref{tab:results_study_1} displays the results for the force model and the motor-vehicle model. The outbuilding model was not used because of the lack of suitable labels. These results were generated by using the data from the active learning process for the corresponding models. The table includes performance metrics from the PTM and the keyword model. Since separate active learning processes were used for each model, there was an opportunity to combine the data from each active learning strategy to finetune a model on a larger set of inputs. The results from the use of all labelled data (1,500 MO texts) are displayed in Table \ref{tab:final-model}. That table has 10 entries because the model was built 10 times in order to obtain an accurate spread of results. Recall that the models were built with an element of randomness; they can be different on each occasion. 

\paragraph{Motor Vehicle Model.} The BERT model has an MCC of 0.97 and a recall of 0.94 when using only the active learning data (900 texts) are used(Table \ref{tab:results_study_1}). This increases to an MCC of 0.97 and recall of 1.0 when all labelled data are used (1500 texts) (Table \ref{tab:final-model}.). In comparison the keyword model achieves an MCC of 0.81 and a recall of 1.0. It is worth noting that the keyword test set result was unusually satisfactory - the validation set and the train set which were used to create the model actually had lower MCC scores than the test set. This is the reverse of what one would expect because the model is usually always better at classifying the data on which it was built.

\paragraph{Force Model.} The results for the use-of-force model are similar. BERT has an MMC of 0.86 for the model that is built only on the active learning data (700 texts). When all data (1,500 texts) are utilised, MCC increases to an average of 0.91. The keyword model, when applied to the use-of-force model, yields an MCC of 0.51, which is significantly inferior than that of the PTM. The recall values from the two models, however, are closer to each other. The keyword model has a recall of 0.96, and BERT has a recall of 0.94.

\begin{table}[]
\begin{tabular}{@{}lcccc@{}}
\toprule
                         & \multicolumn{2}{c}{Motor vehicle} & \multicolumn{2}{c}{Force} \\ 
                         & Recall           & MCC            & Recall       & MCC        \\\midrule
Keyword (Validation)     & 1                & 0.65           & 1            & 0.56       \\
Keyword (Test)           & 1                & 0.81           & 0.96         & 0.51       \\
Keyword (Train)          & 0.97             & 0.62           & 0.94         & 0.45       \\\midrule
BERT (Validation)        & 0.94                & 0.85           & 0.94            & 0.89          \\
BERT (Test)              & 0.94                & 0.97           &0.94           & 0.86         \\
BERT (Train)             & NA               & NA             & NA           & NA         \\ \bottomrule
\end{tabular}
\caption[Model metrics. PF1 data. Force used and motor vehicle model.]{\label{tab:results_study_1} Selected metrics from the results of Study 1a. These results are generated only from the MO text that was labeled within the active learning strategy for that model.}
\end{table}


\subsection{Time.} This subsection compares the time that it takes to build and run each model successfully. In particular, a comparison is made between the PTM and the keyword models. In each case, it is assumed that the test data and the validation data need to be labelled for the purposes of model development. Therefore, they are not be included in the comparisons. It is assumed that labelling 100 MO texts takes an hour. Additional modelling time within the active learning process is accounted for, as discussed in the next chapter.

\paragraph{Motor Vehicle Model} The motor vehicle PTM uses 900 texts. Labelling thus took 9 hours. Fine-tuning the model took an additional 7 hours, although this activity does not call for any human input once initiated – it can be completed overnight or while an analyst is engaged in other tasks. Only 100 MO texts had to be labelled for the keyword PTM. The knowledge that was gained from reading the test set, the validation set, and the initial training set was sufficient to produce a suitable keyword recall model from the validation set. The keyword model therefore required an hour of labelling and an hour of research (to expand the list of keywords so as to include plausible alternatives). Therefore, the keyword model can be built and implemented much more rapidly – user time is 2 hours. Conversely, PTM demands 9 hours of user time and 7 hours of training. 

\paragraph{Force Model.}Similarly, the PTMs for the use-of-force model took much longer to build. In this case, 7 hours of labelling were followed by 6 hours of finetuning. The keyword model was complete after 90 minutes.

\begin{table}[]
\begin{tabular}{@{}ccccc@{}}
\toprule
\multicolumn{1}{l}{} & \multicolumn{2}{c}{Force Model}       & \multicolumn{2}{c}{Motor vehicle model} \\\midrule
Run                  & MCC               & Recall            & MCC                & Recall   \\\midrule
1                    & 0.97              & 0.99              & 0.97               & 1.0        \\
2                    & 0.92              & 0.96              & 1.0                  & 1.0        \\
3                    & 0.92              & 0.96              & 0.97               & 1.0        \\
4                    & 0.90               & 0.94              & 0.97               & 1.0        \\
5                    & 0.90               & 0.94              & 0.97               & 1.0        \\
6                    & 0.90               & 0.94              & 0.97               & 1.0        \\
7                    & 0.88              & 0.93              & 0.97               & 1.0        \\
8                    & 0.89              & 0.94              & 0.97               & 1.0        \\
9                    & 0.90               & 0.94              & 0.97               & 1.0        \\
10                   & 0.90               & 0.94              & 0.97               & 1.0        \\\midrule
Mean(CI)             & 0.908 (0.89-0.92) & 0.948 (0.94-0.96) & 0.973 (0.97-0.98)  & 1.0 (1.0-1.0)  \\\midrule
Best Run             & 0.97              & 0.99              & 1.0                  & 1.0        \\ \bottomrule
\end{tabular}
\caption[Model metrics. PF1 data. Force used and motor vehicle stolen models]{\label{tab:final-model} Each run represents the fine-tuning of a single model using all the labelled data. Each run is independent. Results are different between runs as there are random aspects to fine-tuning that can alter the end result. }
\end{table}

\subsection{Explainability} LIME was used to examine the PTMs in order to understand which words had the strongest effect on the classifications. Figure 10.1 is an example of the LIME output from a single MO text; only the 10 most influential words are highlighted.  The prediction is for a burglary with theft of a motor vehicle. The words that are highlighted in orange contribute the most to the corresponding classification; the words that are highlighted in blue count against it. In this case, the three most important words for the decision were all \say{Vehicle}. 

\begin{figure}[!tbp]
  \centering
    \includegraphics[width=\textwidth]{images/lime_pred_output.png}
    \caption[Lime Output for a single MO text for the motor vehicle theft during a burglary model.]{ Lime Output for a single MO text for the motor vehicle theft during a burglary model. The model correctly predicts that a vehicle was stolen. Words highlighted with orange contributed to the positive prediction.}
    \label{fig:lime_out1}
\end{figure}



Although the LIME output that is displayed in Figure 10.1 provides an adequate visual representation of the operation of the model with a single MO text, that style of visualisation does not scale well to multiple texts. Accordingly, a different approach was adopted in order to arrive at a more general representation of the LIME output from several texts. The general approach was to run the LIME algorithm for every MO text in the test set. The coefficients from the local models that were generated for each MO text were stored, and the word clouds in Figure \ref{fig:wordcloud_mv_both}  and Figure \ref{fig:wordcloud_force_both} were generated. The size of a word in the word cloud reflects how important it is for the classification of all MO texts in the test set. Word sizes cannot be compared across word clouds.  



\begin{figure}
     \centering
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/burg_safer_mv_wordcloud.png}
         \caption{Words that contributed to a positive classification}
         \label{fig: wordcloud_mv}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/burg_safer_mv_rev_wordcloud.png}
         \caption{Words that contributed to a negative classification}
         \label{fig: wordcloud_mv_rev}
     \end{subfigure}
        \caption[Wordclouds from  \textbf{motor vehicle} classification model. PF1 data.]{Wordclouds from  \textbf{motor vehicle} classification model. PF1 data. These wordclouds were generated using a fine-tuned BERT model on the PF1 data. The larger a word the more important it is for a classification. Words size is derived from a summation of the coefficients from individual LIME models. Word sizes are not comparable across figures.}
        \label{fig:wordcloud_mv_both}
        
\end{figure}


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/burg_safer_force_wordcloud.png}
         \caption{Words that contributed to a positive classification}
         \label{fig: wordcloud_force}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/burg_safer_force_rev_wordcloud.png}
         \caption{Words that contributed to a negative classification}
         \label{fig: wordcloud_mv_force}
     \end{subfigure}
        \caption[Wordclouds from \textbf{force} classification model. PF1 data.]{{Wordclouds from \textbf{force} classification model. PF1 data.} These wordclouds were generated using a fine-tuned BERT model on the PF1 data. The larger a word the more important it is for a classification. Words size is derived from a summation of the coefficients from individual LIME models. Word sizes are not comparable across figures.}
        \label{fig:wordcloud_force_both}
        
\end{figure}

\subsection{Bias} Table \ref{tab:PF1_bias} highlights the mean of the Pearson correlation coefficients for the metric in the first column. The mean was calculated from 10 randomly initiated model builds, as described in Section 10.3.3. It is clear from the table that there are no linear correlations between the accuracy of the classifications and the statistical properties of the MO texts. All correlations are very close to zero, as are their ranges. 


\begin{table}[]
\begin{tabular}{@{}lll@{}}
\toprule
                               & motor vehicle            & Force                    \\ \midrule
MO Length                      & 0.092 (0.150 to 0.009)  & - 0.01 (0.071 to -0.099) \\
Number of Word pieces          & 0.007 (0.060 to -0.085) & 0.001 (0.065 to -0.067)  \\
Ratio MO Length to Word pieces & 0.066 (0.141 to -0.042) & -0.004 (0.089 to -0.069) \\ \bottomrule
\end{tabular}

\caption[PF1 data - bias results]{\label{tab:PF1_bias} This table gives the mean Pearson correlation coefficients between the probability of classification from the NLP model and the metrics listed in the first column.  The value in the table is the mean of the ten Pearson coefficients. Figures in bracket are the range.}
\end{table}





\section{Discussion} This section discusses the results that were presented in the preceding one. The section is structured around the questions that were outlined in Chapter 7.

\subsection{Can PTMs Classify MO Texts Accurately? } The results in Table \ref{tab:final-model} demonstrate that, in the limited classification tasks that are explored here, PTMs can classify MO texts accurately. High MCC scores indicate that the models learn the relevant patterns well and can classify unseen texts with a high degree of accuracy.

\subsection{Are PTM better than the basic keyword method?} The results from the PTM and the keyword method are compared in Table \ref{tab:results_study_1}. The keyword model and the PTM have similar recall values, and they both perform adequately when tasked with finding positive instances. The MCC values, however, are different. This difference shows that the PTMs are much more efficient on the whole. Although the keyword models find most of the positive instances of a classification, they also include many FPs. In other words, the keyword models classify more negative instances as positive than they should.

How problematic is this FP issue? The answer to that question depends on the problem and the number of texts that are overidentified. The absolute number of overclassifications might be manageable for rare classes because, even if one takes a large proportion of a small absolute value, the total number of false positives would be low. However, for more balanced classes, even the moderate overclassification of a large number of instances may result in the misclassification of a large absolute number of texts. It is this issue that affected the keyword model and the different classification problems, as explained in the section that follows.
In the training data set, the number of MO texts that the keyword method labels as instances of theft of a motor vehicle is approximately 40\% higher than the actual number of such thefts. Sometimes, for example, the MO describes the vehicle that was used to leave the scene of a crime, irrespective of whether it was stolen or not. Although the keyword search exhibits appropriate recall, in that it is likely to discover all of the burglaries that involved the theft of a motor vehicle, it also mislabels many other MO texts. Consequently, a thorough check of all labelled MO texts is necessary for a reliable labelling scheme to be produced. This requirement causes model building to take longer than previously argued. For example, there are 9,961 burglary texts, with an underlying base rate of vehicular theft of 9\% (as estimated from the test and validation sets). The expected number of vehicular thefts within the burglary texts would therefore be 897. However, the keyword model classifies 1,727 texts as referring to a stolen vehicle. These 1,727 texts must then all be read for the FPs to be filtered. The estimated time that this task would consume is 17 hours, based on the earlier assumption of a reading speed of 100 texts per hour. Therefore, although the keyword model is much quicker to build initially (2 hours), approximating the performance of the PTM requires more time (19 hours).

The motor-vehicle classification that was presented above concerns an imbalanced data class. Accordingly, the keyword model could reduce the search space to a relatively small size, with an 80\% reduction of the set of all burglary texts (from 9,961 to 1,727). However, since the force model concerns a more balanced classification problem (the estimated split between instances in which force is used and instances in which force is not used is 60-40), the keyword model cannot truncate the search space to the same extent as in the motor-vehicle classification problem. The use-of-force model can only shrink the search space by 17\% (from 9,961 to 8,268) because the incidence of FP classifications is too high. For the remaining 8,268 texts, approximately 82 hours of labelling would be necessary to eliminate classification errors so as to approach the performance of the PTM.

The evidence indicates that the PTMs perform better than the basic keyword model when tasked with classifying MO texts. Although building PTMs and labelling texts takes longer initially, the results, as measured by MCC, are far superior than those of using the basic keyword models for a comparably shorter period of time. Two issues that have not been explored yet are that PTMs require specialist skills to operate and that keyword models can be made more intricate. That PTMs are complex is not in dispute. However, it is possible that they can be packaged for simple operation by nonspecialists so that there is no requirement to understand their complexities. However, at this stage, it would be unwise to dismiss the difficulty of implementing PTMs at police forces. This problem is discussed further in the final part of the thesis. Secondly, keyword models can be made more intricate, which would undoubtedly result in higher MCC scores. This said, PTMs emerged because probabilistic models were proven to be more robust than intricate rule-based ones – they are easier to maintain and generally exhibit superior performance when fed with unseen data.

\subsection{Are PTMs explainable?} The evidence from the LIME models indicates that the PTMs use words that are consistent with human explanations for the classification of MO texts. This said, it is worth reiterating that the LIME models investigate local models of a selection of MO texts and that they are not explaining the model in a global context – not all words will have the same effect in every MO text. 

The LIME output from the motor vehicle model (Figure 10.2a) shows that words such as \say{vehicle} and \say{car} are important for the classification of texts. This tendency is similar to that which humans exhibit and indicates that the model is operating similarly to a human tasked with classifying the texts. The model highlights meaningful words rather than ones with spurious correlations. The words that contribute negatively to the motor-vehicle classification are also of interest (Figure 10.2b). The words that are selected are more evenly sized and therefore of similar importance. Most of the words are common, which indicates that there are no particular patterns in the negation of the positive classification of motor-vehicle burglaries. This result was expected because past experiences with MO texts indicate that there are no instances of negative reporting. For example, no MO text states that a motor vehicle was not stolen. This, however, is not true of the use-of-force model. The application of force or its absence is generally noted in those texts.

Similarly to the motor-vehicle model, the LIME output from the use-of-force model is commensurate to the expected output of a human tasked with classifying one of the MO texts. \say{Force} and \say{smash} are prominent examples. In contrast to the motor-vehicle model, however, there is also a strong pattern in the words that contribute to negative classifications. The word \say{insecure} is prominent in the negative word cloud.

Although the explanations are local, the LIME output offers an adequate explanation of the classificatory choices. The \say{Explainable} section of the ALGO-CARE framework contains the following question: \say{Is appropriate information available about the decision-making rule(s) and the impact that each factor has on the final score or outcome?}. Arguably, the output is sufficiently explainable for the classification of each text to be justified. At the individual level, the texts are explainable, and a justification can be given for each classification by reference to the LIME explanations. At a global level, however, the model is not wholly explainable. If one takes the word \say{factor} from the quotation above to denote a word in the MO texts, then the factor in question cannot be said to have the same effect on the final classification in all instances. This difference results from the tendency of the model to use the surrounding context of a word as well as individual words to compute final effects. This tendency is problematic for all models in which strong interaction effects between factors are present, not just for text data.

The problem of explainability is unlikely to be settled on these pages. Tests would need to be conducted with a number of different stakeholders, including members of the public and police officers. However, the LIME output shows that the models work as expected, and classificatory decisions are made for appropriate reasons, that is, they are not based on spurious correlations.


\subsection{Are PTMs Biased?} Models are biased if their performance differs systematically across types of instances. As noted previously, the PF1 is accompanied by limited metadata. There are no victim data on which to test bias. Therefore, only bias against text statistics is investigated here. Is there any bias that is related to the statistical properties of the texts, such as length or wording? The limited investigation did not detect any biases or systematic failings in relation to certain types of the text, as shown by the Pearson correlation coefficients. This finding has an important implication – if there is bias against certain victim characteristics which impacts, say, the length of an MO text, then that bias may not manifest itself as a degradation in probability accuracy. Therefore, a lack of correlation between victim characteristic and probability accuracy is not proof of absence of bias in the recording of the data, only of a lack of bias in model performance.


\subsection{Limitations} The general limitations are discussed in the final part of the thesis. This section covers the limitations that are specific to the present study. In general, it has two main limitations. The first has to do with the number of classification tasks, and the second has to do with the investigation of bias:

\begin{itemize}
    \item Classification Tasks. Only two classifications tasks were selected, and both are related to burglary. Although the two tasks concern problems of different types, that is, balanced and imbalanced classes, and although the results are satisfactory, the generalisability of the results to all types of crime is limited. The same is true of the applicability of the results to other classification tasks, such as the identification of burglaries that only target outbuildings. However, it is clear that there are problems for which PTMs can be a useful means of classifying MO texts.
    
    \item Bias. The bias investigation was severely limited by the lack of victim data within the data set. Study 1c compensates, for this shortcoming partially because it draws on victim data from which it is possible to assess classificatory bias.
    
\end{itemize}

\section{Conclusion} In this narrowly focused study, PTMs were found to classify MO texts adequately across balanced and imbalanced classes. The tasks were to detect burglaries that involve the theft of a motor vehicle and burglaries in which force is used. In addition, the PTMs were found to perform better than simpler keyword models because they could discriminate more accurately in order to reduce the incidence of FPs. Despite the longer setup time and, in particular, the length of the process of labelling training data, PTMs are more efficient than keyword models. LIME was employed to understand how the models arrived at the classifications. In all cases, there appeared to be a sound rationale for the decisions of the models. The words that were more influential in the classificatory problems were also words that would have been important if a human had completed the classification tasks. Bias was only examined partially due to a lack of victim data. There did not appear to be any significant bias against texts of particular lengths or out-of-vocabulary words. This first study provides a solid basis for the use of PTMs. However, the use of active learning was not studied, and the applicability of the results is relatively narrow.

If applicability is to be expanded, it is important to discover whether the models work satisfactorily when used with other types of police free texts. Do they work when applied to different crimes? Only burglary was studied here. Do they work in other police areas, and can they facilitate the processing of other types of police data? These questions are answered, in part, in the subsequent chapters, in particular in the replication study that re-examines the classification problems that were explored here. That study concerns another police force. Before the replication study is presented, however, the next chapter investigates the use of active learning and its usefulness in reducing the burden of labelling.

