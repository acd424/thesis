\chapter{Study 1c: PF2 Burglary MO}

\section{Introduction} The primary aim of this chapter is to examine the potential for replication of the work that has been presented so far. Replication is fundamentally important if work is to be undertaken on a large scale. Police forces in the UK can have different processes and training standards. Therefore, what works in one force area would not necessarily also work in another. The main rationale of this study is to provide further evidence for the purposes of Supporting Objective 2, \say{Evaluate how effective PTMs are with MO data}.
 
In addition to replication, this chapter extends the analytic approach in four respects that reflect the different types of available data. First, the data allowed for an exploration of bias against victims with certain characteristics. Secondly, the data allowed for the completion of an additional classification task, namely an examination of burglaries that involve only entering an outbuilding. Third, it was possible to use the models that were developed for the PF1 data to label the PF2 data. This provides insights into the applicability of sharing the models across different police forces. Fourth, the models were fine-tuned on data from one year and used to label data from a subsequent year. This procedure sheds light on the decay of analytic performance over time.

The main finding of this chapter is that the results from PF1 are largely replicated with PF2, with no significant decline in performance. Accordingly, the key conclusion of this chapter is that PTMs are likely to be applicable at police forces other than the ones that are tested here.

\subsection{Research Questions} This chapter aims to answer three research questions:

\subsubsection{Can the results in Study 1a, the PF1 burglary study, be replicated in a different police force?} Study 1a inquired whether PTMs can be used to classify burglary MO texts in two different scenarios, which have to do with the use of force and the theft of motor vehicles. In each case, the PTMs were finetuned on PF1 data, resulting in appropriate accuracy. In this study, the PTMs are fine-tuned on the same problems but with PF2 data. In addition, it was also possible to build a model for the outbuilding only model. The outbuilding only model was introduced in Study 1a, but it was not completed because the PF1 data did not contain references to that type of burglary. The outbuilding only model is intended to detect whether a burglary targeted only an outbuilding, such as a shed, without the main home of the victim being breached.

\subsubsection{Can models trained with data from one police force be used in another force?} Fine-tuning PTMs on the same task at two different police forces enables the models to be used across areas. It also becomes possible to ascertain whether they are generalisable. If their applicability is indeed broad, then large benefits are likely to result from the dissemination of the models across police forces, which would reduce the resource burden of model creation. This problem is outside of the scope of the first question, which presupposes that the models are built from data for a particular police force. In this second question, the models are built with PF1 data then used on PF2 data.

\subsubsection{Are models accurate over time?} Language changes both through the introduction of new words and as a result of changes in the usage of existing lexical units. In policing contexts, officers can also be encouraged to record different facts over time. These changes could potentially change the form or the wording of an MO text. If the language of MO texts changes so as to differ from the language that the PTMs are finetuned on, then one can expect performance to deteriorate. Although only two years’ worth of data are used here, this hypothesis is tested by finetuning a model on data from one period and testing it on data from a subsequent period. Understanding how or when the performance of a model may deteriorate is important for ensuring that the model that is being used has been trained correctly.

\section{Data} The data that are used in this study are from PF2. They were described comprehensively in Chapter 8. The PF2 data were whitelisted by the project team in order to remove personally identifying information. This process was also described in Chapter 8. The main difference from the PF1 data results from the addition of victim characteristics, namely ethnicity and sex, as metadata. This addition is conducive to a more profound investigation of the potential biases that the PTMs produce.

Beyond victim characteristics, additional details were provided after the models had been built for validation purposes. Links to stolen vehicles were added in order to facilitate the validation of the vehicular theft model. A link is an entry in the police database that indicates whether a vehicle was stolen during a given burglary. This provides an additional verification that enables the performance of the finetuned PTM to be assessed. The completion of the database link results in structured data that is easy to search. PF2 analysts expect “stolen” links to have higher completion rates than flags (the structured data that were introduced earlier). A “stolen” link is therefore an appropriate structured indication of whether a vehicle was stolen. It can be compared to the model classification of the text data.

The PF2 data also contained more details about the date and time of the offences that had been committed. The PF2 data included details on the years, months, days, and times of offences, whereas the PF1 data included only months. Consequently, the data on dates can be analysed in greater detail. As an interesting aside, the PF2 data also cover the period of the initial Covid-19 pandemic in the UK, including the first lockdown. The effects of that lockdown on intra-crime variation for burglary can also be observed.


\section{Methods} The methods of this study were introduced in Chapter 9 and recapped in Chapter 10, which is on Study 1a. The general process is similar to that which was explained previously. The deviations from the approach that was described in the general introduction are listed below.

\subsection{Labelling} As in Study 1a, the fine-tuning of the PTMs is a supervised learning process. Therefore, labelled data are required for the models. The data were labelled by two researchers, with the author holding the casting vote in the event of disagreement. The MO text was selected through an active learning strategy, as detailed in Chapter 9. On this occasion, the labelling data pool was limited to burglaries committed between October 2018 and the end of 2019 (as mentioned in the data chapter, October 2018 coincides with the introduction of the new data-recording system for PF2). This restriction was introduced in order to facilitate the investigation of the accuracy of the model over time, that is, to enable the third research question of the study to be answered. Active learning was only conducted for the motor-vehicle model (reason explained later). Accordingly, all PTMs were fine-tuned on data that had been selected for the motor-vehicle model through active learning. In total, 1,982 MO texts were read and labelled for the burglary classification models.

\subsubsection{Fine-tuning Models} There were no significant differences between the fine-tuning methods that were applied to the PTMs in this study. Fine-tuning was completed by using the same methods as those that were outlined in Chapter 9. The BERT-large model was used. The hyperparameters were all set in the manner that is described in Chapter 9.

\subsubsection{Performance} The additional data fields that are provided with the PF2 data allowed for a deeper investigation into bias than had been possible with the PF1 data. Bias against individuals of certain sexes and ethnicities was explored by comparing PTMs across different victim characteristics. Bias was explored by using the metrics Equality of Opportunity (EoO) and Predictive Parity (PP), both of which were introduced in the methods chapter. EoO is based on recall and measures the disparity of the probability of a true positive (TP) across groups. For example, given that a classification is positive, what is the probability of finding it? PP is based on precision and is a measure of the disparity of the probability of false positives (FPs) across groups. For example, given that the model finds that a classification is positive, what is the likelihood that the classification is correct?

These metrics were calculated for each test set, and a cross-validation experiment was completed. As noted earlier, a reference group was selected for each bias in order to determine whether there is a difference between the reference group and the remainder of the population. The reference groups were \say{white European} and \say{male}, and they were compared to the groups \say{all other ethnicities} and \say{females}, respectively. Unknown and missing values were excluded from the analysis.  \footnote{The analysis was also conducted with these missing values included in the comparison groups and there was no significant difference in the result.}. 

No comparison to a basic keyword model was conducted. The advantages of the PTMs over the keyword approach were explained in Chapter 5 and demonstrated in Study 1a. However, it was possible to make a comparison with another method that police forces may use. Police forces often record some aspects of intra-crime variation as flags. Flags are typically key words or phrases that a police officer can select to describe a crime. In the PF2 data, these flags had been selected from a series of dropdown menus on the crime-recording software. These flags are much easier to search than free-text data because they are structured and are therefore be used often to find crimes of interest. The following process was followed in order to compare the flags to the model: firstly, a decision was made about the flags that describe classification types accurately. For example, it was determined which flags highlight burglaries in which force was used. The list of flags that were used for each classification is displayed in Table \ref{tab:burg_keywords}. Secondly, the monthly counts of crimes that do and do not meet the criteria of the classification were summed. This step was completed for both the crimes that were selected by reference to flags and to the crimes that were selected through the use of the NLP model. It was possible to compute monthly percentages of positive classifications from the sums of the positive and the negative classifications. Finally, the percentage of positive classifications was plotted as a time-series line, and the line plots were compared.


\begin{table}[]
\centering
\begin{tabular}{p{0.3\linewidth}|p{0.6\linewidth}}
\toprule
Classification                    & Flags                                                                                     \\ \midrule
\multirow{4}{*}{Motor vehicle}    & Instrument Used, Key Used, Stolen                                                            \\
                                  & Instrument Used, Key Used, Key Used                                                          \\
                                  & Property, Conveyance, Car                                                                    \\
                                  & Property, Conveyance, Motorcycle                                                             \\ \midrule
\multirow{2}{*}{Force}       & Trademarks- Attack Method Premises                                                           \\
                                  & Entry Method, Attack Method Premises                                                         \\ \midrule
\multirow{2}{*}{Outbuilding} & Location, Garage - Includes premises for sale and repair but does not include petrol station \\
                                  & Domestic|location, Garden - Driveway, Shed                                                   \\ \bottomrule
\end{tabular}
\caption[Burglary Keywords]{\label{tab:burg_keywords}The keywords used to filter the MO Keywords data column in the PF2 Burglary data. }
\end{table}


As mentioned in the data section, there was an additional validity check on the motor-vehicle classification, namely for the presence of a link between a stolen vehicle and the crime. These data were also used to check the validity of the vehicular theft model and were added to the monthly time-series plots that were described in the previous paragraph. The calculation method was the same.   

\subsubsection{Model Performance Over Time.} Model performance over time was investigated by fine-tuning the model on data from one period and testing it on data from a subsequent period. All PTMs were fine-tuned and initially tested on data from the period between late 2018 and the end of 2019. For the replication element of this study, all MCC metrics were gathered from a test set that was randomly selected from the same set of dates. However, a separate test set was also built for 2020, which allowed the model from the earlier time period to be tested on the 2020 data. It is possible that the 2020 data are not ideal for comparative purposes due to the Covid-19 pandemic. The pandemic resulted in severe mobility restrictions as the government tried to curtail the spread of the virus, and it expanded the lexicon of the general public. However the effect of the pandemic on burglary MO texts may have been less severe because it is not immediately clear how the pandemic would change burglary methods and, therefore, the words that the police use to describe burglaries. However, if there is a significant degradation in model performance over the years, then the Covid-19-induced variation would be one source of change that would require further investigation.   

\subsubsection{PTM transfer-ability} PTMs that were fine-tuned on PF1 data were used to label the PF2 test sets. MCC scores were calculated and directly compared to the MCC scores from the models that were built from the PF2 data. For example, the force model that was built from the PF1 data was used to label the PF2 test set, and the labels that were generated were compared to the force labels. Comparing the two MCC scores indicates how accurate a model that is generated by one police force can be when it is employed by another police force.

\section{Results} The results of the replication study are presented first. The MCC scores and explainability are directly comparable to the earlier study of the PF1 data because the two studies are based on the exact same methods. The estimates of bias are different because the PF2 data cover victim characteristics. Therefore, the metrics of extrinsic bias for the sex and ethnicity groupings were examined. The comparison with the NLP labels for which the police recorded keywords are displayed as line plots after the bias results. Thereafter, the exposition turns to the MCC results for the change in time period and the reuse of models across forces.

\subsection{MCC} The MCC results are presented in Table \ref{tab:lancs_mcc}. The table refers to the test sets from 2018–2019 and from 2020–2021. However, before these sets are explored, it is necessary to explain, in brief, how much data were labelled and why.
 
 As detailed in the methods chapter, when the active learning strategy was used, labelling for the validation set would cease when MCC exceeded 0.9. The motor-vehicle model, which was selected for labelling first,  never achieved this value (see Table  \ref{tab:results_1c}). For the reasons that are given in the next paragraph, additional labelling was unlikely to result in increases in MCC. Consequently, all labelling ceased after the 16th active learning batch. 

The motor-vehicle task was selected for labelling first. However, all tasks (i.e. the use-of-force and outbuilding only tasks) were labelled at the same time. At Batch 16, enough texts had been labelled to gauge the necessity of additional labelling for the two other classification tasks. The use-of-force classification task had achieved an MCC of 0.92 by Batch 5 (see Table \ref{tab:results_1c}). Therefore, no further labelling was necessary.

The outbuilding task did not reach an MCC of 0.9 by Batch 16; the MCC scores stabilised at approximately 0.85 at Batch 5 (see Figure  \ref{fig:mcc_burg_lancs}.). It was therefore unlikely that additional labelling would increase the MCC score. However, a single batch of additional active learning was conducted, with a fine-tuned outbuilding model applied to the final selection. The MCC of the model (tuned on 16 batches of motor-vehicle and one batch of selected outbuilding data) did not increase as a result. Therefore, the author determined that no further labelling was necessary. The data from this 17th batch are omitted for simplicity.

\begin{table}[]
\centering
\begin{tabular}{@{}lcccccccc@{}}
\toprule
Batch       & 1    & 2    & 3    & 4    & 5    & 6    & 7    & 8    \\ \midrule
Motor vehicle          & 0    & 0.48 & 0.65 & 0.73 & 0.56 & 0.82 & 0.8  & 0.82 \\
Force       & 0.52 & 0.71 & 0.82 & 0.88 & 0.92 & 0.93 & 0.88 & 0.92 \\
Outbuilding & 0.33 & 0.23 & 0.72 & 0.84 & 0.87 & 0.85 & 0.85 & 0.86 \\\midrule
Batch       & 9    & 10   & 11   & 12   & 13   & 14   & 15   & 16   \\\midrule
Motor vehicle         & 0.75 & 0.72 & 0.88 & 0.86 & 0.82 & 0.75 & 0.82 & 0.72 \\
Force       & 0.88 & 0.92 & 0.92 & 0.9  & 0.92 & 0.91 & 0.93 & 0.91 \\
Outbuilding & 0.85 & 0.86 & 0.85 & 0.84 & 0.86 & 0.84 & 0.85 & 0.86 \\ \bottomrule
\end{tabular}
\caption[Batch metrics - PF2 data. All models]{\label{tab:results_1c}MCC values (based on the validation set) for models fine-tuned on PF2 Burglary data. Batch refers to the active learning batch e.g. after 5 batches of labelling (500 MO texts) the motor vehicle model had an MCC of 0.56 }
\end{table}


The MCC scores for the models are reported in the subsections that follow. An MCC score of 1 is optimal, while a score of 0 is equivalent to a finding of randomness. The results that are discussed below are confined to the 2018-2019 test set. The 2020-2021 test set scores are discussed at a later stage. 

\subsubsection{Motor Vehicle model} As explained previously, the motor-vehicle model was selected as the first model for labelling via the active learning strategy. The values of MCC after each active learning batch, which were calculated by using the validation data, are displayed in Figure \ref{fig:mcc_burg_lancs}  and in Table \ref{tab:results_1c}. The highest value that was attained was 0.88, which is below the requirement of the stop condition (0.9). Labelling ceased after the 16th batch because it had become apparent that there were no further positive classifications within the pool of potential training data. In other words, the active learning strategy had already selected all MO texts that refer to the theft of a motor vehicle into the training data, and there were no more positive examples that could be used for learning. In fact, the last positive example had been found in Batch 11. It is clear from the plot in Figure \ref{fig:mcc_burg_lancs} that the additional negative examples, which formed Batches 12–16, did not facilitate the fine-tuning of the model. Therefore, further fine-tuning was deemed unnecessary. Consequently, the fine-tuning stopped after 16 batches, and the model was tested on the test set.




\begin{figure}[!tbp]
  \centering
    \includegraphics[width=\textwidth]{images/mcc_burg_lancs.png}
    \caption[MCC scores for the PF2 burglary models.]{{MCC scores for the PF2 burglary models.} MCC scores are shown after each iteration of the active learning strategy. The Force and Outbuilding models peak relatively early on at batch 5 and 6. Whereas the motor vehicle model peaks at 11. Some of the variation will be attributable to the random initialisation of the models. Source: Author generated.}
    \label{fig:mcc_burg_lancs}
\end{figure}


The MCC scores for the test set can be found in Table \ref{tab:lancs_mcc}. The model that was finetuned over 10 runs had a mean MCC score of 0.98, which is indicative of near-perfect performance. In half of the runs, the model classified each of the 200 MO texts in the test set correctly. The MCC metrics for the motor-vehicle model are comparable to the scores from the motor-vehicle model that was built on and applied to the PF1 data (mean of 0.97).

\subsubsection{Force model} The use-of-force model was applied to the same data that were labelled during the active learning for the motor-vehicle model. The mean MCC score from the 10 final initialisations on the 2018-2019 test set was 0.93. These scores are higher than the MCC score for the validation set, indicating that the validation set may have made the classification of the MO texts more difficult. These MCC results are comparable to the models that were finetuned and tested on the PF1 data (mean of 0.91).


\subsubsection{Outbuilding Model} The MCC scores for the application of the outbuilding model to the test data are also better than the validation set scores. The mean of the 10 initialisations on the entire 2018-2019 test set is 0.90. This is the lowest score across all three models. The outbuilding model was not built with the PF1 data because they are not suitable for its purposes. Therefore, the outbuilding results that are presented in this study cannot be replicated directly.



\begin{table}[]
\centering
\begin{tabular}{@{}lcccccc@{}}
\toprule
         & \multicolumn{2}{c}{Motorvehicle} & \multicolumn{2}{c}{Force} & \multicolumn{2}{c}{Outbuilding} \\ \midrule
Run      & 18/19           & 20/21          & 18/19       & 20/21       & 18/19          & 20/21          \\
1        & 1.00            & 0.89           & 0.93        & 0.92        & 0.91           & 0.94           \\
2        & 1.00            & 0.93           & 0.94        & 0.93        & 0.90           & 0.93           \\
3        & 0.94            & 0.88           & 0.93        & 0.94        & 0.90           & 0.94           \\
4        & 0.97            & 0.93           & 0.93        & 0.95        & 0.89           & 0.93           \\
5        & 0.94            & 0.88           & 0.94        & 0.96        & 0.92           & 0.94           \\
6        & 1.00            & 0.91           & 0.90        & 0.93        & 0.91           & 0.94           \\
7        & 0.97            & 0.88           & 0.90        & 0.94        & 0.85           & 0.96           \\
8        & 1.00            & 0.91           & 0.92        & 0.95        & 0.90           & 0.92           \\
9        & 0.97            & 0.90           & 0.92        & 0.96        & 0.91           & 0.94           \\
10       & 1.00            & 0.90           & 0.95        & 0.96        & 0.89           & 0.92           \\\midrule
Mean     & 0.98            & 0.90           & 0.93        & 0.94        & 0.90           & 0.94           \\\midrule
Best Run & 1.00            & 0.93           & 0.94        & 0.96        & 0.92           & 0.96           \\ \bottomrule
\end{tabular}
\caption[Final model MCC metrics. PF2 data. All models.]{\label{tab:lancs_mcc}MCC values (based on the test sets) for models fine-tuned on PF2 Burglary data. Scores are generated from 10 separate fine-tunes based on all labelled data. 18/19 refers to the test set from only the years 2018 and 2019, similarly 20/21 refers to the years 2020 and 2021.}
\end{table}




\subsection{Explainability} LIME was used once more to understand how the words in the texts contribute to the final classification. By way of reminder, BERT uses words and their surrounding context. Therefore, it is difficult to form a global understanding of the workings of the model. LIME provides a local understanding of each MO text by deleting words randomly to enable their effect on the final classification to be discerned. This approach is scaled up in this thesis through the application of LIME to all MOs in the test set and through the use of word clouds to highlight the most important words, as identified by the individual LIME model coefficients. The word clouds for the motor-vehicle, use-of-force, and outbuilding models are displayed in Figure \ref{fig:wordcloud_mv_both_lancs} , Figure\ref{fig:wordcloud_force_both_lancs} and Figure \ref{fig:wordcloud_home_both_lancs} respectively.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/mv_wordcloud_positive.png}
         \caption{Words that contributed to a positive classification}
         \label{fig: wordcloud_mv_lancs}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/mv_wordcloud_negative.png}
         \caption{Words that contributed to a negative classification}
         \label{fig: wordcloud_mv_rev_lancs}
     \end{subfigure}
        \caption[Wordclouds from  \textbf{motor-vehicle} classification model. PF2 data.]{{Wordclouds from  \textbf{motor-vehicle} classification model. PF2 data.} These wordclouds were generated using a fine-tuned BERT model on the PF2 data. The larger a word the more important it is for a classification. Words size is derived from a summation of the coefficients from individual LIME models. Word sizes are not comparable across figures. Source: Author generated.}
        \label{fig:wordcloud_mv_both_lancs}
        
\end{figure}


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/force_wordcloud_positive.png}
         \caption{Words that contributed to a positive classification}
         \label{fig: wordcloud_force_lancs}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/force_wordcloud_negative.png}
         \caption{Words that contributed to a negative classification}
         \label{fig: wordcloud_force_rev_lancs}
     \end{subfigure}
        \caption[Wordclouds from  \textbf{force} classification model. PF2 data.]{{Wordclouds from  \textbf{force} classification model. PF2 data.} These wordclouds were generated using a fine-tuned BERT model on the PF2 data. The larger a word the more important it is for a classification. Words size is derived from a summation of the coefficients from individual LIME models. Word sizes are not comparable across figures. Source: Author generated.}
        \label{fig:wordcloud_force_both_lancs}
        
\end{figure}




\begin{figure}
     \centering
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/home_wordcloud_negative.png}
         \caption{Words that contributed to a positive classification}
         \label{fig: wordcloud_home_lancs}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/home_wordcloud_positive.png}
         \caption{Words that contributed to a negative classification}
         \label{fig: wordcloud_home_rev_lancs}
     \end{subfigure}
        \caption[Wordclouds from  \textbf{outbuilding} classification model. PF2 data.]{{Wordclouds from  \textbf{outbuilding} classification model. PF2 data.} These wordclouds were generated using a fine-tuned BERT model on the PF2 data. The larger a word the more important it is for a classification. Words size is derived from a summation of the coefficients from individual LIME models. Word sizes are not comparable across figures. Source: Author generated.}
        \label{fig:wordcloud_home_both_lancs}
        
\end{figure}




The word clouds for the motor vehicle model exhibit a similar pattern to those from the PF1 data, see Figure \ref{fig:wordcloud_mv_both_lancs}. Firstly the most prominent words in the word cloud for a positive classifications (i.e., a motor vehicle was stolen) are words that a human might expect to use when completing the same classification task. The three most important words are \say{car}, \say{vehicle}, and \say{keys}. It should also be noted that these words are disproportionately important, which is why their size in the figure is much larger than that of other words. In contrast, the word cloud for words that contribute to a negative classification (Panel B in Figure \ref{fig:wordcloud_mv_both_lancs}) contains words that are much more similar in size. There is no observable theme, likely because the absence of car theft is not explicitly recorded in the MO texts.


The word clouds for the use-of-force model are similar in the positive classification case (i.e., force was used). The model uses words that are similar to the ones that a human might use if entrusted with the same classification task. However, some of the more important verbs are less prominent, and there appears to be a stronger focus on nouns, in comparison to the word clouds for the PF1 data. On the whole, the pattern of the important words is clear and logical. Unlike the motor-vehicle model, the negative classification, “no force used”, is often reported, and there is a clear pattern in the second word cloud (Panel B in Figure \ref{fig:wordcloud_force_both_lancs}).  The word \say{insecure} is the most important, for obvious reasons. \say{Unknown} is also prominent because it is often used to indicate that the method of entry is unknown, reflecting lack of clear evidence of use of force. 

The outbuilding word cloud is similar in structure to the motor-vehicle word cloud. The positive classification cloud contains a smaller number of disproportionately important words. \say{Shed}, \say{Garage}, and \say{garden} are the most important among them. The negative classification word cloud contains words that are closer in size and, in general, words that are encountered throughout all burglary MO texts. Again, this tendency is likely the result of failure to report on negative classifications explicitly.

Each of the pairs of word clouds indicates that the model uses words that a human would also rely on in determining the classification of a text. It may thus be inferred that the models focus on the most appropriate features of the text and not on spurious correlations. The next section investigates the biases that the models may exhibit in relation to sex and ethnicity.

\subsection{Bias} Bias within the models was investigated by reference to the characteristics “sex” and “ethnicity”. The reference groups were “males” and “white Europeans”. The models were investigated by exploring metrics of extrinsic bias, EoO, and PP. Table \ref{tab:lancs_bias}  displays the results from the two models that were built from the active learning data and from the tenfold cross-validation experiment models. The results are described in relation to the partition of the data, that is, by reference to sex and ethnicity rather than to specific models. The reader will recall that 0 is indicative of no bias, that a positive number is indicative of bias in favour of the reference group, and that a negative number is indicative of bias against the reference group. The theoretical maximum and minimum values are 1 and -1, respectively.

\subsubsection{Ethnicity} Two significant p values emerged from the cross-validation experimentation, and they are both for ethnicity. One is for EoO in the motor-vehicle model, and the other is for PP in the use-of-force model. In both cases, the mean shows that there is a slight bias against the reference group, that is, that the models may discriminate against white Europeans.

A review of the results from the model that was built from the active learning data indicates that most values for EoO and PP are close to zero, indicating little bias. Four out of the six EoO metrics are negative; the same is true of five of the six PP metrics. Once more, both findings indicate that there is slight discrimination against white Europeans. The results across the models are mixed. The direction of the bias is only consistent in the outbuilding model. However, even then, the bias in that model does not produce a statistically significant result in the cross-validation experiment. In summary, the values that reflect bias are small, but the results are not sufficiently consistent to indicate that the PTMs that classify MO texts in the PF2 data are systemically biased.


\subsubsection{Sex} The evidence for bias on the basis of sex is weaker still, and no statistically significant results emerged from the cross-validation experiments. For PP, the number of negative values is equal to the number of positive values. For EoO, the number of positive values exceeds the number of negative ones by one. In conclusion, there is no evidence that the PTMs that classify MO texts from the PF2 data exhibit bias against either sex.

\input{bias_table}

\subsection{Flag Comparison} This section compares the NLP-generated labels for the three models with the flags that the police may search in order to identify an intracrime variation of interest. In addition, the presence of links between stolen vehicles and burglary is explored.

\subsubsection{Motor Vehicle Model} The time-series plot for the motor-vehicle model is displayed in Figure \ref{mv_ts}. The police-generated labels \say{Linked vehicle} and \say{Flagged} should only be fully considered after 2019 because of the aforementioned change in data-recording systems. The two striking elements of the plot are that the NLP labels and the linked-vehicle labels are very well matched (the Pearson correlation coefficient is 0.94) and that the flags cover much fewer crimes. The latter tendency is observed across the classifications. In a discussion, the analysts from PF2 recognised that the flags do not have a high completion rate. However, based on their experience, they thought that the linked-vehicles data would be completed to a high standard.

An error analysis was conducted in order to explore the differences between the NLP model and the linked stolen vehicles. The error analysis reviewed 100 MO texts in which the NLP model had identified references to a stolen motor vehicle but for which there was no linked vehicle. In total, there were 432 errors of this kind. Of the 100 MO texts that were examined, 63 did refer to a stolen vehicle. Therefore, the classification from the NLP model was correct. The remainder (37) had been labelled incorrectly by the PTM. The majority of these errors (21) occurred when only vehicle keys had been stolen. These erroneous classifications may be useful in the context of vehicular theft because keys can be used to steal a vehicle after a burglary, but they do not reflect the purpose for which the model was trained. 


\begin{figure}
  \includegraphics[width=\linewidth]{images/mv_linked_time_series_plot.png}
  \caption[Motor vehicle model time series plot]{A time series plot of the motor vehicle classification. Showing data generated form the PTM model (NLP), linked vehicles and flags. Source: Author generated.}
  \label{fig:mv_ts}
\end{figure}

\subsubsection{Force Model} The use-of-force model only compares flags to PTM labels. The plot is displayed in Figure \ref{fig:force_ts}. As with the previous time-series plots, the most notable finding is that the PTM finds many more burglaries that involved the use of force than the police officers who use the flag system. Once more, this finding is consistent with the analysts’ view that the flag system is not used appropriately in practice. The other notable finding is that the PTM labels appear to be seasonable – the proportion of crimes in which force is used to enter a building is consistently higher in the winter months than in the summer months.  


\begin{figure}
  \includegraphics[width=\linewidth]{images/force_time_series_plot.png}
  \caption[Force used model time series plot]{A time series plot of the force used classification. Showing data generated form the PTM model (NLP) and flags. Source: Author generated.}
  \label{fig:force_ts}
\end{figure}


\subsubsection{Outbuilding Model} The outbuilding model plot displays the PTM-generated labels alongside the police-generated flags. The plot is displayed in Figure \ref{fig:outbuild_ts}. Like in the case of the other two plots, the PTM returns more crimes with a positive classification. However, the numbers that are returned here are closer than in the other plots. The early 2020 spike in the two time series coincides with the first Covid-19 lockdown in the UK. This spike may indicate that the lockdown policies resulted in a proportional shift in burglary types.


\begin{figure}
  \includegraphics[width=\linewidth]{images/outbuilding_time_series_plot.png}
  \caption[Outbuilding only model time series plot]{A time series plot of the outbuilding only classification. Showing data generated form the PTM model (NLP) and flags. Source: Author generated.}
  \label{fig:outbuild_ts}
\end{figure}

\subsection{Model transfer-ability} This subsection reports on the usage of models from one police-force area in another police-force area. The results are for the use of the PF1 models on the PF2 data; the reverse analysis could not be conducted for data security reasons. The MCC results in Table  \ref{tab:results_transfer} show that the models are reasonably transferable. In each case, the MCC of the transferred model is lower, which accords with expectations. However, the drop is not particularly significant in all cases. This finding demonstrates that models that are built with data from one area can be useful in another area. 



\begin{table}[]
\begin{tabular}{@{}llcc@{}}
\toprule
\multicolumn{1}{c}{Test Set} & \multicolumn{1}{c}{Model} & PF2 Model PF1 Data & PF1 Model PF1 Data \\ \midrule
18/19                        & Motor vehicle             & 0.93                   & 0.98                   \\
20/21                        & Motor vehicle             & 0.80                   & 0.90                   \\
18/19                        & Force                     & 0.91  & 0.93  \\
20/21                        & Force                     & 0.90  & 0.94 \\ \bottomrule
\end{tabular}
\caption[Model metrics. Models tested on alternate police force.]{\label{tab:results_transfer} MCC scores for the use of models built with PF1 data and used to classify PF2 data. PF2 metrics included for comparison. }
\end{table}

\subsection{Performance over time} Even though the training data only cover the 2018-2019 period, the test sets were built for both 2018–2019 and 2020–2021 so as to enable observation of the variation in model performance over time. The results in Table \ref{tab:lancs_mcc}  show the results from the 10 model initialisations in which the active learning data were used to fine-tune the PTM. The mean result for the 10 initialisations is reported here. There is a sizeable drop in the performance of the motor-vehicle model, from 0.98 to 0.90. This said, 0.9 may still be adequate, depending on usage. The performance of the use-of-force model improves slightly, from 0.93 to 0.94 (note that the highest-scoring run of the 2020-2021 set is superior to that of the 2018-2019 set). The performance of the outbuilding model also improves. The improvement is larger, with the relevant score increasing from 0.90 to 0.94.


\section{Discussion} This section synthesises the results that were presented in the preceding one in the context of the main research questions that were described at the start of the chapter. Each question is explored in turn, and the results are compared to those from the original study, which draws on PF1 data.

\subsection{Can the results in Study 1a be replicated in a different police force?}Study 1a set out to determine whether PTMs can be utilised to classify MO texts. The two classification tasks were 1) “Was a motor vehicle stolen during the burglary?” and 2) “Was force used to enter the building during the burglary?”. In addition, Study 1a inquired whether the PTMs are explainable and whether they work in the way that a human might do, that is, without relying on potentially spurious correlations in the data. In Study 1a, bias was examined to a limited extent due to a lack of data on victim characteristics.

The performance results in the replication study were equivalent to the results from the original study. Both resulted in high MCC scores, indicating that high-performing models can emerge from the fine-tuning of PTMs. An additional classification problem was explored in the replication study, namely that of outbuilding-only burglaries. A model was also fine-tuned for this problem, and it exhibits appropriate performance and a high MCC score.

If one compares the labels that were generated from the PTMs to the police-generated flags, one finds that the PTMs return a much higher number of crimes. Combined with the high MCC scores and the error analysis of the linked data, this finding suggests strongly that the PTM pattern in question is a more accurate reflection of intra-crime variation. Again, this result highlights the advantages of PTMs over existing police processes for exploring intra-crime variation.

Explainability was tested by having the LIME model generate word clouds which showed the most important words for each classification model. As with the PF1 models, the replication study produced word clouds that enhance trustworthiness. The important words that are highlighted in these clouds are entirely consistent with the words that a human may use to make a classificatory judgement. Therefore, they indicate that the model classifies similarly to a human.

More data on victim characteristics were available in the replication study than in the original study. Therefore, the models could be explored so as to detect bias against individuals of certain ethnicities and sexes. The results show that there is no evidence of systematic bias in the classifications of the fine-tuned PTMs. It should be noted that the text seldom made reference to the characteristics in question.

Therefore, bias was only likely to be introduced indirectly through systematic variation in the language and/or quality of the MO rather than through explicit references. It emerged from the bias investigation in the original study, Study 1a, that the length of the texts and the percentage of BERT words were not correlated with either of the bias metrics. In consequence, even if the MO texts on, say, Asian victims, had been short, the model would not have necessarily performed poorly in classifying them.
There may be a number of different ways in which biases can be introduced into the chain that leads from a crime being committed to the formulation of an MO text and its subsequent classification. Firstly, the crime may not be recorded because the victim may prefer not to interact with the police; in such cases, there is no MO text. If the victim does interact with the police, the interaction might be suboptimal (e.g., due to language barriers). Consequently, the information that is available might not be sufficient for an accurate and comprehensive description of the crime. Finally, a PTM is built on data that are scraped from the Internet. These data are almost assured to reflect common biases in society and may perpetuate them through the classifications. The bias investigation in this study only concerns the last problem, that is, the use of the PTM. The first two channels by which bias is transmitted are beyond the scope of the study, as explained previously. The results here indicate that the biases that are inherent in the PTM do not affect the classification of burglary MO texts in the context of the particular classification tasks under observation.

In summary, this study replicated the satisfactory results from the original and extended them, proving that PTMs perform well when tasked with the classification of burglary MO texts. In addition, the models classify the texts by using words that are similar to the ones that humans would use, offering evidence in favour of the proposition that the models are trustworthy. The limited investigations revealed no evidence of systematic bias in the model classifications.


\subsection{Can models trained in one police force area be used in another force?} Replicating the first study with data from a second police-force area highlights opportunities for transplantation. If model performance is unaffected by transposition, then the utility of the model is higher because models can be reused across forces without the need to share data. The results have shown that models can be transferred from one police force to another while retaining a reasonable level of performance. One implication is that forces can share models for direct use or seed the start of the fine-tuning of a separate model and therefore reduce the labelling burden. The practical implications may be significant, for example if the knowledge that is needed to classify a model is relatively specialised, as in the case of modern-day slavery crimes.

That models can be reused across forces also has implications for the implementation of PTMs. In the UK, for instance, there are 43 police forces, all with similar crime-recording techniques, a common language, and similar resource pressures. A central repository of models would be useful to all forces. Such a repository would allow sharing to be maximised and result in a commensurate reduction in the labelling burden. In addition, the technical aspects of model running and finetuning could also be conducted centrally, reducing the training burden across the 43 forces. Expanding the sharing of models to such an extent would require much more extensive experimentation than what this study, with its sample size of 2, can offer. Nevertheless, the results that were presented on the preceding pages are encouraging.


\subsection{Are fine-tuned PTMs accurate over time?} As language use changes, so does the performance of models. The language of an MO text reflects intracrime variation. If that variation changes, for instance in response to the adoption of a new security technique, then so does the language of the MO texts. Models therefore have to be examined in order to ensure that they remain relevant to the language that is used. The models in the study were trained on data from one year then tested on data from a subsequent year. There was no perceptible drop in performance in either of the three classifications tasks. It appears that the models are robust to some changes that occur over time and even to significant disruptions such as the Covid-19 pandemic. However, despite the general decline in burglary, there is no evidence to suggest that a new type of intra-crime variation emerged. Variation that may have changed the language being used in the second time period used for this experiment. The evidence of the robustness of the models to the passage of time is limited, and there is no evidence of robustness to new criminal techniques and the resultant changes in language. Changes such as these need to be monitored, and the findings that were presented here certainly do not imply that the validity of finetuned PTMs does not need to be re-examined as time passes. 

\section{Conclusions}This replication study provided additional evidence for the proposition that PTMs can classify police MO texts effectively by extending the problems of the original to another police-force area and to an additional classification task. In addition, it was shown that there is no evidence of classificatory bias on the basis of either sex or ethnicity. The replication study also investigated the performance of models over time, finding no perceptible drop in classificatory power. This indicates the models will remain useful over extended periods of time. However, the study was relatively weak, and a more thorough study would be required for a definitive assessment of the rate at which models ought to be refreshed.

The models were also shown to be effective in solving the same problems with data from different police forces. This finding suggests that it may be possible for forces to share models. Model sharing would reduce the labelling, computational, and skills burden of using PTMs considerably. This may prove important in the practical implementation of PTMs because it would significantly reduce costs. It could also indicate that the centralised co-ordination and, perhaps, the development of some aspects of the relevant labour would be efficient.

These studies showed that PTMs can be effective when used with MO text data and across a number of different classification problems. However, MO texts are not the only texts that police forces generate. Police incident logs contain both crime and non-crime data. The next case study builds on this work by using PTMs to classify antisocial behaviour incident logs. 





