\chapter{Study 1c: PF2 Burglary MO}

\section{Introduction} The primary aim of this chapter is to examine the potential for replication of the work undertaken so far. Replication is fundamentally important if work is to be undertaken on a large scale. Police forces in the UK can have different processes and training standards and so it is not immediately obvious that what will work in one force area will work in another. The main reason for this study is to provide further evidence for supporting objective 2 - Evaluate how effective PTMs are with MO data.
 
In addition to replication, this chapter  extend the analytic approach in four respects that reflect the different types of data available. First, the data allowed an exploration of bias related to victim characteristics. Secondly, the data allowed an additional classification task to be undertaken, namely an examination of breaking into an outbuilding. Third, it was possible to use models developed for the PF1 data to label the PF2 data. This provides insight into whether the models are applicable in different police forces.  Fourth,  the models were fine-tuned on data from one year and used to label data from a later year. This sheds light on  whether and how analytic performance decays over time.

This chapter finds that the results from PF1 were largely replicated in PF2 with no significant decline in performance.  Hence the main conclusion of this chapter is that PTMs are likely to have applicability across police forces not just those tested here.  

\subsection{Research Questions} This chapter will aim to answer the following research questions:

\subsubsection{Can the results in Study 1a, the PF1 burglary study, be replicated in a different police force?} Study 1a investigated whether PTMs could be used to classify burglary MO texts in two different scenarios, whether force was used to enter and if a motor vehicle was stolen. In each case PTMs were fine-tuned on PF1 data that produced good accuracy. This study will fine-tune PTMs on the same problems but using PF2 data.  In addition, it was also possible to build a model for the \emph{out-building only} model. The \emph{out-building only} model was introduced in study 1a, but was not completed because the PF1 data did not contain that style of burglary. The \emph{out-building only} model is seeking to detect whether a burglary targeted just an outbuilding e.g. shed, without also entering the main home.

\subsubsection{Can models trained with data from one police force be used in another force?} By fine-tuning PTMs on the same task in two different police forces, it is then possible to use the models from one police force into another police force area to see if the models are able to generalise across police forces. If they do have broader applicability then there is likely to be huge benefits in sharing the models across police forces thereby reducing the resource burden for model creation. This is different from the previous question because in the first question the models were built for each police force using their own data. In this question the models are built with PF1 data then used on PF2 data.

\subsubsection{Are models accurate over time?} Language changes, both in the introduction of new words and how old words are used. In policing contexts what Officers are encouraged to record can also change over time. These changes could potentially change the form or wording of an MO.  If the language of MO texts changes away from the language that the PTMs was fine-tuned on then we can expect a drop in performance. Although limited by two years worth of data we test this hypothesis by fine-tuning a model on data from one period then testing the model on a subsequent period. Understanding how or when a models performance may drop is important to ensuring that a correctly trained model is always being used.

\section{Data} The data used for this study is from PF2. The data was introduced fully in Chapter 8. The PF2 data was whitelisted by the project team to remove personally identifying information, this process was described in Chapter 8. The main difference from the PF1 data though is the addition of victim characteristics as meta-data, namely ethnicity and sex, which allow a greater investigation into potential biases whilst using the PTMs. 

Alongside the victim characteristics, additional detail was also provided after the models were built to help validate the models. To assist with validation of the motor vehicle theft model, links to stolen vehicles were provided. A link between a burglary and a stolen vehicle is made in the police database if a vehicle is stolen during that burglary. This provides an additional verification check to understand how well the fine-tuned PTM is performing. The completion of the database link is  structured data and so is easily searchable. PF2 analysts expect the stolen link to have a higher completion rate than the flags (the structured data introduced earlier).The stolen link is therefore a good structured representation of if a vehicle was stolen with which to compare the models classification of the text data.

The PF2 data also had more details surrounding the date and time of the offences committed. PF2 data included details on year, month, day and time of  offences, whereas the PF1 data included only month. Allowing greater analysis by date of the offence. As an interesting aside, the PF2 data also contained the initial period of the Covid-19 pandemic in the UK including the first lockdown. Effects of the first lockdown on intra-crime variation for burglary can be seen.   


\section{Methods} The methods for this study were introduced in Chapter 9 and recapped in Chapter 10 for study 1a. The general process is similar to that already explained. The variations form the general introduction are listed below.

\subsection{Labelling} As with Study 1a fine-tuning the PTMs is a supervised learning process and so labelled data is required for the models. Data was labelled by two researchers with the author providing the casting vote in event of disagreement. The MO text was selected through an active learning strategy as detailed in Chapter 9. On this occasion the labelling data pool was limited to those burglaries committed in October 2018 to end of 2019 (as mentioned in the data chapter October 2018 coincides with the introduction of the new data recording system for PF2). This restriction was to facilitate the investigation into model accuracy over time, the third research question of this study.  Active learning was only conducted for the motor vehicle model, so all PTMs were fine tuned on data that had been selected through active learning for the motor vehicle model. In total 1982 MO texts were read and labelled for the burglary classification models.

\subsubsection{Fine-tuning Models} Fine-tuning PTMs is the training of the model that focusses on the data specific to the classification problem, in this case the MO data. There were no significant differences for the fine-tuning methods of the PTMs for this study, it was completed using the same method outlined in Chapter 9. The BERT large model was used throughout. The hyper-parameters were all set as mentioned in the methods chapter, Chapter 9. 

\subsubsection{Performance} The additional data fields provided with the PF2 data allowed a more in depth investigation into bias than had been possible with the PF1 data. With this study bias related to sex and ethnicity was explored by comparing PTMs across different victim characteristics. Bias was explored using the metrics Equality of Opportunity (EoO) and Predictive parity (PP) (both introduced in the methods chapter). EoO is based on recall and measures the disparity of the probability of a true positive across groups, i.e. given that a classification was positive what was the probability of finding it.  PP is a based on precision and is a measure of disparity on the probability of false positives across groups, i.e. given that the model found the classification was positive, what was the likelihood that the model was correct. 

These metrics were calculated for each test set and a cross-validation experiment was completed. As noted earlier for each bias a reference group is selected in order to see if there is bias between the reference group and the remainder of the population. The reference groups were \emph{White-European} and \emph{Male} throughout, they were therefore compared against \emph{all other ethnicity's} and \emph{females} respectively. Unknown or missing values were excluded from the analysis \footnote{The analysis was also conducted with these missing values included in the comparison groups and there was no significant difference in the result.}. 

The comparison to a basic keyword model was not conducted. The power of the PTMs over the keyword approach had been explained in Chapter 5 and demonstrated in Study 1a. However, a comparison to another another method that police forces may use was possible. Police forces often record some aspects of intra-crime variation as Flags. Flags are typically key words or phrases that a police officer can select that describe a crime. For PF2 these flags are selected through a series of dropdown menus on their crime recording software. These flags are much more easily searchable than the free-text data as they are structured and so are often be used to find crimes of interest. To compare flags against the model the following process was followed. Firstly decide which flags adequately describe the classification type e.g. decide which flags highlight burglaries were force is used. A  list of flags used fo reach classification is at Table \ref{tab:burg_keywords}. Secondly sum the monthly counts of crimes that do and do not meet the criteria of the classification, this is done for both those crimes selected through the flags and those selected through the NLP model. From the sums of positive and negative classifications compute monthly percentage positive classification. Finally plot percentage positive classification as a time-series line plot. Compare line plots.

\begin{table}[]
\centering
\begin{tabular}{p{0.3\linewidth}|p{0.6\linewidth}}
\toprule
Classification                    & Flags                                                                                     \\ \midrule
\multirow{4}{*}{Motor vehicle}    & Instrument Used, Key Used, Stolen                                                            \\
                                  & Instrument Used, Key Used, Key Used                                                          \\
                                  & Property, Conveyance, Car                                                                    \\
                                  & Property, Conveyance, Motorcycle                                                             \\ \midrule
\multirow{2}{*}{Force}       & Trademarks- Attack Method Premises                                                           \\
                                  & Entry Method, Attack Method Premises                                                         \\ \midrule
\multirow{2}{*}{Outbuilding} & Location, Garage - Includes premises for sale and repair but does not include petrol station \\
                                  & Domestic|location, Garden - Driveway, Shed                                                   \\ \bottomrule
\end{tabular}
\caption[Burglary Keywords]{\label{tab:burg_keywords}The keywords used to filter the MO Keywords data column in the PF2 Burglary data. }
\end{table}


As mentioned in the data section for the motor vehicle classification there is an additional validity check - a linked stolen vehicle to the crime. This data was also used to check the validity of the motor vehicle stolen model and is added to the monthly time series plots just described. The calculation method is the same.   

\subsubsection{Model performance over time.} Model performance over time was investigated by fine-tuning on data from one period of time then testing on a separate later period of time. All PTMs were fine-tuned and initially tested on data from late 2018 to end of 2019. For the replication element of this study all MCC metrics were gathered from a test set randomly selected from the same set of dates. However a separate test set was also built for the year 2020, this allowed the model built in the earlier time period to be tested on 2020 data. The year 2020 may not be an ideal comparison as it  was the year in which the Covid-19 pandemic hit the UK.  The pandemic induced severe mobility restrictions as the government tried to curtail the spread of the virus. This pandemic expanded the lexicon of the general public. However the effect of the pandemic on burglary MO texts may be less severe, because it is not immediately clear that the pandemic would change burglary methods and therefore the words used by the police in the subsequent descriptions. However, if there is a significant degradation of model performance across years, then Covid-19 induced variation will be one source of change that will need investigating.     

\subsubsection{PTM transfer-ability} PTMs fine-tuned on PF1 were used to label the PF2 test sets. MCC scores were calculated and were directly compared with the MCC scores from the models built on the PF2 data. So for example the \emph{force used} model built on the PF1 data is used to label the PF2 test set and the labels generated are compared against the \emph{force used} labels. Comparing the two MCC scores allows an understanding of how accurate a model generated in one police force might be in a second police force. 

\section{Results} The results will be presented for the replication study first. The MCC scores and explainability are directly comparable to the earlier study with the PF1 data, as they use exactly the same methods. Estimates of bias are different as the PF2 data has victim characteristics so extrinsic bias metrics were investigated along sex and ethnicity groupings. Following on from the bias results the comparison of the NLP labels with the police recorded keywords will be shown as line plots. After theses results have been explored then the MCC results for both the change in time-period and the reuse of models across forces will be stated.

\subsection{MCC} The MCC results are presented in Table \ref{tab:lancs_mcc}. MCC results are presented with reference to the test sets from both 2018/19 and 2020/21. However before these are explored there is a brief description on how much data was labelled and why.

 As detailed in the Methods Chapter the approach with the active learning strategy was to stop labelling when the MCC reached above 0.9 on the validation set. This level of 0.9 MCC was not achieved with the motor vehicle model (see Table \ref{tab:results_1c}), and for reasons explained in the next paragraph additional labelling was unlikely to increase the MCC to over 0.9.  Consequently the labelling was stopped after active learning batch 16.  

The motor vehicle task was the first model picked for labelling, but all tasks (i.e. also force used and outbuilding only tasks) were labelled at the same time  during this labelling. So at batch 16 there was enough labelled texts to take a view on additional labelling required for the other two classification tasks. The force used classification task achieved a MCC of 0.92 by batch 5 (see Table \ref{tab:results_1c}) so there was no requirement for additional labelling. 

The outbuilding task did not reach 0.9 by batch 16, but the MCC scores had stabilised at circa 0.85 since batch 5, see Figure \ref{fig:mcc_burg_lancs}. It was therefore unlikely that additional labelling was going to increase the MCC score. However one single batch of additional active learning was conducted, with the final selection using a fine-tuned outbuilding model. The resulting model MCC (tuned on 16 batches of motor vehicle and 1 batch of outbuilding selected data) did not increase and therefore further labelling was judged by the author not to be required. The data related to this seventeenth batch is not included for simplicity.

\begin{table}[]
\centering
\begin{tabular}{@{}lcccccccc@{}}
\toprule
Batch       & 1    & 2    & 3    & 4    & 5    & 6    & 7    & 8    \\ \midrule
Motor vehicle          & 0    & 0.48 & 0.65 & 0.73 & 0.56 & 0.82 & 0.8  & 0.82 \\
Force       & 0.52 & 0.71 & 0.82 & 0.88 & 0.92 & 0.93 & 0.88 & 0.92 \\
Outbuilding & 0.33 & 0.23 & 0.72 & 0.84 & 0.87 & 0.85 & 0.85 & 0.86 \\\midrule
Batch       & 9    & 10   & 11   & 12   & 13   & 14   & 15   & 16   \\\midrule
Motor vehicle         & 0.75 & 0.72 & 0.88 & 0.86 & 0.82 & 0.75 & 0.82 & 0.72 \\
Force       & 0.88 & 0.92 & 0.92 & 0.9  & 0.92 & 0.91 & 0.93 & 0.91 \\
Outbuilding & 0.85 & 0.86 & 0.85 & 0.84 & 0.86 & 0.84 & 0.85 & 0.86 \\ \bottomrule
\end{tabular}
\caption[Batch metrics - PF2 data. All models]{\label{tab:results_1c}MCC values (based on the validation set) for models fine-tuned on PF2 Burglary data. Batch refers to the active learning batch e.g. after 5 batches of labelling (500 MO texts) the motor vehicle model had an MCC of 0.56 }
\end{table}


 The model MCC scores are reported in turn in the following subsections. A MCC score of 1 is a perfect score, 0 is equivalent to random. The results discussed below are confined to the 18/19 test set. The 20/21 test set scores are discussed later.  

\subsubsection{Motor Vehicle model} As explained the motor vehicle model was selected as the first model for labelling via the active learning strategy. The values of the MCC after each active learning batch, calculated using the validation data, can be seen in Figure \ref{fig:mcc_burg_lancs} and in Table \ref{tab:results_1c}. The highest value attained was 0.88, which was below the requirement for the stop condition (0.9). The labelling was stopped after the sixteenth batch after it had become apparent that there were no more positive classifications within the pool of potential training data. In other words the active learning strategy had already selected all MO texts with a motor vehicle theft for the training data, there were no more positive samples to learn from in the potential training data. In fact the last positive example had been found in batch 11. From the plot at Figure \ref{fig:mcc_burg_lancs} it is clear that the additional negative examples, forming batch 12 to 16, did not aid the model fine tuning and so further fine-tuning was not deemed necessary. Consequently model fine-tuning stopped at 16 batches and the model was tested on the test set.

\begin{figure}[!tbp]
  \centering
    \includegraphics[width=\textwidth]{images/mcc_burg_lancs.png}
    \caption[MCC scores for the PF2 burglary models.]{{MCC scores for the PF2 burglary models.} MCC scores are shown after each iteration of the active learning strategy. The Force and Outbuilding models peak relatively early on at batch 5 and 6. Whereas the motor vehicle model peaks at 11. Some of the variation will be attributable to the random initialisation of the models. Source: Author generated.}
    \label{fig:mcc_burg_lancs}
\end{figure}


MCC scores on the test set can be found at Table \ref{tab:lancs_mcc}. The fine-tuned model over the 10 runs had a mean MCC score of 0.98, near perfect performance. On half of the runs the model correctly classified each of the 200 MO texts from the test set. The MCC metrics for the motor vehicle model are comparable to the scores from the motor vehicle model built and used on the PF1 data (mean of 0.97). 

\subsubsection{Force model} The force model used the same data labelled from the active learning conducted on the motor vehicle model. The mean MCC score from the final ten initialisations was 0.93 on the 2018/19 test set. These scores are higher than the MCC score for the validation set, indicating that perhaps the validation set had difficult to classify MO texts.  These MCC results are comparable to those models fine-tuned and tested on the PF1 data (mean of 0.91). 


\subsubsection{Outbuilding Model} The MCC scores for the outbuilding model on the test data were again better than the validation set scores. The mean of the ten initialisations on all of the 2018/19 test set is 0.90. This is the lowest score across all three models. The outbuilding model was not built with the PF1 data as the data was not suitable. Therefore there is no direct replication for the outbuilding results presented in this study.



\begin{table}[]
\centering
\begin{tabular}{@{}lcccccc@{}}
\toprule
         & \multicolumn{2}{c}{Motorvehicle} & \multicolumn{2}{c}{Force} & \multicolumn{2}{c}{Outbuilding} \\ \midrule
Run      & 18/19           & 20/21          & 18/19       & 20/21       & 18/19          & 20/21          \\
1        & 1.00            & 0.89           & 0.93        & 0.92        & 0.91           & 0.94           \\
2        & 1.00            & 0.93           & 0.94        & 0.93        & 0.90           & 0.93           \\
3        & 0.94            & 0.88           & 0.93        & 0.94        & 0.90           & 0.94           \\
4        & 0.97            & 0.93           & 0.93        & 0.95        & 0.89           & 0.93           \\
5        & 0.94            & 0.88           & 0.94        & 0.96        & 0.92           & 0.94           \\
6        & 1.00            & 0.91           & 0.90        & 0.93        & 0.91           & 0.94           \\
7        & 0.97            & 0.88           & 0.90        & 0.94        & 0.85           & 0.96           \\
8        & 1.00            & 0.91           & 0.92        & 0.95        & 0.90           & 0.92           \\
9        & 0.97            & 0.90           & 0.92        & 0.96        & 0.91           & 0.94           \\
10       & 1.00            & 0.90           & 0.95        & 0.96        & 0.89           & 0.92           \\\midrule
Mean     & 0.98            & 0.90           & 0.93        & 0.94        & 0.90           & 0.94           \\\midrule
Best Run & 1.00            & 0.93           & 0.94        & 0.96        & 0.92           & 0.96           \\ \bottomrule
\end{tabular}
\caption[Final model MCC metrics. PF2 data. All models.]{\label{tab:lancs_mcc}MCC values (based on the test sets) for models fine-tuned on PF2 Burglary data. Scores are generated from 10 separate fine-tunes based on all labelled data. 18/19 refers to the test set from only the years 2018 and 2019, similarly 20/21 refers to the years 2020 and 2021.}
\end{table}




\subsection{Explainability} LIME was again used to understand how the words of the texts are contributing to the final classification. As a reminder BERT uses the word and the surrounding context of the word, so a global understanding of how the model works is difficult to ascertain. LIME provides a local understanding for each MO text by randomly deleting words to see what effect they have on the final classification. This is scaled up in this thesis by utilising LIME on all MOs in the test set then using word clouds to show the most important words based on the individual LIME model coefficients. Word clouds for the motor vehicle, force and outbuildings model can be seen in Figures \ref{fig:wordcloud_mv_both_lancs} ,\ref{fig:wordcloud_force_both_lancs} and \ref{fig:wordcloud_home_both_lancs} respectively.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/mv_wordcloud_positive.png}
         \caption{Words that contributed to a positive classification}
         \label{fig: wordcloud_mv_lancs}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/mv_wordcloud_negative.png}
         \caption{Words that contributed to a negative classification}
         \label{fig: wordcloud_mv_rev_lancs}
     \end{subfigure}
        \caption[Wordclouds from  \textbf{motor-vehicle} classification model. PF2 data.]{{Wordclouds from  \textbf{motor-vehicle} classification model. PF2 data.} These wordclouds were generated using a fine-tuned BERT model on the PF2 data. The larger a word the more important it is for a classification. Words size is derived from a summation of the coefficients from individual LIME models. Word sizes are not comparable across figures. Source: Author generated.}
        \label{fig:wordcloud_mv_both_lancs}
        
\end{figure}


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/force_wordcloud_positive.png}
         \caption{Words that contributed to a positive classification}
         \label{fig: wordcloud_force_lancs}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/force_wordcloud_negative.png}
         \caption{Words that contributed to a negative classification}
         \label{fig: wordcloud_force_rev_lancs}
     \end{subfigure}
        \caption[Wordclouds from  \textbf{force} classification model. PF2 data.]{{Wordclouds from  \textbf{force} classification model. PF2 data.} These wordclouds were generated using a fine-tuned BERT model on the PF2 data. The larger a word the more important it is for a classification. Words size is derived from a summation of the coefficients from individual LIME models. Word sizes are not comparable across figures. Source: Author generated.}
        \label{fig:wordcloud_force_both_lancs}
        
\end{figure}




\begin{figure}
     \centering
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/home_wordcloud_negative.png}
         \caption{Words that contributed to a positive classification}
         \label{fig: wordcloud_home_lancs}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/home_wordcloud_positive.png}
         \caption{Words that contributed to a negative classification}
         \label{fig: wordcloud_home_rev_lancs}
     \end{subfigure}
        \caption[Wordclouds from  \textbf{outbuilding} classification model. PF2 data.]{{Wordclouds from  \textbf{outbuilding} classification model. PF2 data.} These wordclouds were generated using a fine-tuned BERT model on the PF2 data. The larger a word the more important it is for a classification. Words size is derived from a summation of the coefficients from individual LIME models. Word sizes are not comparable across figures. Source: Author generated.}
        \label{fig:wordcloud_home_both_lancs}
        
\end{figure}




The word clouds for the motor vehicle model demonstrate a similar pattern as those from the PF1 data, see Figure \ref{fig:wordcloud_mv_both_lancs}. Firstly the most prominent words in the word cloud for a positive classification (i.e. a motor vehicle was stolen) are words that a human might expect to use when completing the same classification task. The three most important words being \say{car}, \say{vehicle} and \say{keys}. Also note that these words are disproportionately important, they are much larger than the remainder of the words. This contrast with those words contributing to a negative classification. Figure \ref{fig:wordcloud_mv_both_lancs} panel B, has word sizes that are much more even across all words selected. The words for the negative classification have no observable theme, this is likely because it is not written in the MO when a car is not stolen.

Word clouds for the force model are similar for the positive case (force was used). The model is using words that are commensurate with what a human might use for the same classification task. Although some of the more important verbs are less prominent, and there appears to be more of a focus on nouns than the PF1 data. Overall though the pattern of important words is clear and logical. Unlike the motor vehicle model, the negative classification, no force used, is often reported and so there is a clear pattern to the second word cloud. Figure \ref{fig:wordcloud_force_both_lancs} panel B. Here the word insecure is the most important word for obvious reasons. \say{Unknown} is also prominent as it is often used to say that the method of entry was unknown, reflecting a lack of obvious force used to enter the building. 

The outbuilding word cloud is similar in structure to the motor vehicle model. The positive classification cloud has a smaller amount of disproportionately important words, here \say{shed}, \say{Garage} and \say{garden} being the most important words. The negative classification word cloud has words that are more equitable in size, and in general the word cloud represents words that are encountered throughout all Burglary MOs. Again this is likely because the  negative classification is not explicitly reported.

Each of the word cloud pairs has given confidence that the model is using the words that a human might use in determining the classification of each text. This gives confidence that the models are picking up on the correct features of the text, and not spurious correlations. The next section investigates any bias that the models may have in relation to sex and ethnicity.

\subsection{Bias} Bias within the models was investigated along sex and ethnicity characteristics. The reference groups were male and white-european respectively. Models were investigated by exploring extrinsic bias metrics, equality of outcome and parity of prediction. Table \ref{tab:lancs_bias} has the results from both the models built on the active learning data and also the 10 fold cross-validation experiment models. The results are described in relation to the partition of the data i.e sex and ethnicity rather than by model. As a reminder 0 is no bias, a positive number is bias in favour of the reference group and a negative number is bias against the reference group. The maximum and minimum possible values are 1 and -1 respectively.

\subsubsection{Ethnicity} There are two significant p values from the cross-validation experimentation and they are both for ethnicity. One is for equality of outcome for the motor vehicle model, the other one is for predictive parity for the force model. In both cases the mean shows that the bias is very slightly against the reference group, that is white Europeans are potentially discriminated against by the models. 

When reviewing the results of the model built from the active learning data we see that most values of both equality of outcome and predictive parity are close to zero indicating little bias. Four out of six of the metrics are negative for the equality of outcome and five out of the six predictive parity metrics are negative, again both indicate a slight discrimination against white Europeans. The results are mixed across the models. Bias is only consistent in one direction for the outbuilding model. However, even then bias for outbuilding model does not register a statistically significant result with the cross-validation experiment. In summary there are small values of bias, but not consistent results to indicate systemic bias for PTMs classifying MO texts in the PF2 data.
 
\subsubsection{Sex} The evidence for sex bias is weaker still, there are no statistically significant results from the cross-validation experiments. The predictive parity has an equal number of negative values as it does positive values. For equality of outcome there is one more value of positive than negative. In conclusion there is no evidence of sex bias from the PTM when classifying MO texts for the PF2 data.

\input{bias_table}

\subsection{Flag Comparison} This section compares the NLP generated labels for the three models with flags that the police may search in order to identify the intra-crime variation of interest. In addition the presence of links between stolen vehicles and the burglary is explored i.e. burglaries that have a stolen vehicle linked to that crime.

\subsubsection{Motor Vehicle Model} The time series plot for the motor vehicle model can be seen at Figure \ref{mv_ts}. The police generated labels - both \say{Linked vehicle} and \say{Flagged} should only be fully considered after 2019 because of the aforementioned change in data recording systems. The two striking elements of the plot are firstly that the NLP labels and the linked vehicle labels are very well matched (Pearson correlation coefficient of 0.94) and secondly that the flags return much fewer crimes. This low return from the flags is repeated throughout the classifications. In discussions with the analysts from PF2 they recognised that the flags do not have a high completion rate, though based on their experience they thought that the linked vehicles data would be completed to a high standard. 

To explore the differences between the NLP model and the linked stolen vehicles an error analysis was conducted. The error analysis reviewed one hundred of the MOs where the NLP model had identified a motor vehicle stolen but there was no linked vehicle. In total there were 432 errors of this kind. Of the one hundred MOs checked - 63 did have vehicle stolen in the MO texts and so the NLP model was correct.The remainder (37) were labelled incorrectly by the PTM. The majority of these errors was where only the vehicle keys were stolen and not an actual vehicle (21). Although these latter errors maybe useful in the context of this crime, because keys can be used later to steal a vehicle,  they are not what the model was trained on.   


\begin{figure}
  \includegraphics[width=\linewidth]{images/mv_linked_time_series_plot.png}
  \caption[Motor vehicle model time series plot]{A time series plot of the motor vehicle classification. Showing data generated form the PTM model (NLP), linked vehicles and flags. Source: Author generated.}
  \label{fig:mv_ts}
\end{figure}

\subsubsection{Force Model}The force model only shows a comparison between the flags and the PTM labels. The plot is at Figure \ref{fig:force_ts}. As with the previous time series plot the most notable finding is that the PTM  finds many more burglaries with the use of force than the flag system produced by police officers. Again this finding is consistent with the analysts view that the flag system is not well used. The next notable finding is the apparent seasonality in the PTM labels. Here it can be seen that the proportion of crimes where force is used to enter the building is consistently higher in the winter months than the summer months.   


\begin{figure}
  \includegraphics[width=\linewidth]{images/force_time_series_plot.png}
  \caption[Force used model time series plot]{A time series plot of the force used classification. Showing data generated form the PTM model (NLP) and flags. Source: Author generated.}
  \label{fig:force_ts}
\end{figure}


\subsubsection{Outbuilding Model} The outbuilding model plot only shows the PTM generated labels alongside the police generated flags. The plot is at Figure \ref{fig:outbuild_ts}. As with the other two plots the PTM returns more crimes with the positive classification, although the number returned here are closer than the other two plots. The spike in the two time series in early 2020 coincides with the first Covid-19 lockdown in the UK. This spike may indicate a proportion shift in burglary type as a result of lockdown policies.


\begin{figure}
  \includegraphics[width=\linewidth]{images/outbuilding_time_series_plot.png}
  \caption[Outbuilding only model time series plot]{A time series plot of the outbuilding only classification. Showing data generated form the PTM model (NLP) and flags. Source: Author generated.}
  \label{fig:outbuild_ts}
\end{figure}

\subsection{Model transfer-ability} This section of the results reports on the usage of models from one police force area in another police force area. The results are for  the use of PF1 models on PF2 data, the reverse was not possible due to data security limitations. The MCC results in Table \ref{tab:results_transfer} show that the models are reasonably transferable. In each case the MCC is lower for the transferred model, which as one would expect, but the drop is not that significant in all cases. This demonstrates that models built in one area will have some utility in another force area. The implications of which are discussed later.  


\begin{table}[]
\begin{tabular}{@{}llcc@{}}
\toprule
\multicolumn{1}{c}{Test Set} & \multicolumn{1}{c}{Model} & PF2 Model PF1 Data & PF1 Model PF1 Data \\ \midrule
18/19                        & Motor vehicle             & 0.93                   & 0.98                   \\
20/21                        & Motor vehicle             & 0.80                   & 0.90                   \\
18/19                        & Force                     & 0.91  & 0.93  \\
20/21                        & Force                     & 0.90  & 0.94 \\ \bottomrule
\end{tabular}
\caption[Model metrics. Models tested on alternate police force.]{\label{tab:results_transfer} MCC scores for the use of models built with PF1 data and used to classify PF2 data. PF2 metrics included for comparison. }
\end{table}

\subsection{Performance over time} Test sets were built for 2018/19 and 2020/21, even though the training data only came from 2018/19. This allowed a view of how model performance would vary over time. The results in Table \ref{tab:lancs_mcc} show the results of the 10 model initialisation using the active learning data to fine-tune the PTM. The mean result of the ten initialisations is reported here. For the motor-vehicle model there is a sizeable drop in performance from a 0.98 to 0.90, although a performance of 0.9 may still be adequate depending on usage.  For the force model there is an increase, but it is very small from 0.93 to 0.94 (also note the the 20/21 test set has a higher top scoring run than the 18/19 set). The outbuilding model also increase this time by a larger amount going from 0.90 to 0.94.


\section{Discussion} This section synthesises the results presented in the section above in relation to the main research questions given at the start of the chapter. Each question is explored in turn with comparisons to the original study with the PF1 data.

\subsection{Can the results in Study 1a be replicated in a different police force?}Study 1a set out to understand if PTMs can be utilised to classify MO texts. The two classification tasks were 1)Was a motor vehicle stolen during the burglary? 2) Was force used to enter the building during the burglary?  Additionally Study 1a explored whether the PTMs would be explainable and therefore generate trust that the models were working as human might do, and not relying on perhaps spurious correlations in the data. In study 1a bias was studied to a limited extent due to a lack of victim characteristic data.

Performance results in the replication study were equivalent to the results in the original study, both produce high MCC scores indicating high performing models are possible from fine-tuning PTMs. In addition in the replication study an additional classification problem was explored, that of burglaries into out-buildings only. A model was also fine-tuned on this problem and the resulting model also showed good performance with a high MCC score.

Comparing the labels generated from the PTMS to police generated data we find that the PTMs return much more crimes. Combined with the high MCC scores, and the error analysis with the linked data, this gives confidence that this return is a more accurate reflection of the intra-crime variation. Again this highlights the benefits of the PTMs over existing police processes for exploring intra-crime variation. 

To test explain-ability the LIME model was also used to generate word clouds which showed the most important words for each classification model. As with the PF1 models, the replication study produced word clouds that enhance trustworthiness. The important words highlighted by these clouds were entirely consistent with the words that a human may use to make the classification judgement and therefore they again give confidence that the model is classifying in a way commensurate with how a human would classify the texts.

In the replication study there was more victim characteristic data than the original study, so the models could be explored for bias along ethnicity and sex characteristics. The results showed that there was no evidence of systematic bias in the classifications of the fine-tuned PTMs. It should be noted that within the texts there was very little reference to the particular characteristics mentioned i.e. victim sex and ethnicity generally was not described or referenced to. This means that any bias was likely to be introduced indirectly through systematic variation in language and or quality of the MO rather than explicit mentions. From the bias investigation in the original study, 1a,  we note that length of text and percentage of BERT words was not correlated with either of the bias metrics and so, even if for instance Asian victims had short MO texts the model would not necessarily perform poorly against those MOs. 

As a reminder there \emph{may} be a number of different routes to introduce biases in the chain that leads from a crime being committed to the formulation of a MO text and the subsequent classification. Firstly the crime may not be recorded as the victim may prefer not to interact with the police therefore there is no MO text. If the victim does interact with the police the interaction might be sub-optimal (e.g. due to language barriers) and so the information passed might not accurately describe the crime in full. Finally the PTM is built on data that has been scraped from the internet, this data is almost certainly likely to reflect biases in everyday society and so may perpetuate these into the classifications. The bias investigation in this study can only pass comment on the final route - the use of the PTM. The first two routes are beyond the scope of this study as already explained. The bias results found here therefore indicate that any biases that are inherent in the PTM are not affecting the classification of burglary texts for the problems investigated. 

In summary the replication study has replicated the good results from the first study and has extended them proving that PTMs offer good performance in the classification of burglary texts. Additionally the models are classifying the texts using similar words that humans would use offering evidence that the models are working in a trustworthy manner. From the limited investigations of bias conducted, there is no evidence of systematic bias in the model classifications.


\subsection{Can models trained in one police force area be used in another force?} By replicating the first study in a second police force area the opportunity arose to use the models trained in one police force on the data of another police force. If models performance is carried from one force to another then model utility becomes more enhanced as models can be re-used across forces without the need to share data. The results have shown that models can be transferred from one police force to another and retain a reasonable level of performance. The implications are that models can therefore be shared across forces for direct use or to seed the start of fine-tuning of a separate model and therefore reduce the labelling burden. This may have important practical implications if for instance the knowledge for classifying a model is relatively specialised, for example the detection of modern day slavery. 

That models can be reused across forces also has implications for PTM implementation. In the UK for instance there are 43 police forces, all with similar crime recording techniques, a common language and resource pressures. If models can be shared then a central repository of models would therefore have utility across all forces. A central repository of models would allow maximum sharing and a commensurate reduction of the labelling burden. Additionally the technical aspects of model running and fine-tuning could also be housed centrally reducing the training burden across the 43 forces. Extension of model sharing to this extent would need much more experimentation than is offered here, with a sample size of just two. Never the less the results presented here are encouraging.


\subsection{Are fine-tuned PTMs accurate over time?} As language use changes so will eventually the performance of the models. The language in a MO text is reflective of the intra-variation of a crime, so as the variations changes, for instance in response to a new security technique, then so will the language in the MO texts change. Models will therefore have to be checked to ensure that they remain relevant for the language used. In this study the models were trained on data from one year the tested on data from a subsequent year. There was no perceptible drop in performance in either of the three classifications tasks. This gives confidence that the models are robust to some time change, and this during a change of significant social upheaval i.e. the Covid-19 pandemic. 

However, there is no evidence to suggest that, despite a general decline in burglary, here was any new type of intra-crime variation. Variation that may have changed the language being used in the second time period used for this experiment. This means that whilst we have limited evidence that models are robust to the passage of time, there is no evidence about the robustness to new criminal techniques and therefore a change in language. Changes like this will have to be guarded against and the findings here certainly do not exclude the necessity to check the validity of fine-tuned PTMs over time.


\section{Conclusions}This replication study has provided additional evidence that PTM are able to effectively classify Police MO texts by extending the problem to another police force area and one more additional classification task. In addition the study was able to extend the investigation into bias by showing that there was no evidence of bias in classification on either a gender or an ethnicity basis. The replication study also investigated the utility of models over time, finding no perceptible drop in classification power. This indicates the models will remain useful over extended periods. However, the study was relatively weak,  and a more thorough study would be required for a definitive assessment of the refresh rate for models. 

In addition, models were  shown to be effective on the same problems but on data from a different police force. This suggests model sharing between forces may be possible. Model sharing would considerably lower the labelling, computational and skills burden required to use PTMs. This may prove to be an important aspect with respect to the practicality of implementing PTMs because it would significantly reduce resource costs. It could also indicate that centralised coordination, and perhaps development, of some aspects of work, would be an efficient approach. 

These studies have shown that PTM can be effective with MO text data, across a number of different classification problems. However MO texts are not the only text that police forces have. Another style of text are police incident logs, that encompass crime and non-crime data. The next case study will progress this work  by using PTMs to classify anti-social behaviour incident logs.
 






