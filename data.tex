\chapter{Data and Data Processing}


\section{Introduction} 

This chapter will introduce the data that have been used in the studies within this thesis. This chapter will give background information on the text data that is used with the language models, and how the composition of the data may effect the performance of the models that are utilised. The data was from two main sources, Saferleeds a crime reduction partnership based in West Yorkshire and Lancashire Constabulary. 

The Saferleeds data originated from West Yorkshire Police, though underwent screening by SaferLeeds before being released to The University of Leeds for a number of projects. Lancashire Constabulary data was primarily provided to the University of Leeds for the previously mentioned Covid-19 ESRC funded project. The author contributed extensively to screening of the Lancashire data before it was provided to the University, but screening of the SaferLeeds data was conducted by SaferLeeds themselves with no technical input from University of Leeds.  

As both data sources went through some form of screening to remove personally identifiable information the data is not in the exact same form as the police services would use it, this may have a negative impact on model accuracy, as information will have been removed, however it does mean that if police forces were to utilise the language models introduced later on they should expect better model accuracy as they will have access to all of the data.

\subsection{Chapter Outline}
This chapter will first introduce police textual data  before introducing both datasets, from SaferLeeds and Lancashire Constabulary, in detail. The Chapter will then explain in greater detail how the Lancashire data were prepared and desensitised for analysis away from Lancashire Constabulary servers. This preparatory step is of interest because free text data is highly likely to include personal data and so if researchers want to utilise the data away from police servers then they will have to implement steps to reduce the risk of personal data loss. Removing personal data references from the police free text data also allowed the use of non-vetted personal for data labelling, a time consuming and laborious task, that nevertheless is critical for supervised machine learning tasks. The use of non-vetted personal allows for much more flexibility in recruitment of the data labellers, and so greatly reduced the data labelling burden. 

\section{Police Data} The data used in all studies were exclusively police generated data. In particular the data was also secure police data, in that it originally held personal information and so is not freely available to the public. The use of sensitive police data generates two main problems. Firstly from a practicality perspective the data had to be de-identified, as mentioned above. Secondly as police data does not accurately reflect the totality of crime that is committed \parencite{Tarling}, this will have implications of bias, as introduced in the earlier NLP chapter. The next section introduces the two types of police data used in this study, MO texts and incident logs. Weakness with the police data are returned to at the end of the chapter.


\subsection{Modus Operandi Data} A Modus Operandi (MO) text is usually a short text document of one to three sentences that describes the main elements of a crime, the MO is but one element of data recorded about a crime. MO data is not explicitly generated for crime prevention work. MO data is designed to be a short description of a crime that can be released to other agencies, typically within the criminal justice system. This influences the content of the MO data, such that it should not contain personally identifying information or excessive details such as lists of stolen items. MO data makes up only a small portion of the data, including the free text data, that is recorded about an individual crime. Underneath the MO data sits a more fuller incident summary that contains more information as well as more detailed incident descriptions from witnesses.  Examples of MOs can be seen in Table \ref{tab:example_mo}.

The selection of MO data had two benefits. Firstly the text passages were a relatively short but condensed description of the crime - as text passages become long they are much more computationally expensive to compute. Of course the trade-off with short text is that it can lack details about the crimes they describe.  The second and perhaps more important was the lack of personal data in the text, this gave the police forces more confidence to share the text with us. Undoubtedly other sources of textual information, incident summaries and witness statements for example, will have more information to extract, but they are also riskier to share as they are likely to contain more identifiable data. MO data was therefore a pragmatic compromise between data security and data utility.

Typically the MO data for each crime is also accompanied by flags that help to explain intra-crime variation. Intra-crime variation here means the variation between crimes of the same administrative designation. As an example Residential burglary is an administrative crime classification, but within that crime type there is variation such as the use of force or not to enter the property. Flags help to systematically (i.e. not in free-text) record intra-crime variation that is not otherwise recorded in the mandatory recording fields. Typically flags are an additional field that the police officers select to record specific details about a crime, such as the entry point of a burglary, or the use of a weapon in an assault. They are the digital equivalent of a check box at the end of a form. As the fields are not mandatory the completion rates can be poor, and in the studies within this thesis we are able to compare the NLP models to the Officer generated flags giving an indication of completion rate.

In summary MO texts are short selective descriptions of crime, that only positively report the known key events of a crime. They are however edited and designed to give a coherent understanding of the crime. They may contain identifiable information, and quite often they are complemented by a series of flags that give further systematic detail on intra-crime variation.

\begin{table}[]
\centering
\begin{tabular}{p{0.1\linewidth}p{0.8\linewidth}}

\toprule
MO 1 & Attacked property is a privately owned end terrece multi occupancey dwelling. Between times stated suspect/s enter through insecure ground floor window. Tidy search conducted and vehicle keys removed from kitchen hooks. Suspect make their escape through same and leave stealing vehicles. Vehicle XXXXX found burnt out                                                               \\ \midrule
MO 2 & Modus operandi summaryâ€¦..Attacked property is a mid-terraced property located on a quiet residential street. Between times stated unknown suspect approaches the front of the property and with bodily force kicks open the basement window. Suspects gain entry to the property and untidy search in conducted. Suspects exit property with stolen items and make off in unknown direction \\ \bottomrule
\end{tabular}
\caption{\label{tab:example_mo} Two example MOs from the SaferLeeds data. Reproduced from Birks et al 2020}
\end{table}


\subsection{Incident data} Incident data is collected on all issues that are reported centrally to the police. Typically these reports are completed by members of the public verbally through the use of emergency and non-emergency phone numbers to a central call station. However they are also increasingly made using other messaging techniques such as email and online reporting tools. 

Police incident logs are generated as the information is received, they are the first record of an incident and they may or may not include a crime. For the purposes of this study the textual log data received only included incidents that were classified as anti-social behaviour, so they are were not designated as crimes. Logs can include the initial report, the first interactions of Officers as they attend the scene and subsequent reports. These subsequent reports can contradict the original report or add explanatory detail. Generally the logs are not edited, or rationalised to depict a single coherent narrative as with the MO data. This makes comprehension of the log difficult. For instance a report may be made that a Covid-19 rule has been broken, but subsequent reporting from police officers may confirm that no rules was broken because of the relationships between the alleged offenders.

As demonstrated below logs are generally longer and have more word variation than MO texts. They also did not come with additional flags to help systematically record intra-incident variation.


\begin{table}[]
\centering
\begin{tabular}{p{0.1\linewidth}p{0.8\linewidth}}

\toprule
Incident text 1 &                                                               \\ \midrule
Incident text 2 &\\ \bottomrule
\end{tabular}
\caption{\label{tab:example_incident} Two example incident texts from the Lancashire data.}
\end{table}




\section{SaferLeeds Data} The SaferLeeds data comprises crimes committed in the West Yorkshire police area. Two years worth of crimes were provided. Though the years were not specified. Crimes of a sexual nature and or related to domestic abuse were also withheld. All of the data fields supplied with the data can be found in Table \ref{tab:data_fields_saferleeds}. The MO texts came from the Crime Notes column and the flags came from the MO Description column. The SaferLeeds data went through processes unknown to redact identifiable information from the MO texts before it was given to the University of Leeds. Only the Burglary crimes from the SaferLeeds data were used and these are described more fully below.

\subsection{Burglary MO Data} The SaferLeeds data contained 9818 Burglaries. As mentioned previously the year of the crimes was not given but the day of the week and the month was given (Table \ref{tab:data_fields_saferleeds}). The median number of words in a MO text is 65, with the inter-quartile range being (48,88), see Table \ref{tab:corpus_stats} for a comparison with the Lancashire data. The longest MO was 403 words long.


As the main pre-trained lanaguage model to be used was BERT, it is also worth exploring if BERT will recognise the words used in the text. As mentioned previously BERT can only recognised certain words or tokens, if the words are not recognised then they are broken into word pieces that are then recognised, although they may not have the same meaning as the original word. By comparing the MO text with the BERT word list it can be seen that BERT recognises 96\% of the words  (by volume), the remaining 4\% of words are broken into word pieces which the BERT model recognises. As BERT is trained on books and Wikipedia text, not police records, it is worth exploring which of the words in the MO text that BERT does not recognise. Table \ref{tab:non_bert_words}, shows the top ten words by volume of those words that are not recognised by BERT. The table shows that there are words, for example insecure, that may have an important bearing on describing the crime that are not recognised by the BERT model as a single word. Although they will be broken into word pieces and not removed, this disassembling of the word  may be a source of error that prevents the BERT language models from classifying the texts correctly.

\begin{table}[]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\multicolumn{3}{c}{Crime Data Fields}                          \\ \midrule
URN & Crime Type       & OccType      \\
Day           & Month          & PartialPostCode                \\
MODescription & CrimeNotes*             & HOClass            \\
OffenceRec  &  DomViol          &           \\ \bottomrule
\end{tabular}
\caption{\label{tab:data_fields_saferleeds} A table of all the data fields for the SaferLeeds crime data.* Indicates a free text field.}
\end{table}


\section{Lancashire Constabulary Data} The second source of data was from Lancashire Constabulary. The main difference between the Saferleeds data and the Lancashire data is that the Lancashire data included crime data and police incident data. The passage of data from Lancashire Constabulary to Leeds university was also more closely controlled by the author. The data extract specifications was built alongside the Lancashire police analysts and the data was extracted together. In addition the author built the de-identification process, described in detail in the next section, that was used to de-identify the free-text data. The Lancashire data contained both structured and unstructured data fields. The initial data was transferred in January 2021, then there was a secondary data transfer, in February 2022 that allowed additional fields to be extracted for model verification purposes. All crime data fields are shown in Table \ref{tab:data_fields_crime}, incident data fields are shown in Table \ref{tab:data_fields_inc}. The review of the data in this section will focus on three data subsections. Lancashire burglary MO data, which was used to replicate analysis of the Saferleeds data used in study 1. The second data set explored is Actual Bodily Harm (ABH) MO data that is used in study 3 to review how police coded flags may be used to reduce labelling overheads. The third and final data set is anti-social behaviour (ASB) police incident logs that was used to investigate the use of pre-trained language models outside of MO data, police incident logs differ from MO data in being significantly longer and less well edited.  


\input{data_fields_table}


\subsection{Lancashire Burglary MO Data} The Lancashire burglary data consisted of just over twelve thousand reported crimes. It includes all residential burglaries and attempted residential burglaries committed from 1st January 2018 to 31 December 2020. Each reported crime contained a MO text. The median number of words for an MO text is 31 (IQR 22,46). Comparing the Lancashire burglary data to the SaferLeeds data we find that in general it is shorter and more homogeneous, see Table \ref{tab:corpus_stats}, so possibly less descriptive. We would therefore expect models to be poorer as there is less variation in the data on which to discriminate. After the modelling was complete Lancashire Constabulary released additional data to help quantify the effectiveness of the classification model built to identify when a car was also stolen. Lancashire Constabulary provided the results from a data search that showed when a vehicle had been linked to a burglary, and the link of association was 'stolen'. Typically they expect this field to be more complete than text references in the MO data to a stolen vehicle - so it can not be used as a direct metric as the language models can only analyse information stored in the free text data. That is given the selective nature of free-text data a car maybe stolen and logged as linked to the burglary but not mentioned in the free-text MO description. For a complete list of fields provided see Table \ref{tab:data_fields_crime}.

As with the SaferLeeds data we also explore to what extent the words used in the Lancashire MOs are contained within the BERT model vocabulary. By comparing the MO text with the BERT word list it can be seen that BERT recognises 96\% of the words (by volume), the remaining 4\% of words are broken into word pieces which the BERT model recognises. Table \ref{tab:non_bert_words}, shows the top ten words by volume of those words that are not recognised by BERT. Comparing with the SaferLeeds data it can be seen that the overlap with the BERT vocabulary is virtually identical, the top ten words are different, but again there is overlap both in the exact words that were missing from the BERT vocabulary and the meaning of words. This overlap gives confidence that what works for one police data set will work for the other. 

%https://stats.stackexchange.com/questions/325549/how-to-measure-dispersion-in-word-frequency-data
\subsection{Lancashire ABH MO data} Lancashire ABH MO data is used in the final study to build models to detect crimes where Domestic Abuse has been recorded in the MO. 28,882 ABH crimes were identified to build this data set. Crimes were committed during the period from 1 January 2018 to 31 December 2020 and are extracted from the whole of the police force area. ABH MO texts have a similar structure to the burglary texts though they tend to be shorter, Table \ref{tab:corpus_stats}. The median words per document is 22 with the inter-quartile range being (15,33). The gini coefficient is the same at 0.94 so the concentration of the words is very similar to the burglary texts. Similarly to the burglary texts 4.5\% of the tokens in the MO texts are not recognised by the BERT models, however as can be seen from Table \ref{tab:non_bert_words}, the most frequent non-BERT words are different. In this case the most frequent non-BERT words are clearly more aligned with physical altercations between people. ABH MO data had the same data fields provided as the burglary data, listed in Table \ref{tab:data_fields_crime}.

\subsection{Lancashire ASB Incident Logs}As described earlier Incident logs are different to MO data in that they are generated primarily through reports made by members of the public. Incidents do not have to be crimes, and indeed the incidents that text data was received for were not classified as crimes. The incident logs had all been classified as Anti-social behaviour. Incident logs are typically much longer than MO data, as can be seen from \ref{tab:corpus_stats} the median words in a document is over fives times greater than that of the Burglary data standing at 166 for an ASB incident log. The inter-quartile range of word counts is also much larger at (100-290). The gini coefficent is also larger suggesting that the word variability is lower than the MO text. This may be an artefact of two data processing and recording issues. Firstly some of the incident logs come from emails and online reporting forms and these methods contain the field names associated with the inputs. These field names are therefore repeated across texts without variation and so will increase the gini coefficient. The second is that more words were de-identified in the police logs than the MO texts - that therefore more of the rarer words were changed to the same word i.e. xxxxx this change due to processing will increase the gini coefficent as word variation will be artificially lower. Table \ref{tab:non_bert_words} shows the most common words from the ASB documents that will not be understood fully by the BERT model. in this instance the words that  tend to be abbreviations. Of note here is that 'covid' is not recognised, this is because when the BERT model was trained (2018) covid was not quite as infamous as it is now in the 'post-pandemic' era. In total 11.1\% of words in the incident logs are not recognised in their complete form by the BERT model. Again this is higher than the MO data, but not unexpected as the ASB log uses more place names VRNs and telephone numbers as the MO data because the logs are not designed for external use. The Incident data has a smaller subset of data fields than the MO data, the fields provided are listed in Table \ref{tab:data_fields_inc}.

\begin{table}[]
\centering
\begin{tabular}{p{0.15\linewidth}p{0.2\linewidth}p{0.2\linewidth}p{0.2\linewidth}} 
\toprule
                         & Median Words per document & IQR  Words per document & Gini coefficient (Concentration) \\\midrule
SaferLeeds Burglary MO    &          65                 &           (48,88)              &                                                          \\
Lancashire Burglary MO &          31                 &    (22,46)                     &         0.94                               \\
Lancashire ABH MO       &            22               &        (15,33)                 &            0.94                                \\ 
Lancashire ASB Logs      &           166                &         (100,290)                &           0.97                           \\  \bottomrule
\end{tabular}
\caption{\label{tab:corpus_stats} This table contains descriptive statistics on the different text corpus used in the thesis. Median was used as the average as the distribution of words is skewed. IQR stands for inter quartile range and is the 25th and 75th percentiles. Gini coefficenient express how equal the frequency distribution of words, a value close to 1 indicates that there was very little variety in the use of the words. }
\end{table}

\begin{table}[]
\centering
\begin{tabular}{p{0.2\linewidth}p{0.7\linewidth}}
\toprule
Data Set& Top Ten Non-BERT Words                                       \\ \midrule
SaferLeeds Burglary MO       &  egress, insecure, untidy, complainant, upvc, terraced, semi-detactched, occupant, jemmy, comp                                                                                        \\
Lancashire Burglary MO       & XXXXX, undetected, insecure, untidy, aggrieved, terraced, burglary, trespasser, UPVC, unoccupied \\
Lancashire ABH MO             & XXXXX, aggrieved, bruising, altercation, reddening, intoxicated, undetected, agg, assaulting ,soreness    \\
Lancashire ASB Incident text &  XXXXX , inf , covid, npt , cctv , nuisance , informants , pls , pcso , fcm       \\ \bottomrule
\end{tabular}
\caption{\label{tab:non_bert_words} This table contains words that are not in the BERT language model list but are in the police data used. Only the top ten missing words by volume are listed. The words that do not appear in that list will be broken down into word pieces and so meaning may be lost. XXXXX is the symbol for redacted words.}
\end{table}
\section{Data cleaning and de-identification} This sections sets out the steps for the de-indentification the Lancashire data that were undertaken before it could be removed from the Lancashire constabulary servers. Whitelisting was used as the method to de-identify the data. De-identifying the data is the process of removing personally identifying information from the data. For structured data this is generally a trivial task, for example removing the second half of a postcode generalises the data sufficiently such that individuals can not be identified through location data even in sparsely populated areas. However free text data is not structured and although there is guidance on what should and should not go into the free-text fields, essentially there are no limits on what an Officer can input with regards to personal information. Full names, addresses, date of births can all be entered into a free text box without technical issue, even though procedurally they should not be entered. In some instances, for example the incident data, personal information is expected and a necessary part of the data being logged. However, MO data is designed from the outset to be released to third parties, though principally still within the criminal justice system, and so should not routinely contain personal identifying information.

Medical research has studied the issue of de-indentifying data extensively, models built for this task range from simple rule based models to more intricate NLP based models \parencite{meystre2010automatic}. However there is certainly no consensus that any of these models work perfectly in all situations \parencite{narayanan2014no}. Each de-identification model style has downsides, the machine learning models require a lot of labelled data to train, and tend to be difficult to explain. The rule based models also require knowledge of the data and are not robust against unseen phrases within the data. For this research the most important characteristics for the de-identification process where easily explainable rules and a risk adverse approach.

Due to limitations of exposure to the data precluding confidence in more sophisticated models, and the need for a simply-explained and a low risk approach the author chose to use a whitelist method. Whitelist methods uses a list of safe words. If a word in the police free text data is not on the safe list then it is redacted from the police text. The resulting text is therefore only constructed from the words on the safe list. This is a simple de-identification method, which is easy to explain and deterministic, but has the downside of potentially redacting rare but important words. 

The next section will explain the whitelist procedure in more detail by exploring the practical implementation of this process. Data cleaning was used alongside this process to homogenise the text such that it improved the retention rate of the de-indentification process. As an example of data cleaning rudimentary spell checking was conducted so that incorrectly spelt words were corrected and therefore matched a word on the whitelist where appropriate. 


\subsection{Data cleaning} In addition to screening the data for de-identification there was also data cleaning to homogenise the text so that information was not lost when certain words or tokens were removed. This included spelling correction, replacing jargon and replacing detailed information with a representative placeholder. The different aspects of this process are discussed below:

\begin{enumerate}
    \item{Misspellings.} Common misspelling were identified. These were added to a misspellings list. This misspelling list was then used to correct words in the MO text before it was de-identified. There were just over 900 common misspellings and typography's identified that were then corrected.
    
    \item{Jargon.} Although jargon would be identified through the process other processes later, some of the words or phrase were changed to represent words that were more likely to be recognised by the PTMs. An example was changing m/v to motor-vehicle. 
    
    \item{Placeholders.} This was replacing particular pieces of information with the type of information that it was. For example, it is relatively common for a vehicle number plates to be recorded in the text, but adding every number plate to the safe word list is neither practical or desirable, but noting the fact that there was a number plate value present is potentially key information. Most vehicle number plates in the UK tend to follow a specific format and because of this it is relatively easy to identify that particular sequence using simple pattern recognition tools (regular expression). As part of the data cleaning this pattern was recognised and was replace with the token 'NUMBERPLATE', this had the advantage of keeping the information that a number plate had been recorded in the text, but not the personal information related to the actual number plate value.
\end{enumerate}


\subsection{De-identification Process Overview} The de-indentification process to remove personal data was based on a white list approach. Words are removed from the police free text if they are not on the list of approved words, referred to here as the safe list. The safe word list is built in an iterative manner. The bulk of the words are originally seeded through a list of commonly used English words, this is then compared against the unique words from the police free text data. Those words that are in the police free text data but not on the safe list are arranged in frequency order (the most commonly used words are at the top). This frequency list is then reviewed and all words that are deemed safe are then added to the safe words list. At this stage common misspellings can also be identified and added to a data cleaning list. This process can be seen in Figure \ref{fig:whitelist}. At the end of the process there is a list of safe words to keep and a list of transformations that can correct common misspellings or abbreviations.


\begin{figure}[!ht]
  \centering
    \includegraphics[width=\textwidth]{images/Slide1.png}
    \caption{{Whitelist Cycle.} This depicts the cycle to generate the whitelist, which also includes data cleaning to remove spelling and typographic errors. First the whitelist was seeded with an existing list of 5000 common English words. This list was then compared against the police free text and those words that were not on the safe word list were counted and presented in a frequency table. Words in the frequency table are reviewed and those that were not names are added to the safe word list.}
    \label{fig:whitelist}
\end{figure}

\subsection{Base word list} In order to seed the process of generating the safe word list a base list of common English words is required. There are numerous word lists that have been created, however they tend to have a flaw for this usage, and that is that they have been generated from data, for example Wikipedia, that contains names. What was required was a word list that was not just generated through simple frequency lists, and so was unlikely to have common names.

The Oxford 5000 \footnote{https://www.oup.com.cn/test/oxford-3000-and-5000-position-paper.pdf} is a list of what are thought to be the most important words to learn for those learning English, as it was not just based on word frequency it did not contain common names, therefore this list  was used as the base for the list of safe words. As part of forming the base word list , the Oxford 5000 was compared against name lists, principally name lists from the Office for National statistics \footnote{https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages /livebirths/adhocs/008710babynames1996to2016} that contain all forenames and surnames used in England and Wales, to see if words that could be names were used in the list. Throughout this process, because of the variety of names that can be used, there needs to be a balance of risk. As an example in the ONS list of forenames the name 'A' is given, clearly because 'A' is such a popular word and a very rare name then most, if not all, usages of the word A in the MO text are not likely to be referring to an individual. 

Therefore all words from the Oxford 5000 base list were checked against the name lists and a judgement made on whether the word should remain in the safe list or not. Although 220 of the words were also in the ONS names lists no words were removed from the base list as they were all deemed relatively obscure names.


\subsection{Developing the safe word list} The safe word list was further developed by comparing the safe word list with all unique words in the police free text see Figure \ref{fig:whitelist} with a minimum frequency greater than 10. If the word from the police text was not in the safe word list then it was manually reviewed by the author and if deemed sufficiently safe i.e was not likely to impart personal information then it was added to the safe word list in order to allow more of the free-text through the de-identifcation process.

The unique words generated from the police text will contain normal English words, police jargon and misspellings. Additional words that were added to the safe word list were called police words, as they had been generated directly from the police free text data. Examples of \emph{Police} words found in the free text data that were not in the original safe list include \say{complainant}, \say{stated} and \say{suspects}. 5205 additional police words were added to the base word list.

This process of adding additional words to the safe word list was only completed with, and therefore tailored to, the MO data. There was not sufficient time to tailor the process or the resulting word lists to the police incident data. When the police incident data was de-identified it was completed using the wordlists generated from the MO free text data. The effect of this is to remove more text from the incident logs that maybe necessary. As an example the \say{:} symbol was not used in the MO texts and so was not added to the safe word list along with other standard punctuation, whereas it is used in almost every incident log. Therefore every incident log now has the redacted symbol \say{XXXXX} instead of every \say{:}.

Once the safe word list was generated it was then used to de-identify the police text. This step is explained below.


\subsection{De-Identification Process} Once the safe word list had been produced the final data cleaning and de-identification process was completed in the following steps, see Table \ref{Example_deident} for an example output:

\begin{enumerate}
    \item Homogenise text. Tidy the text to remove unnecessary pluralisation's, change jargon and correct common spellings.
    \item Replace Information. Use pattern identification to replace known data types with their category so for example replace an actual number plate value with 'NUMBERPLATE'
    \item Whitelist the text. Check every word in the text with the safe list. The safe list is made of the original base word list and the police words. If the word is in the safe list it is allowed to remain. If the word is not on the whitelist then it is replaced with 'XXXXX'
\end{enumerate}

\begin{table}[]
\centering
\begin{tabular}{p{0.2\linewidth}|p{0.7\linewidth}}
\toprule
Example MO       & Suspect(s) unknown steal zebra pattern clothes and hit vctim. They leave in a car vl51pld towards big hill. Hitting Philip Schofield as they flee. \\ \midrule
De-Identified MO & Suspect unknown steal XXXXX pattern clothes and hit victim. They leave in a car NUMBERPLATE towards big hill. Hitting XXXXX XXXXX as they flee.    \\ \bottomrule
\end{tabular}
\caption{\label{Example_deident} This table depicts a single example MO, not real, before and after the de-identification process.}
\end{table}


\subsection{Data Security} This process is not perfect, in the context of names, there still exists the possibility that a persons name will get through if it is made up of normal words e.g. May Summer. However because of additional procedural controls in place, this process was deemed sufficiently robust to reduced the vast majority of risk. The additional security measures in place were the data infrastructure that was used to secure the data and procedural process . This data infrastructure heavily restricted data export and only allowed access to the data by named members of the research team. 

\subsection{De-identification Results} The de-identification process was not formally tested with the MO data. That is the MO data has not been systematically searched for personal data to understand what percentage of personal data, principally names and addresses, passed through the de-identification processed untouched. What is known however is how much of the original text data was recovered. This metric is of interest because significant effort is made to develop more sophisticated techniques principally to reduce the result of false-positives i.e. removing non-personal data. For example in Table \ref{Example_deident} it can be seen that the word zebra is removed because in the context of police MO text data it is a rare word, however arguably there is no need to remove this word and to do so can unnecessarily lose information on which to build the language models. 

For police MO texts the data recovery rate was 97\%, that is 97\% of the words, by volume,  used in the police MO texts were not redacted. The police incident log  text was lower at 92\%. The police incident data was expected to have a lower recovery rate for two reasons, firstly the word lists were not optimised on the police incident text and secondly the police incident text is expected to contain personal data and so more text is expected to be removed. The MO data retrieval rate was high, and anecdotally from the those researchers that read the texts, comprehension was not overly affected by the redaction of words. The police incident data redaction rate was higher, because the incident text was already noisy and unedited, the impact on comprehension is difficult to judge, but was certainly thought to have had a greater impact on comprehension than the loss of words for the MO data did.  

\section{Data limitations} The data used in this thesis is limited in a number of key ways. The limitations are generated throughout the data generating process right up to and including the choice of language model used. The key limitations are highlighted below:

\begin{enumerate}
    
    \item Police Data Coverage. That police data does not cover all crimes committed, and the paucity in coverage is not random - it is systematic. This non-random coverage is not new and is well documented \parencite{Tarling}. However it does mean that any patterns or insights drawn through using these techniques with police recorded crime will be subject to these same biases. This is a well known problem and is also a problem when using police structured information.
    
    \item Information completeness. The texts that are provided are not complete representations of the crimes or incidents that they describe. This incompleteness is in some ways deliberate, the police officers or staff only report positively not negatively e.g. they do not generally report on what doesn't happen. Secondly the completeness may be non-deliberate through bias, Officers can only report what they know and as its widely reported that certain sections of the community to do not engage as fully with Police Officers as others \parencite{buil2021accuracy}, it is entirely plausible that police crime descriptions, and therefore the information that they contain are biased. A future area of study would be to analyse crime descriptions across victim and geographical characteristics to ascertain if they are systematically different in their percentage coverage of the key facts.
    
    \item De-identification. The de-identification process will have removed information from the police texts. this was an unavoidable step for this research in order to provide a reasonable level of data security. The information removed will also have been biased towards rarer words, as the de-identification process was biased to keeping more popular words. Although its worth noting again that police staff using these models on their own data within their own systems would not have to complete this step.
    
    \item Model compatibility. Pre-trained language models have a list of words that they are trained to recognise. If a word is used that is not on that list then it is broken down into word pieces that can then be recognised. As these language models were not built on police data there are certain words (see Table \ref{tab:non_bert_words}) that are not recognised by the language model but are frequently used by the police to convey information. As these words will be broken down into pieces it is plausible that meaning will be lost. The impact of breaking down specific informative words is unknown and is important issue for future research.
    
\end{enumerate}


All of the factors above will have contributed to limitations within the data. Some of those factors are inherent to police data and have been well studied for examples issues surrounding data coverage, however other issues such as biases in police textual data are not well studied and will require further study to understand the extent of the bias and errors that they may introduce.

\section{Conclusion} This chapter has introduced the data that is to be used in the resulting studies. All  of the data to some extent was changed in order to facilitate research access. These changes are likely to have a detrimental affect on the ability of the language models, but not excessively as we have seen with the de-identification that the vast majority of text was unchanged. The next chapter will explore the methods that were used across each study with the data presented here.







