\chapter{Data and Data Processing}


\section{Introduction} 

This chapter will introduce the data that have been used in the studies within this thesis. This chapter will give background information on the text data that is used with the language models, and how the composition of the data may effect the performance of the models that are utilised. The data was from two police forces, 1) PF1  and 2) PF2. Both police forces are located in the North of England. 

The PF1 data underwent screening by PF1 before being released to The University of Leeds for a number of projects. PF2 data was primarily provided to the University of Leeds for the previously mentioned Covid-19 ESRC funded project. The author contributed extensively to screening of the PF2 data before it was provided to the University.

As both data sources went through some form of screening to remove personally identifiable information (de-identification) the data is not in the exact same form as the police services would use it. Redaction may have a negative impact on model accuracy, as information will have been removed, however it does mean that if police forces were to further utilise the PTMs they should expect better model performance as they will have access to the raw data.

\subsection{Chapter Outline}
This chapter will first introduce police textual data  before introducing the datasets from PF1 and PF2 . The Chapter will then explain in greater detail how the PF2 data were prepared and desensitised for analysis away from PF2 servers. This preparatory step is of interest because free text data is highly likely to include personal data and so if researchers want to utilise the data away from police servers then they will have to implement steps to reduce the risk of personal data loss. Removing personal data references from the police free text data also allowed the use of non-vetted personnel for data labelling, a time consuming and laborious task, that nevertheless is critical for supervised machine learning tasks. The use of non-vetted personnel allows for much more flexibility in recruitment of the data labellers, and so greatly reduced the data labelling burden. 

\section{Police Data} The data used in all studies were exclusively police generated data. In particular the data was also sensitive police data, in that it originally held personal information and so is not freely available to the public. The use of sensitive police data generates two main problems. Firstly from a practicality perspective the data had to be de-identified, as mentioned above. Secondly, and more generally,  as police data does not accurately reflect the totality of crime that is committed \parencite{Tarling}, this will have implications of bias, as introduced in the earlier NLP chapter. The next section introduces the two types of police data used in this study, MO texts and incident logs. Weakness with the police data are returned to at the end of the chapter.


\subsection{Modus Operandi Data} An MO text is usually a short text document of one to three sentences that describes the main elements of a crime. The MO is but one element of data recorded about a crime. MO data is not explicitly generated for crime prevention work. MO data is designed to be a short description of a crime that can be released to other agencies, typically within the criminal justice system. This influences the content of the MO data, such that it should not contain personally identifying information or excessive details such as lists of stolen items. MO data makes up only a small portion of the data, including the free text data, that is recorded about an individual crime. Underneath the MO data sits a more detailled incident summary that contains more information as well as more detailed incident descriptions from witnesses.  Examples of MOs can be seen in Table \ref{tab:example_mo}.

The selection of MO data had two benefits. Firstly the text passages were a relatively short but condensed description of the crime - as text passages become long they are much more computationally expensive to compute. Of course the trade-off with short text is that it can lack details about the crimes they describe.  The second and perhaps more important was the lack of personal data in the text, this gave the police forces more confidence to share the text with us. Undoubtedly other sources of textual information, incident summaries and witness statements for example, will have more information to extract, but they are also riskier to share as they are likely to contain more identifiable data. MO data was therefore a pragmatic compromise between data security and data utility.

Typically the MO data for each crime is also accompanied by flags that help to explain intra-crime variation. Intra-crime variation here means the variation between crimes of the same administrative designation. As an example residential burglary is an administrative crime classification, but within that crime type there is variation such as the use of force or not to enter the property. Flags help to systematically (i.e. not in free-text) record intra-crime variation that is not otherwise recorded in the mandatory recording fields. Typically flags are an additional field that the police officers select to record specific details about a crime, such as the entry point of a burglary, or the use of a weapon in an assault. They are the digital equivalent of a check box at the end of a form. As the fields are not mandatory the completion rates can be poor, and in the studies within this thesis we are able to compare the NLP models to the Officer generated flags giving an indication of completion rate.

In summary, MO texts are short  descriptions of the means by which crimes were undertaken. Generally, this included the known key events of a crime. They were designed to give a coherent overview of the crime. They may contain identifiable information, and quite often they are complemented by a series of flags that give further systematic detail on intra-crime variation.

\begin{table}[]
\centering
\begin{tabular}{p{0.1\linewidth}p{0.8\linewidth}}

\toprule
MO 1 & Attacked property is a privately owned end terrece multi occupancey dwelling. Between times stated suspect/s enter through insecure ground floor window. Tidy search conducted and vehicle keys removed from kitchen hooks. Suspect make their escape through same and leave stealing vehicles. Vehicle XXXXX found burnt out                                                               \\ \midrule
MO 2 & Modus operandi summary…..Attacked property is a mid-terraced property located on a quiet residential street. Between times stated unknown suspect approaches the front of the property and with bodily force kicks open the basement window. Suspects gain entry to the property and untidy search in conducted. Suspects exit property with stolen items and make off in unknown direction \\ \bottomrule
\end{tabular}
\caption[MO examples from PF1]{\label{tab:example_mo} Two example MOs from the PF1 data, complete with errors. Reproduced from Birks et al 2020}
\end{table}


\subsection{Incident data} Incident data is collected by police on all issues that are reported centrally. Typically these reports are made by members of the public verbally through the use of emergency and non-emergency phone numbers to a central call station. However, they are also increasingly made using other messaging techniques such as email and online reporting tools.

Police incident logs are generated as the information is received. They are the first record of an incident and they may or may not include a crime. For the purposes of this study the textual log data received only included incidents that were classified as anti-social behaviour, so they were not designated as crimes. Logs can include the initial report, the first interactions of Officers as they attend the scene and subsequent reports. These subsequent reports can contradict the original report or add explanatory detail. Generally the logs are not edited, or rationalised to depict a single coherent narrative. This can make comprehension of a log difficult. For instance a report may be made that a Covid-19 rule was broken, but subsequent reporting from police officers may confirm that no rules were broken.

As demonstrated below logs are generally longer and have more word variation than MO texts. They also do not typically come with additional flags to help systematically record intra-incident variation.


\begin{table}[]
\centering
\begin{tabular}{p{0.1\linewidth}p{0.8\linewidth}}

\toprule
Incident text 1 &  brother is throwing bricks at the window xxxxx he has mh issues - he is called xxxxx xxxxx xxxxx this has happened after an argument xxxxx xxxxx is outside the property now xxxxx xxxxx is shouting outside the house xxxxx no damage caused at the moment but is now throwing stones at the top floor flat given out xxxxx dob - xxxxx last name xxxxx xxxxx first name xxxxx xxxxx xxxxx birth date xxxxx relation type xxxxx 06 crime intelligence xxxxx xxxxx has anger management xxxxx . house is locked and secure xxxxx xxxxx xxxxx desc - white male , medium build , 5 ft , 9 , xxxxx brown hair , dark blue jacket xxxxx , light grey pants xxxxx still screaming xxxxx xxxxx symptoms of covid or xxxxx in xxxxx xxxxx xxxxx xxxxx had left prior to our arrival . there is no damage and no trace of him . no reports . cdit review - no ammendments to log as no offences disclosed .                                                             \\ \midrule
Incident text 2 & an email request has been made . default email notification has been made to xxxxx xxxxx . com . email received in fcm 22/10/2020 at 07 xxxxx 36 reference number xxxxx xxxxx incident relates to xxxxx individual location address xxxxx 1 xxxxx xxxxx street name of persons involved if known xxxxx xxxxx and her son is the subject displaying any covid 19 symptoms xxxxx yes time of incident xxxxx 07 xxxxx 30 date of incident xxxxx additional information xxxxx its every weekend now she is constantly breaking the rules but it doesn't matter her because she doesn't work anyway she's a xxxxx xxxxx and its really not fair now and she goes mixing with household with her sons it needs to stop but she won't listen and has been told by neighbours please cross refer into op talla master log log can be closed with thanks further email from the INFORMANT - 15 xxxxx hi that's fine thanks , please could you not mention any names as i don't want is causing any problems thanks \\ \bottomrule
\end{tabular}
\caption[ASB Incident Log Examples]{\label{tab:example_incident} Two example incident texts from the PF2 data. Note \say{xxxxx} are redacted words. }
\end{table}




\section{PF1 Data} The PF1 data comprises crimes committed in a police force in the North of England. Two years worth of crimes were provided. Though the years were not specified. Crimes of a sexual nature and or related to domestic abuse were also withheld. All of the data fields supplied with the data can be found in Table \ref{tab:data_fields_PF1}. The MO texts came from the \emph{Crime Notes} column and the flags came from the \emph{MO Description column}. The PF1 data went through processes unknown to redact identifiable information from the MO texts before it was given to the University of Leeds. Only the Burglary crimes from the PF1 data were used for this study (the reason for this is explained at the beginning of Study 1a). The burglary MO texts are described next.

\subsection{Burglary MO Data} The PF1 data contained 9818 burglaries. As mentioned previously the year of the crimes was not given but the day of the week and the month was given (Table \ref{tab:data_fields_PF1}). The median number of words in a MO text is 65, with the inter-quartile range being (48,88), see Table \ref{tab:corpus_stats} for a comparison with the PF2 data. The longest MO was 403 words long.


As the main PTM to be used was BERT, it is also worth exploring if BERT will recognise the words used in the text. BERT can only recognise certain words or tokens. If the words are not recognised then they are broken into word pieces that are then recognised, although they may not have the same meaning as the original word. 

Comparing the MO words with the BERT vocabulary shows that BERT recognises 96\% of the MO words (by volume). The remaining 4\% of words are broken into word pieces which the BERT model recognises. As BERT is trained on books and Wikipedia text, not police records, it is worth exploring which of the words in the MO text that BERT does not recognise. Table \ref{tab:non_bert_words}, shows the top ten unrecognised words. The table shows that there are unrecognised words, for example ‘insecure’, that may have an important bearing on describing the crime. Although they will be broken into word pieces and not removed, this disassembling of the word may be a source of error that prevents the BERT language models from classifying the texts correctly.



\begin{table}[]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\multicolumn{3}{c}{Crime Data Fields}                          \\ \midrule
URN & Crime Type       & OccType      \\
Day           & Month          & PartialPostCode                \\
MODescription & CrimeNotes*             & HOClass            \\
OffenceRec  &  DomViol          &           \\ \bottomrule
\end{tabular}
\caption[Table of data fields for the PF1 data]{\label{tab:data_fields_PF1} A table of all the data fields for the PF1 crime data.* Indicates a free text field.}
\end{table}


\section{PF2 Data} The second source of data was from PF2. PF2 is also a police force in the North of England. PF1 borders PF2. The main difference between the PF1 data and the PF2 data is that the PF2 data included crime data and police incident data. The transfer of data from PF2 to the University of Leeds was also more closely controlled by the author. The data extract specifications was built alongside the PF2 police analysts and the data was extracted as a joint effort. In addition the author built the de-identification process, described in detail in the next section, that was used to de-identify the free-text data. The PF2 data contained both structured and unstructured data fields. The initial data was transferred in January 2021, followed by a secondary data transfer, in February 2022 that allowed additional fields to be extracted for model verification purposes. 

All crime data fields are shown in Table \ref{tab:data_fields_crime}, incident data fields are shown in Table \ref{tab:data_fields_inc}. The review of the data in this section will focus on two data types. PF2 burglary MO data, which was used to replicate analysis of the PF1 data used in study 1 and secondly anti-social behaviour (ASB) police incident logs that was used to investigate the use of PTMs on police incident logs.

\input{data_fields_table}


\subsection{PF2 Burglary MO Data} The PF2 burglary data consisted of just over twelve thousand reported crimes. It includes all residential burglaries and attempted residential burglaries committed from 1st January 2018 to 31 December 2020. Each reported crime contained an MO text. The median number of words for an MO text is 31 (IQR 22,46). Comparing the PF2 burglary data to the PF1 data we find that it is generally shorter and more homogeneous, see Table \ref{tab:corpus_stats}, so possibly less descriptive. We would therefore expect models to be poorer as there is less variation in the data on which to discriminate. 

After the modelling was complete PF2 released additional data to help quantify the effectiveness of the classification model built to identify when a car was also stolen. PF2 provided the results from a data search that showed when a vehicle had been linked to a burglary, and the link of association was \say{stolen}. Typically they expect this field to be more complete than text references in the MO data to a stolen vehicle - so it can not be used as a direct metric as the language models can only analyse information stored in the free text data. That is given the selective nature of free-text data a car maybe stolen and logged as linked to the burglary but not mentioned in the free-text MO description. For a complete list of fields provided see Table \ref{tab:data_fields_crime}.

As with the PF1 data PF2 MOs were explored to see what percentage of words are contained within the BERT model vocabulary. By comparing the MO text with the BERT word list it can be seen that BERT recognised 96\% of the words (by volume), the remaining 4\% of words are broken into word pieces which the BERT model recognised. This is the same percentage as PF1.  Table \ref{tab:non_bert_words} shows the top ten unrecognised words. The top ten unrecognised does vary across police forces, although there is some overlap in meaning. 

%https://stats.stackexchange.com/questions/325549/how-to-measure-dispersion-in-word-frequency-data

\subsection{PF2 ASB Incident Logs}As described earlier incident logs are different to MO data in that they are generated primarily through reports made by members of the public. Incidents do not have to be crimes, and indeed the incidents that text data was received for were not classified as crimes. The incident logs had all been classified as anti-social behaviour. Incident logs are typically much longer than MO data, as can be seen from \ref{tab:corpus_stats} the median words in a document is over fives times greater than that of the burglary data standing at 166 for an ASB incident log. The inter-quartile range of word counts is also much larger at (100-290).  

Table \ref{tab:non_bert_words} shows the most common words from the ASB documents that are not recognised by the BERT model. Most of the words are abbreviations. Of note here is that \say{covid} is not recognised, this is because when the BERT model was trained (2018) covid was not quite as infamous as it is now. In total 11.1\% of words in the incident logs are not recognised in their complete form by the BERT model. This is higher than the MO data, but not unexpected as the ASB log uses more place names, abbreviations and telephone numbers. The incident data has a smaller subset of data fields than the MO data, the fields provided are listed in Table \ref{tab:data_fields_inc}.

\begin{table}[]
\centering
\begin{tabular}{p{0.3\linewidth}p{0.3\linewidth}p{0.3\linewidth}} 
\toprule
                         & Median Words per document & IQR  Words per document \\\midrule
PF1 Burglary MO    &          65                 &           (48,88)                                               \\
PF2 Burglary MO &          31                 &    (22,46)                                         \\
PF2 ASB Logs      &           166                &         (100,290)                              \\  \bottomrule
\end{tabular}
\caption[Text data descriptive statistics]{\label{tab:corpus_stats} This table contains descriptive statistics on the different text corpus used in the thesis. Median was used as the average as the distribution of words is skewed. IQR stands for inter quartile range and is the 25th and 75th percentiles. }
\end{table}

\begin{table}[]
\centering
\begin{tabular}{p{0.4\linewidth}p{0.5\linewidth}}
\toprule
Data Set& Top Ten Non-BERT Words                                       \\ \midrule
PF1 Burglary MO       &  egress, insecure, untidy, complainant, upvc, terraced, semi-detactched, occupant, jemmy, comp  \\
PF2 Burglary MO       & XXXXX, undetected, insecure, untidy, aggrieved, terraced, burglary, trespasser, UPVC, unoccupied \\
PF2 ASB Incident text &  XXXXX , inf , covid, npt , cctv , nuisance , informants , pls , pcso , fcm       \\ \bottomrule
\end{tabular}
\caption[Non-Bert words PF1 and PF2 data]{\label{tab:non_bert_words} This table contains words that are not in the BERT language model list but are in the police data used. Only the top ten missing words by volume are listed. The words that do not appear in that list will be broken down into word pieces and so meaning may be lost. XXXXX is the symbol for redacted words.}
\end{table}
\section{Data cleaning and de-identification} This sections sets out the steps for the de-indentification the PF2 data.  This was required to meet data protection requirements and took place before the data could be transferred from the PF2 servers to the secure University of Leeds servers.

Whitelisting was used as the method to de-identify the data. De-identification is the process of removing personally identifying information from the data. For structured data this is generally a trivial task, for example removing the second half of a postcode generalises the data sufficiently such that individuals can not be identified even in sparsely populated areas. For the police staff who input text there are essentially no limits on what can be included.  Full names, addresses, date of births can all be entered into a free text box without technical issue, even though procedurally they should not be entered. In some instances, for example the incident data, personal information is expected and a necessary part of the data being logged. However, MO data is designed from the outset to be released to third parties, though principally still within the criminal justice system, and so should not routinely contain personal identifying information.

Medical research has studied the issue of de-indentifying data extensively. Models built for this task range from simple rule based models to more intricate NLP based models \parencite{meystre2010automatic}. However there is no consensus that these models work perfectly in all situations \parencite{narayanan2014no}. Each de-identification model style has downsides, the machine learning models require a lot of labelled data to train, and tend to be difficult to explain. The rule based models require extensive knowledge of the data and are not robust against unseen phrases within the data. For this research the most important characteristics for the de-identification process where easily explainable rules (to be explained to and understood by police staff) and a risk adverse approach (to avoid any data protection issues arising). For this reason a whitelisting approach was chosen.

Whitelisting is a well known, conceptually straightforward and safe approach. Whitelist methods uses a list of safe words. If a word in the police free text data is  on the safe list then it is kept, if it is not on the list it is redacted. The resulting text is therefore only constructed from the words on the safe list. This is a simple de-identification method, which is easy to explain and deterministic, but has the downside of potentially redacting rare but important words. 

The next section will explain the whitelist procedure in more detail. Data cleaning was used alongside this process to homogenise the text and improve the retention rate. As a preliminary stage of data cleaning, spell-checking was conducted so that incorrectly spelt words were corrected and therefore matched a word on the whitelist were appropriate. 

\subsection{Data cleaning} In addition to screening the data for de-identification there was also data cleaning to homogenise the text so that information was not lost when certain words or tokens were removed. This included spelling correction, replacing jargon and replacing detailed information with a representative placeholder. The different aspects of this process are discussed below:

\begin{enumerate}
    \item{Misspellings.} Common misspelling were identified. These were added to a misspellings list. This misspelling list was then used to correct words in the MO text before it was de-identified. There were just over 900 common misspellings and typography's identified that were then corrected. Misspellings were identified by hand.
    
    \item{Jargon.} Although jargon would be identified through other processes later, some of the words or phrase were changed to represent words that were more likely to be recognised by the PTMs. An example was changing m/v to motor-vehicle. Again this list was generated through reading representative samples of texts.
    
    \item{Placeholders.} Some information was replaced with generic placeholders denoting the type of information while removing personal identifiers. An example was UK vehicle license plates that follow known formats which were replaced with the token ‘NUMBER\_PLATE’. 
\end{enumerate}


\subsection{De-identification Process Overview} The de-indentification process  used a white list approach. Words were removed from the text if they were not on the list of approved words, referred to here as the safe list. The safe list was built in an iterative manner.  It was seeded with  a list of  frequently used English words (detailed further below) that were compared to the text. Those words not on the safe list were arranged in frequency order (most common at the top). This list was then reviewed and words deemed safe were added to the safe list. Common misspellings were identified and added to a data cleaning list. The process is shown in Figure \ref{fig:whitelist}. The outputs of the process  were a list of safe words plus a list of transformations for common misspellings or abbreviations.



\begin{figure}[!ht]
  \centering
    \includegraphics[width=\textwidth]{images/Slide1.png}
    \caption[Whitelist Cycle.]{This depicts the cycle to generate the whitelist. The cycle also includes data cleaning to remove spelling and typographic errors. First the whitelist was seeded with an existing list of 5000 common English words. This list was then compared against the police free text and those words that were not on the safe word list were counted and presented in a frequency table. Words in the frequency table are reviewed and those that were not names are added to the safe word list.}
    \label{fig:whitelist}
\end{figure}

\subsection{Base word list} In order to seed the process of generating the safe word list a base list of common English words is required. There are numerous word lists that have been created, however they tend to have a flaw for this usage, and that is that they have been generated from data, for example Wikipedia, that contains names. What was required was a word list that was not just generated through simple frequency lists, and so was unlikely to have common names.

The Oxford 5000 \footnote{https://www.oup.com.cn/test/oxford-3000-and-5000-position-paper.pdf} is a list of what are thought to be the most important words to learn for those learning English, as it is not based on word frequency it did not contain common names, therefore this list  was used as the base for the list of safe words. As part of forming the base word list, the Oxford 5000 was compared against name lists, principally name lists from the Office for National statistics \footnote{https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages /livebirths/adhocs/008710babynames1996to2016} that contain all forenames and surnames used in England and Wales, to see if words that could be names were used in the list. Throughout this process, because of the variety of names that can be used, there needs to be a balance of risk. As an example in the ONS list of forenames the name \say{A} is given, clearly because \say{A} is such a popular word and a very rare name then most, if not all, usages of the word \say{A} in the MO text are not likely to be referring to an individual. 

Therefore all words from the Oxford 5000 base list were checked against the name lists and a judgement made as to whether the word should remain in the safe list or not. Although 220 of the words were also in the ONS names lists no words were removed from the base list as they were all deemed relatively obscure names.


\subsection{Developing the safe word list} The safe word list was further developed by comparing the safe word list with all unique words in the police free text see Figure \ref{fig:whitelist} with a minimum frequency greater than 10. 10 was chosen primarily due to time available to clean the data. If the word from the police text was not in the safe word list then it was manually reviewed by the author.  If the word was deemed sufficiently safe i.e was not likely to impart personal information then it was added to the safe word list.  This allowed more of the free-text through the de-identifcation process.

The unique words generated from the police text will contain normal English words, police jargon and misspellings. Additional words that were added to the safe word list were called \emph{police words} , as they had been generated directly from the police free text data. Examples of \emph{police words}  include \say{complainant}, \say{stated} and \say{suspects}. 5205 additional police words were added to the base word list.

This process of adding additional words to the safe word list was only completed with, and therefore tailored to, the MO data. There was insufficient time to tailor the process or the resulting word lists to the police incident data. When the police incident data was de-identified it was completed using the wordlists generated from the MO free text data. The effect of this is to remove more text from the incident logs than is necessary. As an example the \say{:} symbol was not used in the MO texts and so was not added to the safe word list along with other standard punctuation, whereas it is used in almost every incident log. Therefore every incident log now has the redacted symbol \say{XXXXX} instead of every \say{:}.

Once the safe word list was generated it was then used to de-identify the police text. This step is explained below.


\subsection{De-Identification Process} Once the safe word list had been produced the final data cleaning and de-identification process was completed in the following steps, see Table \ref{Example_deident} for an example output:

\begin{enumerate}
    \item Homogenise text. Tidy the text to remove unnecessary pluralisation's, change jargon and correct common spellings.
    \item Replace Information. Use pattern identification to replace known data types with their category so for example replace an actual number plate value with 'NUMBER\_PLATE'
    \item Whitelist the text. Check every word in the text with the safe list. The safe list is made of the original base word list and the police words. If the word is in the safe list it is allowed to remain. If the word is not on the whitelist then it is replaced with 'XXXXX'
\end{enumerate}

\begin{table}[]
\centering
\begin{tabular}{p{0.2\linewidth}|p{0.7\linewidth}}
\toprule
Example MO       & Suspect(s) unknown steal zebra pattern clothes and hit vctim. They leave in a car vl51pld towards big hill. Hitting Philip Schofield as they flee. \\ \midrule
De-Identified MO & Suspect unknown steal XXXXX pattern clothes and hit victim. They leave in a car NUMBER\_PLATE towards big hill. Hitting XXXXX XXXXX as they flee.    \\ \bottomrule
\end{tabular}
\caption[Example MO de-identification process]{\label{Example_deident} This table depicts a single example MO, not real, before and after the de-identification process.}
\end{table}


\subsection{Data Security} Despite best efforts, the possibility remains that a person’s name will slip through the net if it is made up of normal words e.g. May Summer. However, an additional procedural control was used to reduce such risk. This measure was the data infrastructure used to secure the data and procedural process. This data infrastructure heavily restricted data export and only allowed access to the data by named members of the research team. 

\subsection{De-identification Results} The de-identification process was not formally tested with the MO data. That is, the MO data  was not systematically searched for personal data to determine what percentage of personal data remained after whitelisting. What is known however is how much of the original text data was recovered. This metric is of interest because significant effort is made to develop more sophisticated techniques principally to reduce the result of false-positives i.e. removing non-personal data.  For example in Table \ref{Example_deident} it can be seen that the word zebra is removed because in the context of police MO text \say{zebra} is a rare word. However, arguably there is no need to remove this word and to do so can unnecessarily lose information on which to fine-tune the PTMs. 

For police MO texts the data recovery rate was 97\%, that is 97\% of the words, by volume,  used in the police MO texts were not redacted. The police incident log  text was lower at 92\%. The police incident data was expected to have a lower recovery rate for two reasons, firstly the word lists were not optimised on the police incident text and secondly the police incident text is expected to contain personal data and so more text is expected to be removed. The MO data retrieval rate was high, and anecdotally from the those researchers that read the texts, comprehension was not overly affected by the redaction of words. The police incident data redaction rate was higher, because the incident text was already noisy and unedited, the impact on comprehension is difficult to judge, but was certainly thought to have had a greater impact on comprehension than the loss of words for the MO data did.  

\section{Data limitations} The data used in this thesis was limited in a number of key ways. The limitations are generated throughout the data generating process right up to and including the choice of language model used. The key limitations are highlighted below:

\begin{enumerate}
    
    \item Police Data Coverage. Police data does not cover all crimes committed and the paucity of coverage is non-random. This non-random coverage is not new and is well documented \parencite{Tarling}. However it does mean that any patterns or insights drawn through using these techniques with police recorded crime will be subject to these same biases. This is a well known problem and is also a problem when using police structured information.
    
    \item Information completeness. The texts that are provided are not complete representations of the crimes or incidents that they describe. This incompleteness is in some ways deliberate, the police officers or staff only report positively not negatively e.g. they do not generally report on what doesn't happen. Secondly the completeness may be non-deliberate through bias, Officers can only report what they know and as it is widely reported that certain sections of the community to do not engage as fully with Police Officers as others \parencite{buil2021accuracy}. It is entirely plausible that police crime descriptions, and therefore the information that they contain are biased. A future area of study would be to analyse crime descriptions across victim and geographical characteristics to ascertain if they are systematically different in their percentage coverage of the key facts.
    
    \item De-identification. The de-identification process will have removed information from the police texts. this was an unavoidable step for this research in order to provide a reasonable level of data security. The information removed will also have been biased towards rarer words, as the de-identification process was biased to keeping more popular words. Although it is worth noting again that police staff using these models on their own data within their own systems would not have to complete this step.
    
    \item Model compatibility. PTMs have a list of words that they are trained to recognise. If a word is used that is not on that list then it is broken down into word pieces that can then be recognised. As PTMs were not built on police data there are certain words (see Table \ref{tab:non_bert_words}) that are not recognised by the language model but are frequently used by the police to convey information. As these words will be broken down into pieces it is plausible that meaning will be lost. The impact of breaking down specific informative words is unknown and is important issue for future research.
    
\end{enumerate}


All of the factors above will have contributed to limitations within the data. Some of those factors are inherent to police data and have been well studied for examples issues surrounding data coverage, however other issues such as biases in police textual data are not well studied and will require further study to understand the extent of the bias and errors that they may introduce.

\section{Conclusion} This chapter has introduced the data that is to be used in the resulting studies. All  of the data to some extent was changed in order to facilitate research access. As discussed, the changes are likely to have a relatively minor detrimental affect on the ability of the language models. The next chapter will explore the methods that were used across each study with the data presented here.






