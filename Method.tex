\chapter{Methods}

\section{Introduction}
This chapter  explains the methods used to extract information from the text data and understand how well the PTMs worked.  It will focus on three stages of  building and checking the language models. Firstly the approach for labelling the data, active learning, will be explained. Labelled data is required as supervised language models require example texts with the correct label to be presented  in order for the model to learn the patterns. Secondly, once the data is labelled the model is fine-tuned on the police data so that it can discriminate between texts and classify them appropriately, this will be the focus of the second part of this chapter. Finally the chapter will address methods to understand how well the models have performed, using the ALGO-CARE framework, introduced earlier, to guide the selection of metrics. Within each study chapter variations from methods explained in this chapter will be stated. 

\section{Data Labelling} In order to use supervised machine learning techniques a portion of the data has to be read and assigned the correct label so that the PTMs can be fine-tuned. Deciding which data to consider for labelling is an important process and a technique to assist with this,  called active learning, was introduced in the Chapter 3. Active learning relies on labelling small batches of data then using the PTM in question to find those unlabelled data that it is most uncertain about for the next batch of labelling. This section will explain the labelling and active learning processes used to label the data for the different studies explained in the forthcoming chapters.

\subsection{Labels}

Throughout this thesis language models are going to be used to classify texts, and so the labels that must be given to the data are generally labels that either include or exclude a text from a particular classification. As an example of labelling a burglary MO was read and it was labelled either as having a car stolen or not having a car stolen, thus the classification was \say{car stolen} and within that classification texts were either labelled \say{1} if they had a car stolen or \say{0} if they had not. If it was unclear if an event had happened then it was assumed that it hadn't happened.  The same MO text would also be labelled for other events such as the use of force. Each set of classification label e.g. car stolen labels, had to cover all eventualities that could be contained within the text. For the PF1 data only the author labelled the data. For the PF2 data two data labellers were employed, both labellers labelled the same data. Where there were disagreements between the labellers the author adjudicated.

\subsubsection{What To Label For?}

For both sets of data the following process was generally followed for establishing what subject to label for and how to assign labels. Firstly labels were suggested based on the hypothesis to be investigated or the problem at hand. Once the first suggestion of labels was made a random selection of texts were read. This first pass of the data was to ensure that the text covered the event of interest and secondly to form labels that would cover all eventualities for that event. Once this was completed a practice session was then held with all labellers to run through and discuss a random sample of texts. For the ASB practice sessions, the practice session also had a former Detective Inspector present to assist with the discussion and decisions.

\subsubsection{Labelling Implementation}

Once the practice sessions were completed the labellers would then label the texts in batches of one hundred texts for MO data and fifty for police incident data. Labelling was completed online and in isolation. Occasionally there was feedback to the labellers if the need for clarification arose as a result of unforeseen results within the text. For both batch types (MO texts of 100 and incident texts of 50) it would generally take around 1 hour to label a single batch. The data labelling was completed asynchronously, which meant that labelers were free to label at a time convenient for them - however this had the effect of elongating the labelling cycle as moving onto the next stage can only be completed when all labelling of each batch was complete. Labelling was conducted in Excel sheets with conditional formatting that only allowed pre-allocated responses to be selected.

The first few sets of texts were randomly chosen for labelling, after which an active learning strategy was then used to choose the most important texts to label from a modelling perspective. The next section explains in more detail how the active learning strategy was used to select the texts for labelling.


\subsection{Active Learning}Active learning was introduced earlier in Chapter 3 and is a technique to reduce the labelling burden required for training a model by seeking out examples that will help the model improve the most. As labelling texts is a resource intensive procedure, active learning was used to reduce that burden by more intelligently select texts to be labelled to help improve the model accuracy more quickly than by random selection. 


\subsubsection{General Process.}  

Figure \ref{fig:active_process} depicts the general process for the active learning strategy. Starting in the top left corner. As a reminder three data sets have to be built for the modelling process. These are:

\begin{enumerate}
    \item Test set. Picked randomly. Used to estimate the effectiveness of the trained PTM. This data set is never seen by the model during training and so the data is new to the model at test time.
    \item Validation set. Picked randomly. Used to help tune parameters for the PTM. This data is used during the training of the model to gauge progress but it not used directly by the model for fine tuning, but rather to prevent overfitting.
    \item Train set. Selected through active learning. Used to train the PTM. This is the actual data that the language model will be fine tuned on and directly influence the models classifications.
\end{enumerate}

\begin{figure}[!h]
  \centering
    \includegraphics[width=\linewidth]{images/Slide1.jpeg}
    \caption[Active Learning Process.]{Active Learning Process. Start by randomly selecting \emph{n} data and labelling. Steps 1 and 2 randomly label Test and Validations sets. Step 3 uses randomly labelled data to train a model and to classify all unlabelled data. The texts with the most uncertain scores are then labelled and added to the train set to further fine-tune the model. Data is iteratively added to the data set until the model has satisfactory performance.}
    \label{fig:active_process}
\end{figure}

The first to steps in the process are preparatory and they are to randomly select data to be labelled for the test and validation sets. The third step is the final random selection, and this random selection selects the first batch of texts for the train set. Once selected these samples are labelled and used to fine-tune a model. The trained model is then used to predict all MO texts that have yet to be labelled. Once completed the results of the model predictions are then used to discover which of the MO texts the model was most uncertain about. 

Quantifying model uncertainty was achieved by ordering of differences in output probabilities. PTMs output log-probabilities for each class. The absolute value of the difference in log-probabilities are then ordered and the MO text relating to the smallest values are selected. This process finds the texts that the model is most unsure about. These selected samples are labelled and then the train, predict, select cycle is repeated. This cycle selects the hardest to label texts on each occasion until the decision is made that the model no longer needs to be fine-tuned. Once this decision is made the active learning process stops. 


\subsubsection{Batch size}
The first decision for an active learning strategy is to decided the batch size, how many texts will be labelled in each sitting. Active learning can be completed with a batch size of 1 allowing for a selection after each text has been labelled. however as the labelling in this research was being completed asynchronously and generally by more than one person this would have led to a very slow labelling rate. For this study the active learning was conducted in batches of 100 texts. 100 texts were selected because it translated into a suitable length of time to devote to labelling data - around 1 hour. Much longer and concentration and accuracy may have been degraded, any shorter and the overall labelling rate will have been degraded. 


\section{Pre-trained Language Models} In all three studies in this work the modelling utilised PTMs that were introduced in the first part of this thesis. PTMs have been trained on large volumes of generic texts to give them a broad understanding of language. These language models are then further fine-tuned by exposure to police texts so that they are then able to classify the police texts as required. Each classification type requires a different fine-tuned PTM, so although all of the burglary texts were classified using a PTM, there was a separate model fine-tuned for each classification type or question. So for example there is a PTM fine-tuned for classifying if a motor vehicle was stolen and a separate PTM fine-tuned for if force was used. As previously mentioned PTMs can be useful in the context of the analysis of police free text data, because they can be utilised with little feature engineering effort as they already have a general language understanding, this is in contrast to other general machine learning models that do not have this understanding pre-built and would therefore require extensive feature engineering thereby increasing the time and technical burden on the police analytical staff. 


Throughout the studies completed here the modelling was classification modelling, that is take a single piece of text, for example a MO text and classify it as either belonging to a labelled group or not. For example the label could be car stolen and each text would either be classed as having a car stolen or not. The type of language modelling task that is required to be completed influences the selection of PTM. For classification tasks encoder models, of which BERT is the most widely used, are the most appropriate selection as they are able to encode the information from the text into a single output - the classification \parencite{PTMsurvey}. 

PTM were utilised through the Transformers package \parencite{wolf2019huggingface} in python. The Transformers software package allows modern transformer PTMs to be used within a simple interface. The package includes the language models as well as the surrounding architecture to utilise the models such as the tokenisers to prepare the text and interfaces to quantify the performance of the models.

Within this thesis two PTMs were used. Firstly BERT was picked as at the time of commencement of this study it represented the most advanced encoder style PTM of its class and was widely regarded as the most capable PTM \parencite{PTMsurvey}. One weakness of BERT however is that it cannot handle long texts, so for the police incident texts another PTM had to be utilised that specialised in computing longer texts. For this reason the Longformer PTM was selected for use with the police incident text. The next sections will introduce these models and explain how they were used.

\subsection{BERT} BERT was first introduced in 2018 \parencite{devlin2018bert} and immediately made an impact in the field of NLP by providing new state of the art scores in a set of benchmark NLP standards, known as the GLUE (General Language Understanding Evaluation) tests \parencite{wang2018glue}. 

This section will first describe BERT, how it is built and the inputs it uses and the outputs it generates. Since inception there have been different varieties of BERT \parencite{rogers2020primer}, essentially different parameter arrangements, although there are many similarities between the models for ease this chapter will focus on BERT-large, an original BERT model. BERT stands for Bidirectional Encoder Representations from Transformers. Transformers will be discussed briefly below, but the bidirectional element of the name refers to the fact that BERT can understand context from left to right and right to left, meaning that words can influence the meaning of those words before and after them, just as they do for humans.

This next section will describe BERT the model, how it is trained, the inputs required, the outputs produced and finally how it was utilised for this research.

\subsubsection{Model Description} BERT is a deep learning model that has been pre-trained to understand the English language (other languages are available). This PTM can then be further fine-tuned on specific tasks across a spread of varied natural language problems. Unlike other machine learning models that have been discussed this model has two stages for its use, pre-training and fine tuning. Both elements use the same model architecture but the first stage, the pre-training, is much more expensive and time consuming than the second which is why it is fixed. The original BERT model was pre-trained on 16 specialist computers for 4 days, with an approximate cost of \$7000 \parencite{devlin2018bert}. For this reason the pre-train phase of a PTM is not a trivial process. Once the pre-training has been completed the PTM is then fine-tuned on representative labelled data from the target NLP task - in this case police text data.

\subsubsection{Training} This section explains in more detail the training phases, pre-training and fine-tuning. It then concludes with a review of the inputs and outputs of the model.

\paragraph{Pre-training} The pre-training for BERT is conducted in two parts, recall the purpose of the pre-training is to train the model to \say{understand English}. To train BERT two representative language tasks were used so that the parameters in the model could be adjusted to correctly encode the information from the input text. Both parts are self-supervised, in that they don't require human labelled data, this means that huge amounts of data can be used without the need for costly human intervention to label parts of the data. The data used for both parts of the training was the BooksCorpus (800M words) and English Wikipedia (2,500M words). The two training tasks were:

\begin{enumerate}
    \item Masked Language Model. 15\% of the tokens from a sentence that is input are randomly masked, these masked tokens are then predicted from the remainder of the sentence. One of the strengths of this procedure is that the model can see both the words left and right of the original masked word, so it can predict the word from all of the context contained within the sentence. This is where the B from BERT comes - because the model can use two directions - bidirectional - to understand each word. 
    
    \item Next Sentence Prediction. Sentences were paired from the training data. Half of the time the sentences followed on from each other in the training data, for the other half the sentences were paired at random and so were not semantically paired.  The model had to predict, given the first sentence, whether the second sentence actually followed the first. This has the benefit of training the model to understand the relationships between sentences as groups of words.
    
\end{enumerate}

\paragraph{Fine-tuning} Fine-tuning is used to adapt BERT to different and specific NLP problems. This can encompass a wide variety of problems such as question-answering or Named Entity Recognition. For our purposes the fine tuning is for classification, and for each classification task a separate instance of BERT was tuned. In order to fine-tune a BERT model for classification an additional classification layer is added as the new final layer in the BERT model. The weights for this final layer are then adjusted as the model learns from the training data presented to the model. Data is presented to the model in batches, a total presentation of all the training data is known as an epoch. There can be multiple epochs in each training cycle. Too many epochs though and the risk is that the model over fits to the training data. A model that has over-fitted to the training data does not generalise well to unseen data. Validation data is used, as a way of detecting the over fitting, and therefore to gauge the number of epochs to use. After fine-tuning the model is ready to be used on unseen instances. The next section explains the inputs of the fine-tuning process and the resulting outputs.

\subsubsection{Inputs and Outputs} BERT does not directly take words as inputs it takes tokens after the texts have been through a tokeniser. A tokeniser takes a sentence as an input and breaks that sentence down into words that can then be converted to numerical embedding. Not all words are recognised by BERT, in fact BERT only has a vocabulary of 30522 words \parencite{nayak-etal-2020-domain}. If a word is not in the BERT vocabulary then the tokeniser will break the word down into recognisable tokens which can in fact be word pieces like \say{int} or \say{un} as well as words. This process helps to make BERT robust to previously unseen words. For instance untidy is not in the BERT vocabulary so it is broken into wordpieces \say{un} and \say{tidy}. This can be a problem because the summation of the word pieces does not always equate to the semantics of the original word and so meaning can be lost \parencite{nayak-etal-2020-domain}. Once the  tokenisation has occurred the words are converted into word embeddings that are vectors 768 numbers in length. As mentioned previously these word embeddings have been built to numerically encode the semantic meaning of each word. It is these embeddings that are then fed to the BERT PTM as the inputs.

Once BERT has been trained and fine tuned the area of interest is the output, as this generally contains the information of interest for the task at hand. For each NLP task there is a different model added to the PTM. For classification a classification model is added to the PTM. The model takes the final information from the BERT model (encoded into a single classification token) and uses a linear classifier model to change the output into probabilities for each potential classification. The final classification is selected by picking the classification with the largest probability. The output from the BERT PTM is a probability for each possible classification.

\subsubsection{Utilising BERT.}  The first step of the utilising the model is to tokenise the input text. This tokenisation takes the input text e.g. MO text and splits the text into tokens. Most of these tokens will be the words, but as mentioned above BERT only recognises 30,522 words and so some words will not be known. These unknown words are split into word pieces that are known. Unlike other NLP models there is no standardisation of the language through shortening words to their lemma, or removing stop (high frequency) words. One choice that is available is either to keep using cased words or transform all letters to lower case. As these documents are typically not edited the tokeniser was set to change all letters to lower case.

Before initiating the BERT model hyperparameters that govern the models behaviour have to be selected. Essentially hyperparameters exist because there is no proven way to optimise how a PTM learns given the data it is to be trained on. As mentioned earlier these hyperparameters include the size of the data batches as the data is fed to the model, the learning rate, how quickly the model changes adjusts to the data it has seen and the amount of times the model sees each piece of data (number of epochs). The batch size was set at 16 the lower of the two recommendations in the original paper \parencite{devlin2018bert}. For the learning rate we again choose the smaller recommended value (2e-5) the tuning of which is governed by the recommended Adam optimiser. To compensate for the lower learning rate we choose a larger value for the number of epochs than that suggested to ensure the models do not stop training short of a good solution. Initially the number of epochs was 8 but that is reduced on a per model basis as necessary with feedback from the validation data.

For each epoch the validation data was used to compute model metrics. This allowed a view of when the model had stopped showing general improvement and was then overfitting to the training data. Once the training was finished the validation set labels were computed by the model to produce classifications for each text in the validation set, typically 200 MO texts or 100 incident logs. Theses classifications were then used to compute model metrics. As discussed in part 1 the metric selected was the Mathews Correlation Coefficient (MCC) which is robust to imbalanced class problems. As an additional step for the active learning sequence the model that had just been trained would then also label all of the remaining unlabelled data so that the next batch for labelling could be selected.

Models can then be saved to a hard drive much like other files for later reuse if required. Typically only the model weights are saved not the entire model framework. 

Once the active learning had finished, and no more data labelling was to be completed, the model was ran ten times on the final training set. BERT models, as with all deep learning models, have random elements to the training process so the results can be slightly different on each training run. Therefore the final training was completed 10 times with the best model being selected by the resulting MCC metrics. 

\subsection{Longformer} BERT models are powerful, but they do not scale well for longer pieces of text, for that reason the BERT model is designed to only take up to 512 tokens as an input. Some researchers have previously circumvented this limit by splitting longer pieces of text into two documents, running the model for the two documents then combining the output, however as context from one part of the document may no longer affect the second this approach is seen as sub-standard \parencite{Longformer}. For this reason BERT models were not suitable for the longer police incident log text which as we have seen can be over three times longer than the MO text. 

The Longformer model \parencite{Longformer} was therefore chosen to classify the police incident text as this architecture is designed for longer pieces of text. The Longformer models use a very similar architecture to the BERT models, in that they are both based on the transformer architecture. The Longformer models, however, have modified the method for calculating Attention. Attention is the method for identifying contextual information across the whole text sequence. Calculating Attention in BERT is quadratic to sequence length, but in the Longformer architecture the model architects were able to modify the calculations so that it can now be calculated linearly with sequence length, though with some loss of specificity \parencite{Longformer}. Thus they are able to accept longer sequences of texts. The Longfomer models were used in the same way as the BERT models, with a separate tokeiniser provided by the transformers package for data preparation.

Even though the Longformer architecture is designed for longer pieces of text the models are still computationally expense to run. In order to minimise the computational expense the batch size was reduced to 8 to reduce the amount of texts that the model would consider at anyone time. The remainder of the models hyperparameters remained the same as the BERT model.

All of the PTMs used in this research have now been introduced and an explanation of how they were utilised given. The next section reviews how the models performance was judged after they had been trained on the training data set with hyper parameters selected with the validation data.


\section{Model Performance} The previous sections have explained how the data was labelled and how the NLP models were trained. This section will now explain how, with a trained model, the performance of that model was explored. Typically performance, especially in computer science, is heavily predicated on how correct the model was. In essence consideration is only given to accuracy and similar metrics such as MCC. Here though we recognise that for a model to be used in service with the police, and most likely all public service settings, the model needs to be more than just accurate. Using the ALGO-CARE mnemonic introduced earlier, we see that accuracy is only one factor in a list of eight factors that are described for using algorthims in a police context. As before we highlight  the two additional factors described in ALGO-CARE that pertain to the implementation of these models. Firstly the framework asks \say{Is appropriate information available about the decision-making rule(s) and the impact that each factor has on the final score or outcome?} in relation to how explainable the results are and secondly  \say{What are the post-implementation oversight and audit mechanisms e.g. to identify any bias?} as the results should be challenge-able. With these factors in mind the performance of the models will be further explored through explainability and bias. These investiagtions are discussed in more detail below. 

Throughout the research model performance will be judged on a randomly selected test set. Test sets were randomly selected from the data before any data was removed for training. None of the test set will have been used for either the training or the validation of the model fine-tuning. This means that during performance assessments the test set is new to the model. Therefore, metrics from the test set  provide a reasonable assessment of how the model will perform on unseen texts. 

\begin{equation}
MCC =  \frac{(TP*TN – FP*FN)}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}
\label{eqn:MCC}
\end{equation}

Where: TP = True Positive, TN = True Negative, FP = False Positive and FN = False Negative.


\subsection{Metrics} As mentioned in the first part of this thesis, there are a multitude of different model metrics that can be used to judge a models performance. Selection of the model metrics should be based on the type of problem and dataset used. In this instance the problem was one of classification with an imbalanced data set i.e. one of the potential classifications was much rarer than the other. As outlined earlier Mathews Correlation Coefficient (MCC) is a good metric for this type of problem as it gives a standard score between 0 and 1 independent of the number of classification categories i.e binary or across more than 2 possibilities. Secondly unlike more basic metrics such as Accuracy the metric is able to account for imbalanced classes where rare instances may be difficult to predict. MCC was calculated using the scikit-learn package in python \parencite{scikit-learn}, the equation for MCC is given in Equation \ref{eqn:MCC}. This metric is the primary metric for understanding how well the PTM got the classifications right overall. However,  also of interest is how did the PTM came about its classifications (explainability) and how well did the PTM do across groups of instances within the dataset (bias). 


\subsection{Explainability}As explained earlier in Chapter 4 how a model came about its predictions is just as important as if it got the predictions correct, as being able to explain how a model is making decisions builds trust in the model. To be explainable the model must be able to explain or show why certain classifications were given. As mentioned in Chapter 4 these can either be global explanations, where the model can be explained for every possible input, or local explanations where the explanation is centred on the individual instances to be classified. For deep learning and in particular NLP it is very difficult to produce global explanations because of the vast array of possible inputs, for this reason we focus on local explanations using the LIME package introduced earlier. 

\subsubsection{LIME} LIME (Local Interpretable Model-agnostic Explanations) \parencite{ribeiro2016should} is an algorithm that is used to explain why a model has made a certain prediction. In essence the model takes a real individual instance to be predicted then modifies that instance slightly, in the case of texts it removes one or more words. The model of interest is then re-ran on the modified instance and the new output noted. Recall that in classifications models the output is a set of probabilities for all classifications and not just a single classification. So even if the final classification has not changed, it is likely that the underlying probability of that classification will have changed. Modifications (of the same instance but modified in a different way) are repeatedly selected on a number of occasions so that a local representation of a number of similar but distinct instances can be built. With these modified instances and their resulting probabilities a simpler local model, such as a linear regression, can be built that is then more easily interpretable. The coefficients of the resulting regression can be used to understand the effects of the modifications and therefore of the feature modified on the final probability. Thus at a local level the prediction can be explained by how much a particular feature (or word) is responsible for changing the probability. See Figure \ref{fig:LIME} for a simplified pictorial example, where the bold red cross is a whole MO text, and the smaller red crosses would be the text with some of the words removed. The black dashed line is then the linear model from which the coefficients of the removed words can be deduced and their impacts understood.
\begin{figure}[!tbp]
  \centering
    \includegraphics[width=\textwidth]{images/Slide2.png}
    \caption[Toy LIME example.]{ Toy LIME example. The bold red cross is the original unmodified instance ( original text) , smaller red crosses are the modified instances (texts with a word removed). The resulting black line is from the regression and is the learned explanation that is locally faithful i.e built on a single text. The true complex decision boundary is represented by the pink/blue background and is true globally, although generally unknown. Reproduced from \parencite{ribeiro2016should} }
    \label{fig:LIME}
\end{figure}


\subsubsection{LIME Implementation.} In order to get a view of explainability the LIME model was ran on all of the test set after the final classification model for each problem. For each MO or incident text random perturbations were conducted 100 times ( selected based on trials, there was little variation in output at 100). This 100 iterations produced a single linear model for each MO text. Once complete the coefficients from each of the resulting linear models were pooled across the entire test set so that a broader view could be taken on the words that were most important for the classifications. This data is then presented in a word cloud, where the size of the word is related to the how important that word is for the final classification in the whole of the test set. The larger the word in the visualisation the more important it was for classification of the police text in that problem. If the words make sense to a human for the classification, then it is likely that the model is using the words to form a judgement in a similar manner to how a human would use them. However, if the larger words don't seem sensible for a classification then it maybe that the model has picked up on a spurious correlation in the training set. These visualisations then allow a judgement to be formed on how the model is working - if this is is inline with human expectations then the model can be considered more trustworthy than had it not been. 

\subsection{Bias}In Chapter 4, three broad areas were identified as sources of bias. i)Data coverage relating to the inconsistencies of reporting crime to the police. (ii) Data completeness - where the police may or may not systematically record different levels of detail about certain crimes or from certain sections of the community. (iii) Finally algorithmic bias was introduced, which given the data, was the algorithm making more or less errors in certain parts of the data. 

The first two are difficult to quantify in the study because the only data available is the police data. We do not have access to the totality of crimes conducted, nor do we have access to perfect descriptions of the crime to understand if there are important elements systematically missing from the police descriptions. The final type of bias, algorithmic bias is within the gift of this research to identify and is an important element for consideration. 

Algorithmic bias typically occurs in PTMs because of the data that was used in the pre-training phase and how that relates to the data being analysed. For instance if certain police texts are very dissimilar to the pre-training data then they may not be classified well, additionally if there is bias within the pre-training data, that may be carried through to classifications biases of police text data. For the purposes of this research we split this bias into two categories those relating to the text and those relating to the crime or incident being described. 

Firstly there are qualities of the texts themselves that can be described through statistical variables - statistics that are produced from the texts that the PTM is used with. Examples are the length of the text and the amount of out-of-vocabulary words. Secondly there are characteristic variables of the crime being described that the model may or may not be able to deduce from the text that is used to train the model. For instance the location of the crime or the gender of the victim. Of course these two types of variables may not be independent of each other, for instance it is possible, though not evidenced, that certain victims groups may have shorter crime descriptions because of the relationship they have with the police. These two variable types are used to explore potential biases with using PTMs.

In the literature bias is often measured through metrics such as extrinsic bias \parencite{goldfarb2020intrinsic}. Extrinsic bias will be used to explore characteristic variables, these metrics are introduced and explored below.

\subsubsection{Extrinsic Bias} \say{Extrinsic bias metrics measure bias in applications, via some variant of performance disparity, or performance gap between groups.} \parencite{goldfarb2020intrinsic}. As an example of potential extrinsic bias from this research, if a burglary classifier had higher error rates for male victims than female victims then this would be an example of an extrinsic bias. \textcite{goldfarb2020intrinsic} identifies the two most popular metrics for investigating extrinsic bias, these are listed and explained below.

But before getting to those definitions we need to remind ourselves of two more basic definitions, \emph{Recall} and \emph{Precision}. \emph{Recall} can take a value between 0 and 1 and represents the percentage of positive instances that have been returned by the model. \emph{Precision} can also take a value between 0 and 1 but in this instance it represents the percentage that were actually positive from those identified as positive by the model. Now the two extrinsic bias metrics are explored.
\begin{equation}
P(\hat{Y}=1|A=x,Y =1)=P(\hat{Y} =1|A=y,Y =1)
\label{eqn:EOOprob}
\end{equation}  
\begin{equation}
\emph{Recall}_x − \emph{Recall}_y
\label{eqn:EOO}
\end{equation}   


\begin{equation}
P(\hat{Y}=1|A=x,Y =0)=P(\hat{Y} =1|A=y,Y =0)
\label{eqn:PPprob}
\end{equation}  
\begin{equation}
\emph{Precision}_x − \emph{Precision}_y
\label{eqn:PP}
\end{equation} 



\begin{itemize}

   

    \item \textbf{Equality of opportunity.} Equality of opportunity occurs when Equation \ref{eqn:EOOprob} is satisfied \parencite{goldfarb2020intrinsic}. That is where the probability of being classified as positive $(\hat{Y} = 1)$, given that the sample is positive (Y = 1), is the same regardless of what group the sample is drawn from ( A = x or A = y). Equation \ref{eqn:EOOprob} is based upon recall and therefore Equality of Opportunity can be measured through Equation \ref{eqn:EOO}. Where $\emph{Recall}_x$ represents the recall from the reference group ( sometimes considered the privileged group) and $\emph{Recall}_y$ represents the group of interest (sometimes referred to as the underprivileged group.\parencite{hardt2016equality} 
    

\item \textbf{Predictive parity} \parencite{verma2018fairness}. Predictive parity is similar to equality of opportunity above, but relates to the probability of incorrect predictions as seen in Equation \ref{eqn:PPprob}. In this case parity occurs when the precision from each group is the same, that is the probability of being identified, given that it wasn't a positive instance is the same regardless of the group the sample is drawn from. Again a simplified form to calculate the metric is given at Equation \ref{eqn:PP}

These extrinsic bias metrics were calculated for each test set. However the weakness with this approach is that only a single data point is obtained on which to judge bias. In order to provide more evidence, rather than just a single metric, a cross-validation process was implemented to provide a bias estimate with confidence interval. This cross-validation process is explained next. 

For the cross validation process 20\% of the available labelled data was randomly selected (available includes all data labelled for the test, validation and train data sets). This 20\% was used as the test set. The remaining 80\% was used as the train set. There was no validation set as the hyper parameters were fixed. A PTM was fine-tuned using the 80\% train set then used to label the 20\% test set. Bias metrics EoO and PP were then calculated on the  20\% test set. This whole process was repeated 10 times so that there were 10 sets of bias metrics. 

For each bias metric a non-parametric hypothesis test of equal means was conducted for each metric, testing if the mean of the metric was 0 or not using all ten data points. A significant p value would indicate bias at a statistically significant level across the experiment. The mean of the ten metrics indicates the direction and the size of the bias. 


\end{itemize}


\section{Summary} In this chapter the main elements of the method have been set out. These methods will be used in each study and form the basis for the analytical approach. In summary the main steps are:

\begin{enumerate}
    \item Label the data. The data will be labelled through an active learning strategy. The labelled data will then be used to fine-tune and test the language model.
    
    \item Fine-tune a PTM. The data labelled will be used to fine-tune a PTM, either BERT or Longfomer. This approach has proven to be quicker than building a NLP model from first principles which entails feature engineering.
    
    \item Test. The language model will be tested for performance (using MCC) , explainability (using LIME)  and bias (using extrinsic bias metrics) to investigate whether the performance of the models is sufficient for utilisation in a police and POP context. 
\end{enumerate}


The next chapters will now introduce each study in turn. Within each study will be a problem introduction, a review of the methods, the results then a discussion of what the results mean. There will be four chapters covering the studies. These chapters will be broken down as follows:

\begin{enumerate}
\item Study 1a - Burglary MO data (data - PF1) 
\item Study 1b - Active learning
\item Study 1c - Replication study - Burglary MO data (data - PF2)
\item Study 2 - Police Incident texts (data - PF2)
\end{enumerate}

After the studies the final part will be a broader discussion of the results from the PTMs and how or if they might be implemented to assist with POP.

