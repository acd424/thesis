\chapter{Study 1b - Active Learning}


\section{Introduction} As demonstrated in the first study a large proportion of the user time when fine-tuning pre-trained language models (PTMs) is taken up by creating labelled data from which the PTM can learn. Active learning was introduced earlier in Chapter 3 and is a technique to reduce the labelling burden required for training a model. Active learning seeks to reduce that labelling burden by highlighting examples that will help the model improve the most. Active learning finds the most difficult to classify texts by using the PTM, that has recently been fine-tuned, to classify all unlabelled data then finding the examples where the individual classification probabilities are closest, i.e where the model is not certain about a classification.  Active learning is explored in this study to understand if the expected benefits materialise with police data and to what extent it slows the modelling because of the additional processes required.

\section{Hypothesis}

The hypothesis for this study is that active learning will reduce the amount of labelling that is required against the standard approach of random sampling. This will be judged through observing the MCC coefficient for each fine-tuning batch. If the active learning is better then we would expect to reach a higher MCC coefficient sooner than if we used the randomly selected data.

\begin{figure}[!tbp]
  \centering
    \includegraphics[width=\textwidth]{images/Slide1.jpeg}
    \caption{Active Learning Process. Step 1 and 2 randomly label Test and Validations sets. Steps greater than two use initially randomly labelled data to iteratively train a model and then select labels based on model predictions.}
    \label{fig:active_process}
\end{figure}


\section{Active Learning Process.} Figure \ref{fig:active_process} depicts the general process for the active learning strategy. Starting in the top left corner. The first two steps in the process are preparatory and they are to randomly select data to be labelled for the test and validation sets. The third step is the final random selection. This third random selection selects the first 100 samples for the training set. Once selected these samples are hand labelled and used to fine-tune a model. The tuned model is then used to predict all MO texts that have yet to be labelled. Once completed the results of the model predictions are then used to discover which of the MO texts the model was most uncertain about. The most uncertain texts are then labeled and added to the training set

In practice the BERT model outputs log-probabilities for each potential classification - positive or negative. The absolute value of the difference in these log-probabilities are then ordered and the MO text relating to the 100 smallest values are selected. These 100 samples are then hand labelled to enable further fine-tuning and so the train, predict, select cycle is repeated until the decision is made that the model no longer needs to be fine-tuned. Once this occurs the active learning process stops. 


 For this study the active learning was conducted in batches of 100. 100 was selected because it translated into a suitable length of time to devote to labelling data - around 1 hour. Much longer and concentration and accuracy of the labeller may have been degraded. Selecting smaller batches may have allowed a quicker convergence of model predictions, as the model gets to adjudicate more often on those texts to label, this is off set however by the process overheads of conducting more model fine-tunes.
 
 
\section{Method}

MCC scores will be compared across models fine-tuned on data from the active learning process to models fine-tuned on pseudo-randomly selected data. If higher MCC scores are reached more quickly with the active learning method then the active learning will have been beneficial. The difference in the number of batches required to reach an equivalent MCC score will give an indication of the value, or not, of using active learning. 

The ideal method would be to compare the MCC scores of the active learning strategy with MCC scores from models using a random data selection approach. However during the labelling of the MO texts a random sampling approach was not taken, and so instead we compare the active learning approach to a pseudo-random sampling approach. Random sampling was not conducted for data labelling as there was not sufficient resources to use the active learning technique and the random approach.

The pseudo-random sampling is generated by using the labelled text from an active learning approach that was not for the model of interest. So in this case the pseudo-random comparison for the motor vehicle active learning approach was the data generated for the force model and \emph{vice versa} for the force model. Two issues with this approach and how they were investigated are explained below.

\subsection{Potential pseudo-random problems}
The first potential problem with the pseudo-random approach is that there may be inherent qualities in the MO text that make it difficult to classify, and so the pseudo-random approach may be picking difficult to classify texts regardless of the outcome as it is based on \say{An} active learning strategy if not \say{The} correct active learning strategy. A random selection of data would not select on general difficulty, as it is truly random. The effect of this in the results would be to reduce the perceived effect of the active learning strategy. If this problem was real within the data then we would expect significant overlap between those MOs selected by both the force and the motor vehicle active learning strategies. 

Additionally, and more importantly, there may be a correlation between the probability of picking a force MO and a motor vehicle MO through AL processes. If this was the case then we would expect the pseudo-random generation to be correlated with the model of interest, i.e there would be a higher proportion of motor vehicle labelled data in the force model than we would expect if the selection was truly random. This can be tested for by comparing the proportion of the active learning subject (i.e motor-vehicle) in the pseudo-random data (active learning data from force model), if the active learning subject is close to the expected proportion(i.e the underlying random base rate) then this is evidence against any correlation. This can be checked for by plotting the proportions of each classification in the data selected.  

\subsection{Pseudo-randomness Checks}

The first check is to see if there is a large overlap between the MO texts actively selected for both models. To do this a count of MOs selected for both models is undertaken. To be counted an MO must have been selected through the active learning strategy's for both models and have been selected in the first n selections, where n is the minimum of the two active learning pool sizes. There were 22 MOs selected for both models from a pool size of 600. This means that there was an overlap of 3.6\% between the two active learning selections. From this value we concluded that there was not a large overlap between the active learning selections and that inherent difficulty qualities of the text will therefore not be a significant factor in the results.  

For the second test which investigates potential correlation between the two model types we review plots showing the proportion of each type of classification relative to the selection approach. These plots are at Figures \ref{fig:active_car} and \ref{fig:active_force}. The individual plots will be explored below, but of interest for this test is panel (b) for each plot. 

Each plot has three lines. The red line is the expected random proportion, this is the proportion calculated from the test and validation sets, 400 samples altogether, given they are randomly sampled we expect this to be a good estimate of the true population proportion. For motor vehicle this was 9\%. The grey line is the cumulative proportion, this was calculated at each stage for each sample and is the total of positive samples divided by the total number of samples at that point. The black line is the batch proportion and is the proportion of positively labelled samples for that batch of 100 samples only. 

Panel (b) plots the proportion of data with a positive classification for one model by the active learning selection of a different model, so panel (b) in Figure \ref{fig:active_car} plots the proportion positively classified as including theft of a motor vehicle but with the force model active selection process. If the lines in panel (b) are close then the pseudo-random selection is selecting the data as a random process would. The individual plots are now explored in detail to see if the pseudo random data selection sufficiently represents a random selection. 


\subsubsection{Motor vehicle plot}We interrogate the lower plot of Figure \ref{fig:active_car} to see if the lines batch and cumulative proportions are close to the random proportion. Indeed we find that all lines are very close. This gives confidence that the data generated for the force model can be though of as random with respect to the motor vehicle model. 

\begin{figure}[!ht]
  \centering
    \includegraphics[width=\textwidth]{images/car_plots_active.png}
    \caption{{Active labelling - Motor vehicle model.} Plots showing the proportion of positively labelled MO notes for a burglary where there was also the theft of a motor vehicle. The labelled index (x-axis) indicates the order in which the data was selected and labelled. Panel a is for active learning based on motor vehicle model probabilities. Panel b is for selection using probabilities based upon the force models.}
    \label{fig:active_car}
\end{figure}


\subsubsection{Force plot} Using the same process as above we interrogate the lower plot of figure \ref{fig:active_force}. The lower plot of Figure \ref{fig:active_force} is the data selected by the motor vehicle model, but tested for the proportion of Force used cases. As with the motor vehicle plot we observe that the cumulative and batch proportions are close to the random proportion line, and so again we can be reasonably confident that using the data in this way represents a pseudo-random selection.

The result from these two plots and the investigation of the selection overlap, gives confidence that the pseudo-random approach is sufficiently random to test the hypothesis that active learning would give an improvement over truly random data selection. The next step is to compare the MCC scores from each batch to see if there was a benefit from utilising the active learning approach.



\begin{figure}[!ht]
  \centering
    \includegraphics[width=\textwidth]{images/force_plots_active.png}
    \caption{{Active labelling - Force model.} Plots showing the proportion of positively labelled MO notes for a burglary where Force was reported as being used. The labelled index (x-axis) indicates the order in which the data was selected and labelled. Panel (a) is for active learning based on Force model probabilities. Panel (b) is for selection using probabilities based upon the motor vehicle models.}
    \label{fig:active_force}
\end{figure}


\section{Results} The results will be explained for each model separately, these results will then be drawn together in the discussion. The results will compare the MCC scores for the active learning and pseudo-random data as outlined above. MCC scores were generated using the validation data set after each fine tuning. Recall that labelling was stopped when the MCC of the validation set reached 0.9, with 1 being the perfect score. Active learning for the force model was stopped at seven batches, which was earlier than the nine batches required for the motor vehicle model.

\subsection{Motor vehicle model.} Table \ref{tab:active_results_car} gives the MCC and Recall metrics for each of the nine batches of active learning data. As expected the MCC generally rises as more data is labelled and reaches its zenith of 0.91 once 900 MO texts had been labelled. The final column of data is the score using all of the pseudo-randomly selected data. This MCC score was from data generated from the force model after seven iterations. For a fair comparison this MCC score should be compared to the seventh batch of the active learning generated data. The active learning therefore has a MCC of 0.88, which is higher than the pseudo random value of 0.80. Comparing the pseudo-random score to all of the active learning values we see that it falls between the fifth and the sixth sets. Indicating that there is a model performance benefit equivalent to labelling an additional 100 MO texts.

\subsection{Force model.} In a similar fashion we observe the MCC values in Table \ref{tab:active_results_force} for the force model. In this table the final column represents the MCC score from the seventh batch of data from the motor vehicle generated data. The final MCC for the active learning model is 0.92, where as the comparable MCC score from the pseudo-random generated data is 0.89. Again we observe that the active learning method has produced a higher MCC score than the pseudo-random approach for a comparable number of labelled data.  Once again we compare the pseudo-random score to all of the active learning MCC scores and see that it falls between the fifth and the sixth set indicating a model performance benefit of labelling an additional 100 MO texts.





\begin{table}[]
\centering
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
Batch  & 1 & 2    & 3 & 4    & 5    & 6    & 7    & \multicolumn{1}{l}{8} & \multicolumn{1}{l}{9} & \multicolumn{1}{l}{Force (7)} \\ \midrule
MCC    & 0 & 0.38 & 0 & 0.66 & 0.78 & 0.86 & 0.88 & 0.85                  & 0.91                  & 0.80                             \\ \midrule
\end{tabular}
\caption{\label{tab:active_results_car} MCC metrics for the motor vehicle model with the data selected through active learning. Each entry is the MCC metric after that batch. The final column refers to data that was selected using the alternative model (force model), the number seven in brackets refers to that data being the seventh batch and for most similar comparisons should be compared to the seventh active learning batch}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{@{}llllllllc@{}}
\toprule
Batch    & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{Motor (7)} \\ \midrule
MCC    & 0.52                  & 0.70                  & 0.82                  & 0.55                  & 0.83                  & 0.91                  & 0.92   & 0.89                \\ \bottomrule
\end{tabular}
\caption{\label{tab:active_results_force} MCC metrics for the force model with the data selected through active learning. The final column refers to data that was selected using the alternative model (motor vehicle), the number seven in brackets refers to that data being the seventh batch and for most similar comparisons should be compared to the seventh  active learning batch}
\end{table}


\section{Discussion} Active learning has proved successful with Police MO data in this study. For both the force and the motor vehicle model the data selected through the active learning approach produced better MCC scores than data generated pseudo-randomly. In essence the benefit of active learning seems to equate to around 100 additionally labelled examples or a 14\% reduction in the required labelling.

Clearly there has been some benefit with utilising the active learning approach, however the utilisation of the approach is not done without cost. The additional cost is primarily a cost in time which can be split in two. Firstly is the additional model training that is required. The additional training burden is required because the model must be trained and allowed to label each of the texts. The exact time varies with the time taken for the model to fine-tune and predict, though as the model is a deep learning model this is a  non-negligible amount of time. For example at the end of the active learning the BERT model would take around 4 hours to train and a further 1 hour to label the remaining data, contributing significantly to elapsed time. 

Secondly active learning adds complexity to the process, and when multiple labellers are being used to label it can slow the process down as labelling must be coordinated in batches. This additional coordination time can also be relatively time consuming. In practice for later studies in this research where multiple labellers were used, labelling of a single batch of  texts was completed on a 48 hourly rhythm to assist with coordination of labellers effort. Thus what could have been achieved in a matter of days, in reality took a couple of weeks. Although the time actual spent labelling was theoretically reduced. 

Additionally it is possible that the size of the batch (100) was too large. Perhaps labelling at a much reduced rate, say batches of 10 to 50, may have seen a greater reduction in overall labelling. This is because the model gets the opportunity to pick out the texts that it finds difficult to classify more often. Again though this reduction in batch size is likely to introduce proportionally more time lost to coordination and model building.

Thus the decision to use active labelling is not entirely clear from this evidence. The decision must be a balance between the ease of adopting a more complex system and the time (both elapsed and user) available for the given task. If user time is short, then active learning has the potential to save between one and two hours a project, it is also likely to positively identify more of the rare class for labelling giving the labeller greater exposure to MO texts of interest. However if the results of the model are required quickly (i.e the elapsed time is of interest) and the user is able to devote more time to the task then active learning may actually delay achieving the final result.


\section{Conclusion} Active learning is a technique for reducing the amount of data that needs to be labelled for supervised learning. The technique was used with the SaferLeeds data for both the force and the motor vehicle models. In both instances the active learning strategy produced better results when compared to a pseudo random data selection strategy. However the gains were only a 14\% reduction in data labelling. When this is offset against the additional coordination costs of the strategy the practical resource saving is likely not that significant. 
 
