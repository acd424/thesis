\chapter{Study 1b: PF1 Active Learning}


\section{Introduction} The first study demonstrated that a large proportion of the user time that fine-tuning PTMs requires is taken up by the creation of labelled data from which the PTM can learn. Active learning was introduced in Chapter 4. It is a technique for reducing the labelling burden of training a model. Active learning is intended to reduce that labelling burden by highlighting the examples that are most helpful for improving the model. Active learning finds the texts that are most difficult to classify by using the PTM that has recently been fine-tuned to classify all unlabelled data. Once all of the data have been labelled, the instances with the closest individual classification probabilities are selected for labelling. This study explores active learning in order to determine whether its expected benefits materialise when it is employed with police data as well as the extent to which the additional processes retard the modelling process.

\subsection{Problem Overview}

This chapter concerns the fourth supporting objective - \emph{Evaluate how effective active learning is with police data}.  Effectiveness is judged by observing the MCC coefficient. If active learning is a better to an alternative (i.e. random) labelling strategy, its MCC score for an equivalent number of labelled data would be higher. This difference in MCC is also considered in view of the additional process that is required to enact the active learning strategy.

\section{Active Learning Process.} Figure \ref{fig:active_process} depicts the general process of the active learning strategy. The first two steps of the process (the top left corner of the figure) are preparatory. Data are randomly selected and labelled for the test and validation sets. The third step is the final random selection. This third random selection yields 100 samples for the training set. Once selected, these samples are labelled by hand and used to fine-tune a model. The fine-tuned model is then used to predict the classification of all MO texts that are yet to be labelled. Once complete, the results of the model predictions are used to discover which of the MO texts the model was most uncertain about. Those texts are then labelled and added to the training set.

In practice, the output of the BERT model comprises log-probabilities for each potential classification, be it positive or negative. The absolute values of the differences in these log-probabilities are then ordered, and the MO texts that are associated with the 100 smallest values are selected. These 100 texts are labelled by hand to enable further finetuning. The train-predict-select cycle is repeated until it is decided that no further finetuning is necessary. At that point, the active learning process ceases.

\begin{figure}[!tbp]
  \centering
    \includegraphics[width=\textwidth]{images/Slide1.jpeg}
    \caption[Active Learning Process.]{Active learning process, Step 1 and Step 2 â€“ random labelling for test and validation sets. Subsequent steps entail using data that are initially labelled at random to train a model iteratively and selecting labels on the basis of model predictions.}
    \label{fig:active_process}
\end{figure}

 The active learning process in this study was conducted in batches of 100. The number of 100 was selected because it results in an appropriate labelling time of around 1 hour. A longer process could have caused the concentration and the accuracy of the labeller to deteriorate. Selecting smaller batches may accelerated the convergence of the model predictions because the model would have adjudicated more often on the texts that were to be labelled. The benefits of convergence, however, would have been offset by the additional processual overheads for each cycle, that is, the time that would have been necessary to find new data to label, to fine-tune the PTM, and to label all of the unlabelled data in accordance with the latest model.
\section{Data and Method}


\subsection{Data}

The data that are used in this chapter are the same as in Study 1a. They cover the burglary MOs from PF1. The classification problems are the use-of-force and motor-vehicle tasks from the preceding chapter.

\subsection{Method}

MCC scores are compared across models that are finetuned on data from the active learning process and models that are finetuned on pseudorandomly selected data. If higher MCC scores are obtained more rapidly with the active learning method, then active learning is assumed to have been beneficial. The difference in the number of batches that is required to reach an equivalent MCC score gives an indication of the utility of active learning.

The ideal method would be to compare the MCC scores from the active learning strategy with the MCC scores from the models that are based on a random approach to data selection. However, the random sampling approach was not adopted during the labelling of the MO texts. Instead, the active learning approach was compared to a pseudorandom sampling approach. Random sampling was not conducted in the course of data labelling because the available resources were insufficient to employ both the active learning technique and the random approach.

The pseudorandom sampling was generated by using the labelled text from an active learning approach that had not been applied to the model of interest. In this case, the data that were generated through active learning for the purposes of the use-of-force model served as a pseudorandom comparator for the motor-vehicle active learning approach. The following paragraphs focus on two issues that affect this approach and the manner in which they were investigated. 

\subsection{Potential pseudo-random problems}
The first potential problem with the pseudorandom approach is that properties may inhere in the MO text that make it difficult to classify. Accordingly, the pseudorandom approach may result in the selection of difficult-to-classify texts, regardless of the outcome, because it is based on an active learning strategy rather than on the correct active learning strategy. A random selection of data would not be of average difficulty because it would be truly random. Consequently, the perceived effect of the active learning strategy would be reduced. If this problem genuinely affects the data, there would be a significant overlap between the MOs that are selected by the use-of-force and the motor-vehicle active learning strategies.

In addition, and more importantly, there may be a correlation between the probability of selecting a use-of-force MO text and a motor-vehicle MO text through active learning processes. If this is the case, then the pseudorandom generation would be correlated with the model of interest. For example, the proportion of motor-vehicle labelled data in the force model would be higher than what one would expect from a truly random selection. This issue can be examined by comparing the proportion of active learning subjects (e.g. motor-vehicle theft) in the pseudorandom data (the data that are selected by using the use-of-force model). A proportion of active learning subjects that is close to expectations (i.e., the underlying random base rate) furnishes evidence against correlation. Such a finding can be verified by plotting the proportions of each classification in the selected data. 

\subsection{Pseudo-randomness Checks}

The first check entails determining whether there is a large overlap between the MO texts that are actively selected for both models. To that end, a count of MOs that have been selected for the two models was completed. To be counted, an MO had to have been selected through the active learning strategy for both models in the first n selections, where n is the minimum of the two active learning pool sizes. A total of 22 MOs were selected for both models from a pool of 600. Therefore, 3.6\% of the two active learning selections overlap. From this value, it may be inferred that the overlap is not large and that the inherent difficulty of the texts is not a significant factor in the results.

For the second test, which investigates the potential correlation between the two model types, plots that depict the proportion of each type of classification relative to the approach to selection are reviewed. These plots are displayed in Figures \ref{fig:active_car}  and \ref{fig:active_force}. The individual plots are explored below. Panel (b) of each plot is of particular interest for the second test.

Each plot has three lines. The red line denotes the expected random proportion, that is, the proportion that is calculated from the test and validation sets (400 samples). Assuming that they are randomly selected, this proportion should be an accurate estimate of the true-population proportion. For the motor-vehicle model, this proportion is 9\%. The grey line denotes the cumulative proportion. It was calculated at each stage and for each sample, and its value is equal to the total number of positive samples divided by the total number of samples at a given stage. The black line is the batch proportion, that is, the proportion of positively labelled samples in a batch of 100.

Panel (b) plots the proportion of data with a positive classification for one model against the active learning selection of a different model. Therefore, Panel (b) in Figure \ref{fig:active_car}  plots the proportion of positive classifications of theft of a motor vehicle with the active selection process of the use-of-force model. If the lines in Panel (b) are in close proximity, then the pseudorandom selection approximates a fully random one. The exposition now turns to a detailed exploration of the individual plots, which should indicate whether the pseudorandom data selection is a sufficiently close approximation of a random selection. 


\paragraph{Motor vehicle plot}We interrogate the lower plot of Figure \ref{fig:active_car} to see if the lines batch and cumulative proportions are close to the random proportion. Indeed we find that all lines are very close. This gives confidence that the data generated for the force model can be thought of as random with respect to the motor vehicle model. 

\begin{figure}[!ht]
  \centering
    \includegraphics[width=\textwidth]{images/car_plots_active.png}
    \caption[Active labelling - Motor vehicle model. ]{{Active labelling - Motor vehicle model.} The plots show the proportion of positively labelled burglary MO texts that were selected based on the theft of a motor vehicle classification. The labelled index (x-axis) indicates the order in which the data were selected and labelled. Panel (a) reflects the proportion of MOs selected that had a motor-vehicle stolen i.e in the second batch, 42\% had motor vehicle stolen. Panel (b) This reflects the proportion of motor vehicle stolen MOs that were selected during the Force active learning.}
    \label{fig:active_car}
\end{figure}


\paragraph{Force plot}  The same procedure was employed to study the bottom plot in Figure \ref{fig:active_force}. That plot covers the data that were selected by the motor-vehicle model but tested for the proportion of use-of-force cases. As with the motor-vehicle plot, the cumulative and the batch proportions are close to the random-proportion line. Once more, one can be reasonably confident that this use of the data represents a pseudorandom selection.

The results from the two plots and the investigation of the selection overlap indicates that the pseudorandom approach is sufficiently random for the hypothesis that active learning would be an improvement on random selection to be tested. The next step entails comparing the MCC scores for each batch to determine whether the employment of the active learning approach has any benefits.



\begin{figure}[!ht]
  \centering
    \includegraphics[width=\textwidth]{images/force_plots_active.png}
    \caption[Active labelling - Force model.]{{Active labelling - Force model.} Plots showing the proportion of positively labelled MO notes for a burglary where Force was reported as being used. The labelled index (x-axis) indicates the order in which the data was selected and labelled. Panel (a) is for active learning based on Force model probabilities. Panel (b) is for selection using probabilities based upon the motor vehicle models.}
    \label{fig:active_force}
\end{figure}


\section{Results} The results are explained for each model separately and then aggregated in the discussion section. The MCC scores for the active learning data and the pseudorandom data were compared as outlined above. The MCC scores were generated by using the validation dataset after each fine-tune. Recall that labelling ceased when the MCC of the validation set reached 0.9, with 1 being a perfect score. Active learning for the use-of-force model ceased after seven batches. Nine batches were needed for the motor-vehicle model. 

\subsection{Motor vehicle model.} Table \ref{tab:active_results_car} displays the MCC and recall metrics for each of the nine batches of active learning data. As expected, MCC generally increases as more data are labelled and peaks at 0.91 with 900 MO texts labelled. The final column in Table 11.1 displays the score that results from the use of all pseudorandomly selected data. This MCC score reflects the data that were generated from seven iterations of the use-of-force model. For a fair comparison, this MCC score should be contrasted to the seventh batch of the active learning-generated data. Active learning therefore has an MCC of 0.88, which is higher than the MCC value for the pseudorandom selection, which is 0.80. If one compares the score of the pseudorandom selection to all of the active learning values, it becomes evident that it falls between the scores for the fifth and the sixth sets. Therefore, the gain in model performance is equivalent to that of labelling 100 additional MO texts.

\subsection{Force model.} The MCC values for the use-of-force model are presented in Table \ref{tab:active_results_force} . The last column of that table represents the MCC score for the seventh batch of the data that were generated from the motor-vehicle model. The final MCC of the active learning model is 0.92; the comparable MCC from the pseudorandomly generated data is 0.89. Once more, the active learning method produces a higher MCC score than the pseudorandom approach for a comparable number of labelled data. When compared to the MCC scores for active learning, the MCC of the pseudorandom selection falls between the fifth and the sixth set, indicating a gain in performance that is equivalent to that which would result from labelling 100 additional MO texts.




\begin{table}[]
\centering
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
Batch  & 1 & 2    & 3 & 4    & 5    & 6    & 7    & \multicolumn{1}{l}{8} & \multicolumn{1}{l}{9} & \multicolumn{1}{l}{Force (7)} \\ \midrule
MCC    & 0 & 0.38 & 0 & 0.66 & 0.78 & 0.86 & 0.88 & 0.85                  & 0.91                  & 0.80                             \\ \midrule
\end{tabular}
\caption[MCC metrics. PF1 data. Car stolen model]{\label{tab:active_results_car} MCC metrics for the motor vehicle model with the data selected through active learning. Each entry is the MCC metric after that batch. The final column refers to data that was selected using the alternative model (force model), the number seven in brackets refers to that data being the seventh batch and for most similar comparisons should be compared to the seventh active learning batch}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{@{}llllllllc@{}}
\toprule
Batch    & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{Motor (7)} \\ \midrule
MCC    & 0.52                  & 0.70                  & 0.82                  & 0.55                  & 0.83                  & 0.91                  & 0.92   & 0.89                \\ \bottomrule
\end{tabular}
\caption[MCC metrics. PF1 data. force used model]{\label{tab:active_results_force} MCC metrics for the force model with the data selected through active learning. The final column refers to data that was selected using the alternative model (motor vehicle), the number seven in brackets refers to that data being the seventh batch and for most similar comparisons should be compared to the seventh  active learning batch}
\end{table}


\section{Discussion} Active learning has been proven to be successful when used with police MO data. For both the use-of-force and the motor-vehicle model, the data that were selected through the active learning approach resulted in higher MCC scores than the pseudorandomly generated data. In essence, the benefit of active learning seems to be equal to that of labelling an additional 100 examples, a 14\% decrease in the burden of labelling.

Clearly, the active learning approach has certain benefits. However, its use is not costless. Additional PTM finetuning is required. The model must be trained and allowed to label each of the texts. Exact training time varies with the time that it is allocated to finetuning and the generation of model predictions. For a deep learning model, this amount of time is not negligible. For example, at the end of the active learning process, training the BERT model would take approximately 4 hours. A further hour would be needed to label the remaining data, contributing significantly to elapsed time.

Active learning also causes the process to become more complex. With several labellers, the process is delayed considerably because labelling must be co-ordinated in batches. This additional co-ordination period can also be lengthy. In practice, subsequent studies with multiple labellers showed that a single batch of texts is labelled in a 48-hour rhythm, which eases the burden on the labellers. What could have been achieved in a matter of days took a fortnight.

It is possible that the batch size (100) was too large. Labelling at a much lower rate, say in batches of between 10 and 50, may have yielded a larger reduction in overall labelling because the model would pick difficult-to-classify texts more frequently. However, this reduction in batch size would have caused more time to be lost in co-ordination and model building.

Additionally it is possible that the size of the batch (100) was too large. Perhaps labelling at a much reduced rate, say batches of 10 to 50, may have seen a greater reduction in overall labelling. This is because the model gets the opportunity to pick out the texts that it finds difficult to classify more often. Again though this reduction in batch size is likely to introduce proportionally more time lost to coordination and model building.

Therefore, the desirability of using active labelling is not self-evident. The decision must reflect a balance between the ease of adopting a more complex system and the time (both elapsed time and user time) that is available for a given task. If user time is limited, then active learning can save between 1 and 2 hours per project. It is also likely to result in more positive identifications of rare classes, providing the labeller with more exposure to MO texts of interest. However, if results must be obtained rapidly, that is, if elapsed time is of interest, and if the user can allocate more time to the task, active learning may be undesirable. 


\section{Conclusion} Active learning is a technique for reducing the amount of data that need to be labelled for the process of supervised learning to occur. The technique was used with PF1 data and applied to both the use-of-force and the motor-vehicle models. In both cases, the active learning strategy produced results that were superior to those that emerged in consequence of the adoption of a pseudorandom data selection strategy. However, the reduction in data labelling was only 14\%. In practice, given the additional co-ordination costs that the active learning strategy entails, the resource savings are likely to be insignificant. 
 
