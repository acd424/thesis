\chapter{Summary of Study Results.}


\section{Introduction} This chapter concludes the second part of this thesis. Its aim is to draw the lessons from the previous studies together by reference to the supporting objectives that were set out in Part 1. The research question is as follows:


\textbf{Can PTMs be used efficiently to extract information from police free-text data, and if so what practical applications for problem-oriented policing does this approach have?}

The supporting objectives are

\begin{enumerate}
\item {\bf Identify the extent of NLP usage with police data.} 
\item {\bf Evaluate how effective PTMs are with MO data.} .
\item {\bf Evaluate how effective PTMs are with Police Incident data.} 
\item {\bf Evaluate how effective Active Learning is with police data.}  
\item {\bf Identify which parts of the POP process might be best supported by the use of PTMs.} 
\item {\bf Identify implementation barriers for PTMs.} 
\end{enumerate}

\section{Summary of Study Results.} This section refers to the supporting objectives to explore the cross-cutting issues that were identified in the studies, as outlined in the preceding parts of this thesis.

\subsection{Extent of NLP usage with police data.} TThis was conducted in the literature survey in Chapter 6. The main findings from that chapter was that, despite the sporadic use of NLP models to analysis police-generated data in research, PTMs had not been studied previously. The study of the PTMs that are applied to police data is important because PTMs are currently the most powerful NLP models, as judged by academic NLP benchmarks, and their performance may therefore be superior to that of the methods that are currently employed to treat police data. In addition, Chapter 6 also indicates that, when they use algorithms and NLP models, the police are concerned about factors other than performance, such as bias within the models and explainability. These factors were examined in the studies and in the performance assessments, which also refer to bias and explainability as well as to the performance metric MCC.

\subsection{Evaluate how effective PTMs are with MO data.} The evaluation of the application of PTMs to MO data was the subject of Study 1a and Study 1c. Study 1a investigated two different classifications by reference to PF1 MO data. Study 1c investigated three different classification tasks by reference to PF2 MO data. In addition, Study 1a compared the PTM models to a simple keyword approach, and Study 1c compared PTMs to an existing flag method. Study 1c also investigated the application of PTMs across police forces and over time. The results from the classifications of the two studies are discussed first, which is followed by an examination of the minor experimentations.

In Study 1a, two classification tasks were undertaken with burglary MO data. The first task entailed determining whether motor vehicles had been stolen during burglaries. The second classification task was to determine whether force had been used to enter buildings during burglaries. In both classifications, the fine-tuned PTMs exhibited high performance, with MCC scores above 0.97. In the motor-vehicle model, one fine-tuned model labelled every MO text in the test set correctly. The downside to these PTMs has to do with the labelled data that are required. PTMs are supervised learning models, which means that they require labelled data from which to learn. Approximately 900 labelled examples were needed, equal to approximately 9 hours, or 1.5 days, of labour. In the case of the PF1 data, there were 9,961 burglaries. Labelling 900 examples by hand takes less time than reading 9,961 MO texts. However, if the amount of burglaries of interest is smaller, for example because the area that is subject to a POP intervention is not large, then spending time on labelling data and fine-tuning a PTM may be inefficient.

Both studies also investigated the explainability of the models. Explainability is important because it is conducive to the formation of trust in the models and because police officers might need to explain how results are generated to interested parties, including the public. Explainability was investigated though the LIME tool. The investigation culminated in word clouds that highlight the most important words for each classification. The word clouds that were generated from the MO data contained words that a human might use to complete the same tasks. For example, the word “smash” is prominent in the use-of-force world cloud, indicating, as one would expect, that if the word “smash” is used, then a property was broken into. However, when an omission, such as not stealing a car, is not described, the word cloud can be inconclusive. An inconclusive word cloud is one in which the words are of a similar size and in which no words are prominent. This, however, is generally a reflection of the structure of the MO data.

Bias was also investigated in both studies. The bias examinations used two metrics, namely 1) EoO, which measures the disparity of the probability of TPs across groups, and 2) PP, which measures disparity of the probability of FPs across groups. These metrics were calculated for the test set and then for 10 cross-validation test sets. 

Due to the differences in data availability across police forces, bias was investigated by using different factors for PF1 and PF2. For PF1, bias was investigated by reference to the statistical characteristics of the MO texts, namely 1) length and 2) the number of word pieces. Word pieces are important because they are a measure of how many out-of-vocabulary words are used in an MO text. An out-of-vocabulary word is a word that is not included in its entirety into the predetermined vocabulary of the PTM. For this reason, it is broken down into multiple pieces that are accessible to the model. For example, PTM vocabularies do not include the word \say{untidy} in their vocabulary. It is broken down into \say{un} and \say{tidy}. The bias investigation drew on the Pearson correlation coefficient to determine whether there was a relationship between the accuracy of model predictions and statistical properties. No evidence of a relationship between PTM performance and the number of word pieces or the length of an MO text was found – there was no evidence of bias. For PF2, the bias investigation was conducted by using victim characteristics, namely gender and ethnicity. There was no strong or consistent evidence of bias on the basis of either characteristic.

In addition, Study 1c investigated the use of PTM models over time and across police forces. There was no significant drop in performance over time, and PTMs that are fine-tuned for one force can be used by another. However, performance deteriorates across forces.
 
\subsection{Evaluate how effective PTMs are with Police Incident data.} Police incident data are different from MO data. The main differences are that police incident data are longer and less well edited than MO data. The incident data also contain more words that are redacted in the course of the whitelisting process. Each of these differences may contribute to inferior PTM performance. Importantly, due to the length of the incident texts, a different PTM, Longformer, was used for the analysis.

The police incident data were explored in a similar manner to the MO data but only for one police force, PF2. Three classification tasks were used to explore the use of PTMs. These classification tasks were analysed in terms of performance (by using MC), explainability (by using LIME and word clouds), and bias (by using the bias metrics EoO and PP). The bias investigation focused on the method by which complaints are received (electronically or by telephone).

The performance of the PTMs when applied to police incident data was not as strong as their performance on the MO data. This said, the results from the police incident data were comparable to the performance of PTMs when applied to recognised benchmark datasets from the literature. Some, but certainly not all, of this decrease in performance may be attributed to factors that are specific to this study. These factors include 1) the need to redact text and 2) suboptimal computing power. Data redaction entails some loss of information. The redaction rate of the police incident texts (8\%) was higher than that of the MO texts (2\%). However, the police would not need to make such redactions. Therefore, if the PTMs had been used by the police, then performance would not have been hampered by redaction. Secondly, the computing power that was available for this study was suboptimal. Consequently, the PTMs could not be finetuned optimally, and maximum performance may not have been attained.

The investigation into explainability revealed that the PTMs were using system-generated text to aid prediction. System-generated text is text that is generated by the police computer systems as they process complaints that are submitted electronically, for example by email. This reliance on system-generated text was reflected in the investigation of bias.

Since there were no data on victim or offender characteristics, the bias investigation focused on the channel through which the reports were received. In broad terms, there are two methods – the public can make telephone calls (emergency and nonemergency numbers) or rely on electronic means (email or online forms). The investigation revealed that the PTMs were biased when classifying incident texts. The bias was consistent with the underlying base rates of the classification types in the delivery method types. For example, since a high percentage of Covid-19 complaints had been made electronically, the PTM was more likely to overclassify logs as Covid-19 complaints if they had been delivered by such means. It is likely that this bias was due, in part, to the system-generated text, which effectively identified a given log as having been received electronically.

\subsection{Evaluate how effective Active Learning is with police data.}  Active learning is a method that is designed to reduce the overall volume of data that require labelling. Active learning achieves this objective through the incremental finetuning of a PTM and the use of the resultant model to select the next set of data to be labelled. The results from Study 1a show that there was, on average, a reduction of approximately 14\% in the volume of data that had to be labelled. However, this efficiency was partially offset by the additional processing time that was needed to finetune the PTM in each round of active learning. Therefore, the results are not conclusive, and the desirability of using active learning depends, in part, on the time that an analyst has at their disposal (if time is short, active learning is useful because there are fewer data to label) and the relevant deadline (if close, then active learning with PTMs may take too long due to the additional processing time).

\section{Study Limitations} This section reviews the limitations of the studies that were presented in this part of the thesis. The limitations are reviewed by reference to the aims of the study.

\subsection{Problems} In general, the results from the studies are encouraging. However, the problems that were identified and used to guide the construction of the models are highly limiting. Only one type of crime, burglary, and one type of incident, ASB, were examined. Each of these problem areas only has three factors, yielding six different problem-incident combinations in total from what could be an infinite combinatorial space. In short, the sample is small. The PTMs were only proven to be useful for a small set of problems.

\subsection{Data types}. Similarly, the diversity and the volume of the data were also limited. Only MO texts and incident logs were considered. Police forces have more document types in their data stores, including crime summaries and witness statements. Some of these documents can be long. The documents that were examined here are relatively short. PTMs were only shown to be useful when applied to data in which short documents predominate. Further investigations are required to determine whether this performance can be repeated with documents of different types, particularly longer ones.

\subsection{Explainability.} Although explainability tools were employed, the results were not trialled robustly. Explanations are context and audience specific, and the explanations that were generated here were not trialled with those who might use them. In addition, the explanations that were generated rely on local models; the global effects of words are not fully understood. However, this deficiency was mitigated by the use of the LIME tool across 200 texts and the aggregation of the findings.

\subsection{Bias.} Three important areas were specified for bias in Chapter 8. The three are data coverage, data selection bias, and algorithmic bias. Each type of bias can have an impact on the final results. The bias investigations in this study focus on algorithmic bias. The other two types of bias were beyond the scope of the research, but could have affected the results. In the case of POP, prevention resources can be allocated inequitably. Bias against victims with certain characteristics could only be investigated by reference to the PF2 MO data. This area would certainly need to be explored more thoroughly for individual models to be used operationally.


\subsection{Conclusion} In conclusion, the use of PTMs with police data was successful. Performance was proven to be satisfactory, especially with MO texts. Limited evidence of bias was found; importantly, none of it revealed bias against victims with certain characteristics. In addition, the explainability tools showed that, for the most part, the PTMs complete classification tasks by using words that would make sense to a human reader.

The conclusion from these studies is that PTMs are useful for classifying police data. How might this classification be used for POP? Where in the SARA cycle could it have the strongest impact? These questions are answered in the next chapter, which applies the results and examines their limitations in the context of POP and the SARA framework.

