\chapter{Summary of Study Results.}


\section{Introduction} This chapter is the first chapter in the third and final part of this thesis. The aim of this part of the thesis is to draw together the lessons from the previous two parts and then asses what they mean for the future of POP and NLP.  This chapter will synthesise the results form the previous studies and describe what implications they have for problem-oriented policing(POP). The following chapter will look more broadly at the possible future research directions for NLP with police data.

This chapter will be broken into four main sections. The first section will review the results from the studies and draw conclusions based on the main research question and the supporting objectives. The second section will utilise the conclusions and limitations presented and using the SARA framework for POP will suggest where PTMs might be best employed. The final section will conclude with barriers to the implementation of PTMs in the POP cycle.

As a reminder the research question is:

\textbf{Can PTMs be used efficiently to extract information from police free-text data, and if so what practical applications for problem-oriented policing does this approach have?}

The supporting objectives were:
\begin{enumerate}
\item {\bf Identify the extent of NLP usage with police data.} 
\item {\bf Evaluate how effective PTMs are with MO data.} .
\item {\bf Evaluate how effective PTMs are with Police Incident data.} 
\item {\bf Evaluate how effective Active Learning is with police data.}  
\item {\bf Identify which parts of the POP process might be best supported by the use of PTMs.} 
\item {\bf Identify implementation barriers for PTMs.} 
\end{enumerate}

\section{Summary of Study Results.} This section will use the supporting objectives as a handrail to explore the cross cutting issues identified in the studies as outlined in the earlier parts of this thesis.

\subsection{Extent of NLP usage with police data.} This was conducted in the literature survey Chapter 6. The main findings from this chapter were that although there were a selection of research using NLP models to analysis police generated data, PTMs had not previously been studied. The study of PTMs with police data is important because PTMs are currently the most powerful NLP models, as judged by academic NLP benchmarks, and so may offer better performance with police data than existing methods. Additionally Chapter 6 also set out that in order to use the algorithms and NLP models the police were also concerned about factors wider than just performance. The police were also concerned about factors such as bias within the models and explainability of the models. These  factors were taken forward to the studies and when performance was assessed it also included bias and explainability as well as the performance metric MCC.

\subsection{Evaluate how effective PTMs are with MO data.} Evaluation of PTMs with MO data was the subject of Studies 1a and 1c. In Study 1a investigated two different classifications with PF1 MO data. Study 1c investigated three different classification tasks with PF2 MO data. In addition Study 1a compared the PTM models to a simple keyword approach. Study 1c additionally investigated the application of PTMs across police forces and over time. The results form the classifications of both studies will be discussed first, then the more minor experimentations will be discussed.

In Study 1a two classification tasks were undertaken with Burglary MO data. The first classification was to test if a motor vehicle had been stolen during the burglary. The second classification task was to understand if force had been used to enter the building during the burglary. In both classifications the fine-tuned PTM provided high performance classifications with MCC scores above 0.97. In the motor vehicle model one fine-tuned model correctly labeled every MO text in the test set.  The downside to this model performance is the labelled data that is required. PTMs are a supervised learning model which means they require labelled data from which to learn. In these examples there was a requirement for around 900 labelled examples. This equates to a time resource of about 9 hours or 1.5 days work.  In the case of the PF1 data there were 9961 burglaries. So hand labelling 900 examples is much quicker than reading 9961 MO texts. However if the amount of burglaries of interest was smaller, for example because the area for a POP intervention was limited, then the time spent labelling the data and fine-tuning the PTM may not be an efficient use of time.


Both studies also investigated the explainability of the models. Explainability is important because it helps develop trust in the models and police officers might need to explain how results were generated. Explainability was investigated though the LIME tool that culminated in word clouds that highlighted the most important words for the classification.  The word clouds that were generated with the MO data reflected words that human might use to make the same classifications. For example the word \say{smash} is very prominent in the force used word cloud. Indicating, as we would expect, that if the word smash is used then a property is broken into. However when an act is not described, such as \emph{not} stealing a car, then the word cloud can appear inconclusive. An inconclusive word cloud is where the words are all of a similar size and no words are prominent. This is however just a reflection of the MO structure overall.

Bias was investigated across both studies. Due to the difference in available data across police forces, bias was investigated using different factors for the different police forces. For PF1 the bias was investigated along MO statistical characteristics, namely 1)the length of an MO and 2) the number of word pieces. Word pieces are important because they are a measure of how many out of vocabulary words were used in the MO. an out of vocabulary word is one that the PTM does not have in its pre-determined vocabulary as a whole word and so breaks into multiple pieces that it does understand, As an example PTMs do not have the word  \say{Untidy} in their vocabulary so it is broken into \say{un} and \say{tidy}. The bias investigation used the Pearson correlation coefficient to determine if there was a relationship between the accuracy of the model prediction and the statistical property. No evidence of a relationship between PTM performance and number of word pieces or length of MO text was found.


For PF2 the bias investigation was conducted using victim characteristics, namely gender and ethnicity. This bias study used two metrics 1) Equality of opportunity which .... and 2) Predictive parity which ..... These metrics were calculated for the test set and then ten cross-validation test sets. Using all of the labelled data a cross validation (CV) experiment was conducted. The CV experiment randomly sampled the data on ten separate occasions to produce a train set ( 80\% of the data ) and a test set (20\% of the data), each CV model trained a PTM and then the labelled test set was used to derive the bias metrics. Thus from the CV experiment 10 values of each metric were produced and a test of differences was conducted. There was no strong or consistent evidence from the PF2 bias investigation of the PTM mis-classifying along victim characteristics.

Additionally Study 1c investigated the use of PTM models over time and across police forces. The investigation over time found no significant drop in performance. The study across forces indicates that PTMs fine-tuned in one force can be used in another force - however the PTM performance does drop. 
 
 
\subsection{Evaluate how effective PTMs are with Police Incident data.} Police incident data is different from MO data. The main differences are that police incident data is longer and less well edited than the MO data. The incident data also had more words redacted through the white-listing process. Each of these differences may contribute to poorer performance by the PTMs. Importantly due to the length of the incident texts a different PTM (longformer) was used for the analysis.

Police incident data was explored in a similar manner to the MO data , although it was only explored in one police force, PF2. Three classification tasks were used to explore the use of PTMs. These classification tasks were explored for performance (using MCC) explainability ( using LIME and word clouds) and bias using the bias metric Equality of Opportunity and Predictive Parity. The bias investigation was conducted into complaint delivery method e.g. electric or telephone.

The performance of the PTMs with police incident data was not as high as the performance of the PTMs with the MO data. Although the results with the police incident data were still comparable to the PTM performance with recognised academic benchmarks. Part, but certainly not all,  of this lower performance may be attributed to factors specific to this research. These factors include 1) The incident text was redacted 2) the computing power was sub-optimal. Data being redacted would mean that some information was lost, the redaction rate in the police incident text (8\%)  was higher than the MO text (2\%), however the police themselves would not need to do this redaction and so if PTMs were used with the police then the performance will not be hampered by the redaction factor as it was in this study. Secondly the computing power available for this study was sub-optimal, this meant that the PTMs could not be fine-tuned in an optimal way and so the top performance may not have been reached. 

The investigation into explainability uncovered that the PTMs were using system generated text to aid predictions. System generated text is text that is generated by the police computer systems as it processes complaints made electronically e.g. by email. This reliance on system generated text was also reflected in the bias investigation.

As there was no victim or offender characteristics data the bias investigation was conducted into the complaint delivery method. Broadly this was split into two methods - telephone (emergency and non-emergency) and electronic ( either email or online form). The investigation showed that the PTMs were biased when classifying the incident texts, and that bias was consistent with the underlying base rates of the classification type in the method delivery type. As an example as a high percentage of covid complaints were sent electronically then the PTM was more likely to over classify logs as covid complaints if they were electronically delivered.  It is likely that this bias was in part due to the system generated text which effectively identified the text as an electronically delivered incident log.

\subsection{Evaluate how effective Active Learning is with police data.}  Active learning is a method designed to reduce the overall number of data that needs labelling. Active learning achieves this by incrementally fine-tuning a PTM and using that model to select the next set of data to label. The results from study 1a showed that there was on average around a 14\% reduction in the data required to be labelled. However this was partially offset by the additional process time to fine-tune the PTM on each round of active learning. Therefore the results were not conclusive, and the decision to use active learning will in part be determined by available analyst time ( if in short supply then active learning is useful as there is less data to label)  and time to deadline (if close then active learning with PTMs may take too long due to additional process time).


\section{Research Limitations}

