\chapter{Conclusions}

\section{Introduction} This is the final chapter of the thesis. This chapter will consist of two sections. The first section will explore how the research covered in the thesis can be expanded. The second section will be the concluding thoughts for the thesis and will summarise the research conducted in this thesis.  

The first section of this Chapter, future research, explores how the research here may be expanded to include additional NLP techniques and or on different text document types. Further development of the research can be through three main avenues. Firstly by improving the research conducted within this thesis. Secondly NLP can be expanded  by using additional NLP techniques that have not been explored in this thesis. Lastly NLP can bne used on different document types, rather than just crime/incident summaries explored here.  


\section{Future Research} The field of NLP is continually growing and NLP models are also becoming more capable in different ways, so the application of NLP to police data is and will continue to be a dynamic field. This future research section is split into three. Firstly there is a section on models, this section is broadly focussed on how the research within this thesis may be improved beyond the scope of this thesis.  The applications section is a brief description on how NLP models can be used more widely with police data, rather than just the classification as demonstrated in this thesis. Finally, the text data types are explored to show that other document types, such as witness statements, can be analysed. 

\subsection{Models}  This section focuses on how future research can improve upon the models produced in this research, namely the use of PTMs to classify short pieces of text. Not all aspects of model performance were investigated in this research, and some that were investigated such as bias, was done in a limited way. The next sections highlight some important areas of research pertaining directly to the type of models built in this research.

\subsubsection{Further replication} The research here has been narrow in scope. Only one type of crime was investigated (Burglary) and with only three different classification types. Although this was partially replicated across two different police forces.  This research should be expanded to include additional crimes, different classifications and additional police forces. In particular further replication of the same use cases across different police forces would allow a much greater understanding of how well the models built in one police force can be reused in different police forces. If models can be reused across police forces then this will reduce the labelling burden as a single model can be produced rather than forty-three separate models (one for each force in the UK). 

\subsubsection{Type} The model used in this research was BERT. Since BERT was built there have be more PTMs produced and made available for free use.  Each of these PTMs has its own characteristics, capabilities and therefore linguistic areas where it excels. By experimenting with different kinds of PTMs, one can discover which one works best for specific uses cases. As an example another popular PTM is ROBERTA \parencite{liu2019roberta}. ROBERTA uses a different method to define which words are being used. This difference means  that  it handles previously unseen words in a more robust way. Police data with lots of acronyms or obscure words may be better represented by this model type, and so classifications may become more accurate. Other models have larger architecture i.e. more parameters to tune. This larger architecture means that bigger models are able to represent more challenging nuances in the text and therefore give more accurate classifications.  Model types are likely to evolve and so understanding which models are most suited to police data will be an ongoing process.

\subsubsection{Hyperparameter tuning} Earlier in the thesis hyperparameters were introduced. Hyperparameters are variables in the model formulation that alter slightly how it trains. An example of a hyperparameter is the number of epochs. Epochs represent the number of times the whole training set is used to train the model. In this research three epochs was used. That means that the model saw each piece of training data on three separate occasions. Tuning hyperparameters involves adjusting their values in order to optimise the model's performance. This can be a time-consuming process, but it is important because the right combination of hyperparameters can improve the model's performance. Hyperparameter tuning was not conducted in this research because the idea was to use a simplified process for classifying the texts. A simple process that could be easily implemented in a police force.  The results from this thesis indicate that the default hyperpareameters, i.e untuned hyperparameters, produced satisfactory models. However hyperparametreer tuning could lead to either more accurate classifications or  a lower requirement for labelled data. Either way adding hyperparameter tuning may lead to an improvement in model performance and so would be a good avenue for further research. 

\subsubsection{Outcome weighting} In this research getting the classification wrong was weighted equally with getting the classification correct, and so the models were trained to reduced the amount of incorrect classifications. However it may be the case, as explored in the conclusion of Study 2, that getting a classification wrong is not equal in all instances. For instance it might be that missing a burglary where a car was stolen is worse than misclassifying a burglary where a car was not stolen. To put this into sharper focus an alternate problem might be trying to find vulnerable victims, missing a vulnerable victim may be more costly than misclassifying non-vulnerable victims. 

To overcome this problem a technique called outcome weighting is used in machine learning to adjust the importance of different outcomes in a classification problem. In reference to the theoretical problem introduced earlier, missing a vulnerable victim might be classed as twice as costly as classifying a non-vulnerable victim as vulnerable. This cost function will have to be built with the end user so that their understanding of the problem, and the costs of misclassification can be coded into the model training. Typically this weighting can be either encoded into the loss function so that the model training is changed or the model outputs can be used in a more sophisticated way to deliver the desired outcome.  


\subsubsection{Vocabulary} BERT has a set amount of words that it recognises. This is called the models vocabulary. The benefit of a word being in the vocabulary is that the word will have a more defined numerical representation. If a word is not in the vocabulary then it is broken down into word pieces until it is recognised, in extremis some words can be classed as unknown. Breaking a word into word pieces can destroy some of the meaning of that word as it is not represented as as single entity. In text where there are a lot of out of vocabulary words the meaning of those words may not be represented well and therefore the classification models may not be that accurate. Any text that uses a lot of jargon is likely to contain a lot of out of vocabulary words.

There are two ways to overcome this problem. Firstly the vocabulary of BERT can be extended so that the it contains other words. The most popular unknown words can be added to the vocabulary thus preventing the word form being broken down. Secondly the unknown words can be changed to  a word or words that is already within the BERT vocabulary.  For instance \say{untidy} was not recognised by BERT and so could be replaced with \say{messy} or \say{not tidy} which are both recognised by BERT. Which approach is better will likely differ by use case.

Overall, making the text and BERT vocabularies more similar can help to improve PTMs performance on specific tasks by increasing the models understanding of the domain in question.

\subsubsection{Pre-train} As mentioned previously there are two parts to utilising a BERT model. First is the pre-training element - which is resource intensive and  gives the model a general understanding of language. Secondly, there is pre-training which is conducted for each specific task. The pre-training was not completed for this research, but in other domains where they have had access to sufficient data, they have conducted pre-training. Where this pre-training has been conducted new variations of BERT have been built. For instance Legal-BERT has been built to understand legal documents \parencite{legal_bert}. Another variation trained on medical data is med-BERT. 

Therefore an interesting avenue of research would be to pre-train a BERT model on police data, perhaps exclusively MO data from across several different forces, to produce an MO-BERT. Given the success in other domains this new model is likely to perform better at classifying MO texts than the regular BERT. This approach may therefore save time and resources when fine-tuning for each additional task as it has a better understanding of the domain specific language from the outset.

This section has demonstrated that there are a number of interesting avenues for additional research to enhance the classification work outlined in this thesis. The next section takes this further by exploring what other NLP techniques, beyond classification of text passages, can be used to enhance POP. 

\subsection{Applications} This next section introduces additional applications of PTMs above the classification type that was used in this research. Additional applications are important because they allow information to be extratcted from the data in different ways or enable access to the key elements of the data (e.g. summarisation).

\subsubsection{Question and Answer} Question answering (Q\&A) is a NLP task that involves using a PTM to answer questions posed in natural language, given a text which contains the answer. This is different to chat style AI where the PTM uses only pre-defined knowledge to answer the question. So for example in our case a police officer may have one or more documents. These documents will then be submitted to the PTM with a question. Using only the text available in the documents the PTM will then generate a response. Q\&A systems can be designed to answer a wide range of questions, including factual questions, definitions, and queries about people. However they have not been trialled against crime documents and so their performance in this area is unknown. This may be more useful when the crime is of low volume or a specific response is needed rather than a binary classification.

\subsubsection{Named Entity Recognition} Named Entity Recognition (NER) is another NLP task that is based on classification. In this instance rather than classify a passage of text it classifies every word within that text. The typical task for this type of model is to extract organisations, people and places from a passage of text. The PTM in this instance will label each word within the text as either nothing, a person, an organisation or a place. For example, in the sentence "Boris Johnson was born in New York on 19 June 1964" the named entities are \say{Boris Johnson}, \say{New York} and \say{19 June 1964}. Where words are positively identified these can then be extracted from the text. The PTM uses both the word itself but also the context of the word to output the classification. Therefore names or places that were never in the training material can still be correctly extracted because they will be used in a similar linguistic context. In a policing context the words of interest may not be people or places, but may for instance be the weapon used in an assault. NER is useful for extracting particularly detailed text that can change from one text document to another. However the labelling burden, because each word has to be labelled, is much higher.  


\subsubsection{Summarisation} Text summarisation involves producing a short summary from a longer document or a collection of documents. The idea is to keep the most important information from the original documents so that the summarisation can be read in isolation. This task is well understood in the NLP domain but is hard to generalise across different language domains because 1) it is inherently hard to quantify what is a good summary and 2) the varying importance of different facts across domains. As the quality of a summary is difficult to quantify it means that there has to be more human intervention in the modelling process which makes it more resource intensive than other NLP tasks. The importance for the police domain is that reviewing cases would be much quicker and easier with a case summary. In some forces these are already produced by an Officer, placing an administrative burden upon them.

This section introduced three methods beyond classification that could be leveraged to support the analysis of free text data. The next section moves beyond models and explores that data types that can be analysed.  


\subsection{Data}

\subsubsection{Document types} Police forces have many different type of text documents above the two studied here, MO descriptions and incident logs. Police forces generate lots of text data when conducting their business including case summaries, witness reports, communication logs and arrest reports. As the document type changes then the underlying characteristics of the document change - and therefore applicable techniques may change. As mentioned previously longer documents are difficult to analyse with PTMs because of the additional memory required to keep track of the whole document. Other shorter documents such as communication logs may have many more abbreviations or poor grammar if they have been hastily recorded or are a direct recording of what was spoken. These different document types will therefore produce different challenges that will have to be overcome in different ways. Therefore researching PTMs with the different document types will allow a greater understand of where PTM use is most effective.


\subsubsection{Languages}Although the research here has been with texts in the english language similar models do exist for other languages \parencite{scao2022bloom}. In addition translation models also make it possible to translate non-english text into english in order to use english models. Further research utilising the non-english models to prove that similar tasks are possible in non-english languages would also be useful to prove the approaches explored here have widespread utility. 

\subsubsection{Bias} The results from the bias explorations within this thesis are encouraging, but are only reflective of one portion of the data journey from creation to model output. Particularly the research presented here focussed on algorithmic bias. To the authors knowledge there is little understanding of the biases effecting the completeness of the details in a textual crime record. This part of the data journey is worthy of more research to understand potential biases that could be inherent to the data from this route.

This section on data highlighted additional research areas for the purposes of expansion of NLP and, in the case of bias, acceptance of NLP with police data. Equally important for acceptance of NLP models is model explainability which is discussed in the next section. 

\subsection{Explainability} Although explainability was introduced and explored in this thesis, the visualisations provided were not formally tested for effectiveness. Explainability, as outlined in Chapter 4 is specific to the audience, and so these visualisations and methods should be tested with all intended audience types. This testing include members of the public (e.g. victims whose crimes maybe classified), police officers who use the model outputs and those that authorise the use of the models. Once the explainability methods have been properly tested and accepted they can then be used as evidence to bring the models into service.

\subsection{Summary} In summary there are a huge number of avenues for potential research, particularly those that have been shown to work in other domains. However two general problems still remain across NLP that may restrict usage with police data. Firstly long texts. Long texts are problematic because the computational requirement rises quadratically with text length - meaning that very long texts require lots more computational power and memory than is generally feasible. Secondly labelled data requirements. In this research around 1000 labelled texts were required for each problem, in some instances this labelling may be disproportionate in resource requirements to the problem being solved.  These challenges however are being actively worked on and it is likely that these restrictions will be less prominent in future meaning that NLP techniques will be widely applicable to police data and the polices requirements for interrogating their free-text data.


\section{Concluding Remarks} This research set out to understand if modern NLP methods, namely pre-trained Language models (PTMs), could be effectively employed in the problem-oriented police (POP) methodology. The hope was to lower the analytical burden of the POP process as the analysis required had previously been considered an inhibiting factor to the successful completion of POP projects.  The exploration of PTM utility was achieved by experimenting with PTMs to explore intra-incident variation relating to descriptions of burglary and anti-social behaviour (ASB). In each case intra-incident variation was specified by using PTMs to classify for a pre-determined characteristic. The robustness of this work was explored through model explainbility and bias. The results (for burglary) were also replicated across different police forces. The results of these experiments were then analysed in the context of the POP cycle and they were found to have particular use in the initial (Scanning) and latter (Assessment) phases of the POP cycle. Thus indicating that PTMs would be able to reduce the analytical burdens of the POP cycle. Latterly issues of implementation were discussed and areas for future work were explored.  In conclusion NLP techniques and models have much to offer police forces. PTMs in particular are very useful at extracting information from free text material. This information can then be utilised for a variety of purposes including generating crime prevention strategies. 






